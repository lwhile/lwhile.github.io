<html>

<head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>
    Go | 神蛋杂谈
</title>
<link rel="shortcut icon" href="https://lwhile.github.io/favicon.ico?v=1576378575617">
<!-- <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous"> -->
<link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://lwhile.github.io/styles/main.css">
<!-- js -->
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="https://lwhile.github.io/media/js/jquery.sticky-sidebar.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>


</head>

<body>
    <div class="main">
        <div class="header">
    <div class="nav">
        <div class="logo">
            <a href="https://lwhile.github.io">
                <img class="avatar" src="https://lwhile.github.io/images/avatar.png?v=1576378575617" alt="">
            </a>
            <div class="site-title">
                <h1>
                    神蛋杂谈
                </h1>
            </div>
        </div>
        <span class="menu-btn fa fa-align-justify"></span>
        <div class="menu-container">
            <ul>
                
                    
                            <li>
                                <a href="/" class="menu">
                                    首页
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/archives" class="menu">
                                    归档
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/tags" class="menu">
                                    标签
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/post/about" class="menu">
                                    关于
                                </a>
                            </li>
                            
                                
            </ul>
        </div>
    </div>
</div>

<script>
    $(document).ready(function() {
        $(".menu-btn").click(function() {
            $(".menu-container").slideToggle();
        });
        $(window).resize(function() {

            if (window.matchMedia('(min-width: 960px)').matches) {
                $(".menu-container").css('display', 'block')
            } else {
                $(".menu-container").css('display', 'none')
            }

        });
    });
</script>

            <div id="main-content" class="post-container main-container">
                <div id="content" class="main-container-left">
                    
    <div class="i-card">
        <b>标签：#
        Go</b>
    </div>
    
        
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/go-timezone">
                        Go 时区用法总结
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2019-02-19</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_1">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-error">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            坊间有个说法，说中美两国的程序员除了薪资、加班强度外，还有个比较有趣的差异，就是在编码时对于时区的敏感程度也不同。
「大部分」美国程序员在时区问题上很有经验，因为美国在日常生活中会使用到 3 个时区。而中国「大部分」程序员在编码时基本不会考虑时区问题，因为大家都在东八区，没那个烦恼。
在过去一年我维护着一个公司内部的日历库，用于做交易时间的计算。在这个过程中被时区问题折磨得很难受，不过也积累一些 Go 处理时区上的经验。
时区配置
时区的配置一定要声明在配置文件中，比如下面这种 yaml 格式：

timezone: Asia/Hong_Kong

别想着用机器时区一劳永逸这个问题，因为机器的变数太多了。
载入时区
在 Go 中，时区被封装在 time.Location 中进行抽象，而载入时区有两种方法

通过 IANA 数据库

给 time.LoadLocation 传入一个符合 IANA 时区数据库的时区名字，比如：

Asia/Hong_Kong


通过偏移量

time.FixedZone(&amp;quot;UTC-8&amp;quot;, -8 **60 **60)
转换时区
比较简单的操作，直接调一个 time struct instance 的 In(*Location) 方法。
修改时区
注意修改时区和转换时区是两种不同的概念。
比如北京时间 20:00pm 转换时区，以美东时区为例，会是 7:00am (冬令时)
而修改时区是指把北京 20:00pm 修改为美东 20:00pm，可以这样操作：

// Pseudo code
var estTime time.Time
var cstTime time.Time
var estTimezone *time.Location
// 通过 time.Date 方法重新生成一个 time struct instance
estTime := time.Date(cstTime.Year(), cstTime.Month(),..., estTimezone)

Round to day
实在想不起一个名词可以形容 Round to day 的过程，我们用这个方法来做日内时间的比较。
比较典型的应用场景是判断是否到了开市/休市时间。由于在 Go 中 time 类型是包含有年、月信息的，所以要表达「每天早上 9:30」 这个时间不大好做。
既然不好做，那就把它们去掉。

// Pseudo code
func RoundToDay(t time.Time) time.Time {
// 注意在 Go 中 zero time 并不是 0 year 0 month 0 day，而是 1，所以用 0 的话会溢出喔
return time.Date(1,1,t.Day(), t.Hour() ...)
}

LMT
LMT 这个问题是在 Round to day 上面引发出来的，说起来也很有趣。LMT 本意是指 local mean time（地方平时，在指定的经度范围内使用一致时间的地方太阳时）。
如果你用试图在 Go 里面用香港时区构建一个 zero time 的话，就会发现最后返回出来的 time struct instance 的 location 信息是 LMT。这个问题是在一处单元测试中发现的，死活无法通过测试（嗯，单测真的很重要）。
不过这是为什么？
我相信知道这个知识点的人绝对少之又少，这是一个非常非常冷的知识（最后通过我那位知识储备量非常丰富的同事知道的这个）

香港時間的授時服務，是香港天文台從1883年成立至今的主要職責。早期香港天文台使用赤道儀及中星儀，透過觀測星象測量時間。當時香港時間是當地平均時間（LMT）UTC+7:36:42（準確值為UTC+7:36:41.8842）。香港天文台最早於1885年1月1日對公眾授時[1]，當年香港天文台於九龍尖沙咀警署設置桅杆，以升降時間球的方式對外發布時間。1885年1月1日，香港天文台於中午12時50分把時間球升到桅杆頂端，然後於下午1時正，首次把時間球降下，成為香港首次報時訊號，並作為香港時間的標準。用於報時的時間球訊號塔，後來於1933年因為電台報時的開展而拆除。1904年10月30日，香港時間正式確定為格林威治標準時間快8小時（GMT+8）[2]。2004年，天文台安裝了一套高準確度授時系統，利用全球定位系統共視方法，向國際度量衡局提供天文台的原子鐘時間數據，參與訂定協調世界時。天文台亦根據國際度量衡局提供的時間數據調校原子鐘，使其準確度保持在一百萬分之一秒以內。現時，香港天文台以銫原子鐘報時系統作為香港時間的標準，誤差僅為每日1微秒之內。香港天文台設有互聯網時間伺服器，為互聯網的用戶提供準確的時間校正服務[3]。

根据上面的记录来看几个例子吧，首先是 1904 年 10 月 30 日后香港使用 GMT+8，我们把时间定格在它前一秒：

location, _ := time.LoadLocation(&amp;quot;Asia/Hong_Kong&amp;quot;)

t := time.Date(1904,10,29,11,59,59,59,location)
fmt.Println(t) // 1904-10-29 11:59:59.000000059 +0736 LMT &amp;lt;- LMT 无疑
然后是 1904 年 10 月 30 日 零点：

location, _ := time.LoadLocation(&amp;quot;Asia/Hong_Kong&amp;quot;)

t := time.Date(1904,10,30,0,0,0,0,location)
fmt.Println(t) // 1904-10-30 00:23:18 +0800 HKT &amp;lt;- HKT 了。
厉害吧。。。还有更厉害的。。。

location, _ := time.LoadLocation(&amp;quot;Asia/Hong_Kong&amp;quot;)

t := time.Date(1942,10,30,0,0,0,0,location)
fmt.Println(t) // 1942-10-30 00:00:00 +0900 JST
用 1942 年 10 月 30 日构建一个香港时间，时区会是（JST）日本标准时间。Why?
原因是这个：

香港日佔時期，又稱為香港日治時期或香港淪陷時期，是指第二次世界大戰時大日本帝國軍事占領香港的時期：由1941年12月25日香港總督楊慕琦投降起，至1945年8月15日日本無條件投降為止；香港人俗稱這段時期為「三年零八個月」。

别以为 Go 的开发团队这么有心，实际上 Go 也是读操作系统的 zoneinfo 文件。只能说在某个层面上操作系统也记录着每个地区的历史。。。

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/go-timezone">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/go-time-parser">
                        关于 time.Parse 99% 的 Gopher 都不知道的秘密
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2019-01-03</time>
                    
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-banana">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/go-time-parser">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/dep-override">
                        使用 override 解决 dep 中的依赖冲突
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2018-05-02</time>
                    
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            公司的 Go 项目使用 dep 做为依赖管理的工具，在使用的过程中，因为项目依赖比较复杂，经常会遇到依赖冲突导致 dep ensure 命令无法执行成功。
比如，正在开发中的项目A依赖了B和C，而 B 项目也依赖了 C 项目。

A 项目的 Gopkg.toml
[[constraint]]
  branch = &amp;quot;master&amp;quot;
  name = &amp;quot;B&amp;quot;

[[constraint]]
  branch = &amp;quot;master&amp;quot;
  name = &amp;quot;C&amp;quot;

B 项目的 Gopkg.toml
[[constraint]]
  branch = &amp;quot;master&amp;quot;
  name = &amp;quot;C&amp;quot;

接下来 A 项目因为开发的需要在 C 项目中新开了分支。在该分支合并进 master 分支之前，需要在 A 项目的 Gopkg.toml中指定分支名称。
[[constraint]]
  branch = &amp;quot;master&amp;quot;
  name = &amp;quot;B&amp;quot;

[[constraint]]
  branch = &amp;quot;new_branch&amp;quot;
  name = &amp;quot;C&amp;quot;

这时候再执行   dep ensure 是无法成功的，会提示依赖发生了冲突。
解决冲突的方法也很简单，在A项目的 Gopkg.toml中将C项目的constraint改为 override 就可以了。等到C项目的修改合并进 master 分支时，再将 constraint 改回来。
[[constraint]]
  branch = &amp;quot;master&amp;quot;
  name = &amp;quot;B&amp;quot;

[[override]]
  branch = &amp;quot;new_branch&amp;quot;
  name = &amp;quot;C&amp;quot;

constraint 与 override 的区别
dep 会如实得处理 constraint 类型的依赖，在这个例子中，尽管在 A 的依赖中将 C 的分支设置为 new_branch，但是在B项目中，引用的包还是依旧使用 master 分支 的代码。
如果将A项目的 constraint 改为 override , 则会强制让 B 项目在拉取 C 项目的代码时，拉取的是 new_branch 分支上的代码，避免因为协同开发导致的问题。

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/dep-override">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/consul-leader-election">
                        使用 Consul 实现分布式环境下的 Leader 选举
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-12-02</time>
                    
                        <a href="https://lwhile.github.io/tag/consul" class="post-tag i-tag
                            i-tag-primary">
            #consul
        </a>
                        
                        <a href="https://lwhile.github.io/tag/fen-bu-shi-xi-tong" class="post-tag i-tag
                            i-tag-other_3">
            #分布式系统
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-other_2">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            想象这样一种业务场景：你在一个分布式环境中部署了一个任务通知系统，或者一个定时任务系统，为了做高可用，每一个节点你都部署了一整套完整的服务。
很快你就会发现，用户会收到重复的通知信息，或者相同的任务也会被执行了多遍。显然这是不行的。
为了让我们的系统能够正常运行，并且又能实现高可用形式的部署，我们需要在这几个节点中选出一个Leader,做为最终执行任务的唯一节点。这篇文章将探讨如何实现这样一种需求，并且介绍如何使用consul来选举出我们的Leader。
一致性问题
我们的问题属于分布式一致性问题的范畴，即在分布式环境下，结点/服务之间如何做到类似单机的运行环境。这个问题有多重要呢，基本上只要是一个正常的分布式系统，一致性都会是一个必须要考虑的问题。这有点像你在打LOL或者吃鸡，你和队友们得保持战术的一致才能控制得住局面。从某个程度上讲，你和队友就组成了一个分布式系统。
解决分布式一致性的算法很多，但在工业界大规模使用的只有两个：Paxos和Raft。Paxos的典型代表有Google的Chubby,还有Apache基金会的Zookeeper，这个Java的同学应该有听说过。Raft的代表有Etcd和TiKV,前者已经成为许多系统不可或缺的一部分，包括Kubernetes,而后者已经在成为NewSQL的代表作上一路狂奔着。不过这里得提一句，不管是Google还是Pingcap（TiKV的开发商）,都对Paxos和Raft应用到工业软件上做了优化。
要正确得实现了Paxos和Raft算法都不是一件特别容易的事，尤其是Paxos，Raft还好些。但如果想每一个服务都嵌入一个Paxos或者Raft，这对于维护将是一场灾难。Google的做法是使用Chubby作为一致性的基础服务提供者，在这之上提供分布式锁和Leader选举的服务（是不是有点PaaS的味道）。而这篇文章将要使用的，是一个能提供类似服务的开源软件：Consul
使用Consul
回到这篇文章的主题，要解决文章最开始提到的那个问题，我们可以在几个服务之间选择一个Leader出来，具体的任务交给Leader，其他结点在Leader出现问题的时候再补上去成为一个新的Leader。我们将Leader的选举交给Consul帮我们解决，Consul只要为我们做一件事就行：给我们一把分布式的锁，哪个服务能拿到这个锁，谁就是Leader。
Consul在我们公司被用来做域名服务和建康检查，而它刚好提供了一个基于Raft算法的Kv store(Kv store可以作为分布式锁的实现基础)的功能。我们可以在这个kv store里面写入一对kv, key是一个固定的值，而value则是能够唯一代表节点/服务的ID，通过Value我们就可以知道谁是Leader。
Consul提供了一个Session机制，用它来代表服务与Consul节点之间的会话关系，并且能提供一个代表这个会话的Session ID,这个ID将作为上面提到的Value写入kv中。
整个流程经过几次调整，最后设计出来是这样的：
服务启动时，向Consul注册一个Session，得到一个Session ID。
将Session ID作为Value，字符串leader(自由定义)作为key，写入到KV中。
若写入成功，则当选为Leader.若写入失败，则表示竞选失败。
每个Session都会有一个TTL（我们公司定的是Consul的最小值10s），当选为Leader的服务需要定时延长改TTL。比如每隔5s延长一次。
竞选失败的节点，则在后台轮询，不断尝试成为Leader。
业务活动发起时，先检查当前所在节点是否是Leader,如果是才能真正发起业务，如果不是，则丢弃。
这样我们就可以解决文章开始时提到的那个问题了。
感觉文章写飘了，说到的东西有点多，Consul的这个点反而没有什么好讲的。其实重点是一致性算法和分布式锁的实现，有兴趣的同学可以看下相关的论文，Google搜一下就有了。笔者最近也在阅读和翻译谷歌实现Chubby的那篇论文，翻译的内容挂在Github上，不过还没有翻译完成，有兴趣的同学欢迎一起翻译，在Github搜”Chubby中文”就能找到项目了，或者点击原文链接也行。
https://mp.weixin.qq.com/s/gkA1qDx8e3XrNXXnxLyHfQ

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/consul-leader-election">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/how-to-design-monitor-platform">
                        如何设计监控平台的告警组件
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-20</time>
                    
                        <a href="https://lwhile.github.io/tag/jian-kong-xi-tong" class="post-tag i-tag
                            i-tag-other_2">
            #监控系统
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_3">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-other_2">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            当业务发展到一定程度的时候，开发人员会开始考虑在系统中引入监控系统来对系统/业务进行监控。绝大多数监控系统都有两大核心功能，一个是工程师通过这个监控系统，能够对整个系统的运行情况一目了然，另外一个，就是当发生意外情况的时候，监控系统能将事件通知到人手上，毕竟人不可能24小时都在工作。这篇文章将要介绍的，就是第二个核心功能的承担着，告警组件。
项目背景
我们公司目前的监控系统采用的是TICK架构中的TI,即用Telegraf采集数据，Influxdb做存储。C我们用了更加流行的Grafana做了替换。至于我们为什么选择了Influxdb，可以参考这篇文章：InfluxDB与Prometheus用于监控系统上的对比
剩下K，即Kapacitor,我们最后抛弃了它，主要还是因为Kapacitor的太过于臃肿，上手和维护成本太高，很多功能我们都用不上，还不如自己开发一个。而Grafana的报警功能其实还可以，但是对于我们来说有个不大不小的缺陷，这里就不提了。于是自己心里先把实现思路过了一遍，觉得能Hold得住，就向领导请示想自己开发，接下来轮子就造起来了。
需求与设计原则
作为一个核心组件，我给自己先定了一个最基本的目标：稳定。功能多不多、炫不炫要让位给稳定性。
接下来开始思考告警组件的两个基本需求：通知，和异常事件的发现。
通知方面，邮件的方式是必不可少的。另外因为我们公司有自己的IM产品线，所以支持Webhook也是要留在考虑项里面。至于短信这些和客户的需求耦合度比较高，所以暂不考虑。
而异常事件的发现，我选择参考Kapacitor的方式，主动去DB做查询，拿到数据再做触发的判断。这种方式有个缺点，如果数据库挂了，那么数据的流入端就断了，这为系统的可用性增加了一个不确定性因素。但我们在Influxdb前面使用relay做了高可用网关，而在我们集群中relay也是高可用的，这可以抵消掉一些上面的不确定性因素。
但是反过来，如果参考Prometheus或者Open-Falcon的方式，在数据送入数据库之前，先经过告警组做判断，我们目前的监控系统就需要增加一个网关，或者将告警功能嵌入relay里面，这样一来相当于监控的数据流在进入DB前会经过两扇门，每一扇都会降低整个系统一定的吞吐量。还有一个点不得不考虑，对配置的每一次修改都需要重启程序，这势必会造成数据的丢失。为了解决这个问题，Prometheus和Open-Falcon都是将待进入DB的数据复制一份，导到告警组件这里来，而这又需要对采集组件的配合。所以基于上面基点的考虑，我就选择了主动去DB做查询的方式。
说完上面两个基本需求，还有一个定制化的需求也要考虑，我们希望能在我们的虚拟机管理平台上像主流的公有云厂商一样能够让用户配置告警的策略，所以这引入另外一个需求：对外暴露易操作的API，让前端妹子调用。
内部设计
明确了基本需求后，接下来开始内部的设计。为了理清思路，我写下了一份我（从使用者的角度）想要的配置文件（yaml格式）,来帮助我对事物进行抽象：
alert:
- name: 宿主机监控
type: influxdb
url: http://120.25.127.4:8086
db: telegraf
interval: 2s
query:
- name: cpu空闲值
sql: SELECT usage_idle FROM cpu WHERE time &amp;gt; now() - 1m
threshold: 100
op: &amp;quot;&amp;lt;=&amp;quot;
- name: 内存使用率
sql: SELECT used_percent FROM mem WHERE time &amp;gt; now() - 1m
op: &amp;quot;&amp;gt;=&amp;quot;
threshold: 50

notifier:
- name: 测试组
enable: true
type: mail 
host: smtp.163.com
port: 25
username: qq912293672@163.com
password: xxxxxx
from: qq912293672@163.com
to: [912293672@qq.com]

从配置文件上可以看到，我对告警组件抽象出了两个大的划分，一个是alert,抽象了数据的获取。一个是notifier,抽象了事件的通知。
在alert里面，我赋予了alert几个属性，其中type用来标识数据库的类型，因为我希望这个告警组件能支持多个存储后端。接下来是query，sql属性让用户自定义数据的查询方式，并且用threshold和op表示触发的阀值以及如何触发。总的来说，我采用了将多个查询实体组合成一个告警单位。这是经过思考的结果，目的是为了避免通知风暴：即很多机器很不幸都出异常的时，多个事件将聚合成一个告警，而不是发出多个邮件，而每个邮件的内容却很少。
而notifier的配置，我特意添加了type,也是为了支持多种通知方式，以及一个开关enable。
将notifier与alert分开，以及alert中包含query的设计，其实也是从Grafana和prometheus中学到的思路。在此感谢下今天的开源文化，让我等普通人有机会学到别人优秀的设计理念。
抽象得差不多后，可以考试编码了，按照分类，将代码主要分为3个模块，notify，alert,service。其中service对应我们上面的第三个需求，对外暴露API。
接口设计
开发语言上我使用的是Go，我们将会有多个query在执行，这刚好对上的Go的强项，并发。下面看下几个主要的接口：
// Executor :type Executor interface {
    Execute() ([]Result, error)
    Interval() time.Duration
    Config() Config
    Close() error
}

因为alert其实可以当做一个获取数据的执行单位，所以我在这里又抽象出了一个执行器Executor,接下来我们只需要让我们Alert实现该接口，就能被调用执行。
type Analyzer interface {
    Analyze(string, interface{}, QueryConfig) (Result, bool)
}

Analyzer接口实现对各个监控系统数据处理。
type Result interface {
    String() string
    QueryName() string}

Result接口抽象了监控系统的返回数据，屏蔽掉各个监控系统之间的数据差异。
type Notifier interface {
    Send(content string) error
    Name() string
    Type() string
    To() []string
    Enable() bool
    Config() *Config
}

通知接口
接下来我们需要实现一个调度器，实现了对上面Executor的调度和控制：
type Scheduler interface {
    Run()
    AddExecutor(executor.Executor)
    RemoveExecutor(name string)
    ExecutorExist(name string) bool
    Stop()
}

核心调度逻辑：
runFn := func(schItem *scheduleItem) {
        bf := bytes.NewBufferString(&amp;quot;&amp;quot;)
        ticker := time.NewTicker(schItem.executor.Interval())
        notiMap := make(map[string]int)
        mutex := &amp;amp;sync.RWMutex{}        for {            select {            // 定时器到期
            case &amp;lt;-ticker.C:
                results, err := schItem.executor.Execute()                if err != nil {
                    log.Error(err)                    continue
                }
                notifiers, err := adaper.ReadAllNotifier()                if err != nil {
                    log.Error(err)                    continue
                }                for _, result := range results {                    // 对通知数进行累加
                    mutex.Lock()
                    notiMap[result.QueryName()]++
                    mutex.Unlock()                    // 通知数已经超过了限制
                    log.Debugf(&amp;quot;notiMap:%+v&amp;quot;, notiMap)
                    result := result                    if notiMap[result.QueryName()] &amp;gt; notiSeqCount {                        if notiMap[result.QueryName()] == notiSeqCount+1 {                            go func(name string) {
                                time.Sleep(notiSleepDuration)
                                mutex.Lock()
                                notiMap[name] = 0
                                mutex.Unlock()
                            }(result.QueryName())
                        }                        continue
                    }                    if _, err := bf.WriteString(result.String()); err != nil {
                        log.Error(err)
                    }
                    bf.WriteString(&amp;quot;&amp;lt;br&amp;gt;&amp;quot;)
                }
                msgBody := bf.String()
                bf.Reset()                // 内容为空则跳过通知
                if msgBody == &amp;quot;&amp;quot; {                    continue
                }                // 遍历通知器将报警发送出去
                for _, notifier := range notifiers {
                    log.Debugf(&amp;quot;bool:%v&amp;quot;, notifier.Enable())                    if !notifier.Enable() {                        continue
                    }
                    notifier := notifier                    go func() {
                        err := notifier.Send(msgBody)                        if err != nil {
                            log.Errorf(&amp;quot;Send %s notify to %s fail:%s&amp;quot;, notifier.Type(), notifier.To(), err.Error())                            return
                        }
                        log.Infof(&amp;quot;Send %s notify to %s success&amp;quot;, notifier.Type(), notifier.To())
                    }()
                }            // 收到退出信号
            case &amp;lt;-schItem.closeCh:
                ticker.Stop()
                log.Infof(&amp;quot;Executor %s exit&amp;quot;, schItem.executor.Config().Name)                return
            }
        }
    }

上面的调度控制中，为了避免某个异常事件在短时间没有解决时，我实现了自己的一个控制逻辑：当同一个query的通知已经连续超过3次时，我会让它定制通知半小时。若半小时异常还继续，则再发三次通知给接受者，如此循环下去。
最后还有HTTP API的实现以及对数据的存储。这属于常规的开发逻辑，和我们这个告警组件的关系不是很大，就不一一介绍了。
总结
写这篇文章主要是总结下设计的思路，尤其是在几个核心问题上。在设计之初，除了告诉自己要保持住稳定性之外，还特别注意了如何对代码做到恰到好处的抽象，这也是最近半年看了那么多优秀开源项目的代码后的想法。老实说我这一次又对自己做得不满意，有机会我重构下整个组件。另外其实还有一个比较棘手的问题，就是如何让告警组件做到高可用（这无法通过简单部署多个服务就能实现，这样会导致通知事件的重复发送），这个问题最近正在解决，可以期待下一篇文章。
https://mp.weixin.qq.com/s/qr8WyroAWqx4D85J89RbvQ

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/how-to-design-monitor-platform">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/why-go-so-different">
                        Go语言究竟不一样在哪
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-13</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-info">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-primary">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                        <a href="https://lwhile.github.io/post/why-go-so-different" class="post-feature-image" style="background-image:url(https://lwhile.github.io/post-images/why-go-so-different.png) ">
                        </a>
                        
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            第一次听到Go语言是在什么时间已经想不起来了。
我是14年进入的大学，在15年面试校内一个技术组织时，被问了解哪些服务端语言。那时候我提到了Go。不过正式把它列入学习计划，还要等到16年的十月份。后来在17年三月份出来实习使用Go做开发，才有更多的机会将它投入实践中。
在Go语言之前，我学习了C，C++， Java，Python。Go语言是唯一一门我认为自己能算得上掌握的语言。
这门语言最与众不同的地方在于，将写并发型的代码变得异常容易。另一个能在简洁程度（指并发）上能和它一比的，大概只有Actor世界里的Erlang/Elixir了。
在继续介绍Go之前我觉得有必要提下它的老爹。不过我要说的不是谷歌(Go最开始是谷歌的内部项目)，而是计算机领域的那三尊神。
Ken Thompson
Rob Pike
Robert Griesemer
如果你知道C语言的作者叫做Dennis Ritchie，并且看过它的一张图片，就是这张：


你应该对这两位老头有印象，很多介绍C语言或者计算机导论的书都有这张照片。右边那位就是Dennis Ritchie，2011年他已经驾鹤西去，在他离世前的一个星期，地球上另一个传奇也走了，他叫做史蒂夫・乔布斯。那一年我刚考上高中，乔布斯走后凡客出了一件纪念衫和一本他的传记，我都买了，现在还留着。

左边那位，就是另外的一尊神，Ken Thompson。Unix知道吧？他最先设计和实现出来的（我说最初的几个Unix版本内核是汇编写的你信吗？）C语言的前身—— B语言，他弄的。Plan9，他参与了。正则表达式和UTF-8编码的设计，他也凑了一脚。在大牛这个称谓泛滥的今天，我只能用神这个字去形容我的这位偶像了。
UTF-8还有另外一个发明者，和Ken一样也是Unix的成员，他就是目前Go语言的实际控制者，Rob Pike。你不一定会写Go代码，但你一定对Go那个萌贱萌贱的Logo有印象，这个Logo就是由Rob Pike的老婆设计的。

最后一尊神，Robert Griesemer，手握几个个赫赫有名的项目，其中两个每天都会有一大波程序员它们打交道。这两个项目就是V8和JVM。Go 1.3之后改变了GC的工作机制，据说就是由他操刀。
三位大神介绍完了，最终我想说的是，Go多多少少会凝聚他们三位在编程方面的宝贵经验，当初也是因为这一点我才会坚定得学习它，虽然我知道可能毕业的时候找不到一份Go这样一门新兴语言的工作。
好了，题外话不继续了，接下来开始列举Go的一些特性。
1. goroutine
目前在并发编程领域，我们有3种最基本的模型选择。
第一种，即多进程/线程模型，这也是C/C++，Java最常用的并发模式。这种模型的好处是因为操作系统的原生支持，语言的实现者不用考虑其调度问题，交给操作系统就行了。但缺点就是会在并发数上升到一定程度后，系统需要将时间片更多花在进程/线程的上下文切换上。Linux默认的线程栈空间大小是1MB, 开一个1000个进程什么也不干，就需要接近1G的内存空间了。
第二种，是事件驱动机制，典型如Nginx和Node。我们都知道Nginx和Node扛并发的能力很强，但实际上占用的资源却极少，很大一部分将所有的事件通过一些特殊的数据结构（如红黑树）组织起来，等到事件发生的时候再放到一个单一的进程/线程去处理。这个模型只适合IO密集型的工作场景，因为事件驱动的本质只是充分利用起CPU的空闲时间。
第三种，就是Go使用的模型，协程模型，和Python，Lua，Erlang/Elixir里面的协程是同一个东西，只不过在Erlang/Elixir里面叫做process，Go里面叫做goroutine。我们知道线程比进程轻量，而协程则比线程更加轻量化。在Go里面，一个协程所占用的基本空间是2KB，只有线程的1/512。换句话说，起1000个空跑的goroutine，大概需要2MB的内存，而起1000个线程，则需要接近1GB。更低的内存占用，意味着可以开更多的并发单位，以及消耗更少的上下文切换时间。
可以看一个简单的例子：
func ShotOut() {
	// 休眠1秒钟
	time.Sleep(time.Second)

	// 向控制台抵茶
	fmt.Println(&amp;quot;给大佬抵茶&amp;quot;)
}

func main() {
	for i := 0; i &amp;lt; 5; i++ {
		// 起5个goroutine
		go ShotOut()
	}

	// 别让main退出，作用类似C里面的getchar()
	select {}
}

如果没有使用并发，那么执行这个程序需要5s，但实际执行时间会在1s左右，这证明我们调用ShotOut函数的时候确实是并发了。
如果 Go 只是像 Python 或者Lua那样简单得引入了协程，那么它绝对不可能有今天的地位。Go的开发团队最伟大的地方在于，他们赋予了Go的runtime调度goroutine的能力，就像Linux内核调度线程那样。正是这一点，才让我们编写并发代码变得更加简单。
2.channel
说完goroutine，还有一个与它配合使用特性叫channel，可以说这两个特性加起来就锻造成了Go的屠龙宝刀。
上面的例子中起了5个goroutine，这5个并发单位都比较简单，彼此之间不需要通信。如果需要呢？channel就是用于相互隔离间的并发单位进行通信的一个消息队列。
看一个简单的例子：
func main() {
	// 声明一个存放int类型的channel
	ch := make(chan int)

	go func() {
		// 休眠1秒钟
		time.Sleep(time.Second)

		// 向channel写入整数1
		ch &amp;lt;- 1
	}()

	go func() {
		// 等待从通道中取出内容
		res := &amp;lt;-ch
		fmt.Println(res) // 1s后输出1
	}()

	// 别让main退出，作用类似C里面的getchar()
	select {}
}

我们对最上面的例子改下需求，要求第一个goroutine执行完毕后，才能继续执行另外两个。第二波执行的这两个goroutine执行完毕后，才能执行最后剩下的2个gourotine。下面是Go的实现，大家可以想下如果用C++或者Java要如何写：
func ShotOut() {
	// 休眠1秒钟
	time.Sleep(time.Second)

	// 向控制台抵茶
	fmt.Println(&amp;quot;给大佬抵茶&amp;quot;)
}

func main() {
	// 声明两个存放int类型的channel
	ch1 := make(chan int)
	ch2 := make(chan int)

	go func() {
		// 休眠1秒钟
		time.Sleep(time.Second)

		// 第1次跑抵茶函数
		ShotOut()

		// 抵完第1杯茶，向channel写入整数1
		ch1 &amp;lt;- 1
	}()

	go func() {
		// 等待第1杯茶递完的信号
		&amp;lt;-ch1

		// 收到信号，开始抵第2和递3杯茶
		ShotOut()
		ShotOut()

		// 通知最后2个
		ch2 &amp;lt;- 1
	}()

	go func() {
		// 等待第2和第3杯茶递完
		&amp;lt;-ch2

		// 递最后两杯
		ShotOut()
		ShotOut()
	}()

	// 别让main退出，作用类似C里面的getchar()
	select {}
}


注意加了go关键字在面前的函数都会以并发的方式进行

3. 无依赖运行
如果说上面那两个特性要在并发环境下才能体现用处，那么Go可以编译成一个完成无依赖的二进制这一功能，是真的能大大提升你编程的意愿：因为你写的东西很容易传播给别人。
就拿能向浏览器输出Hello World的Web程序来举例子好了
pakcage main 

import &amp;quot;fmt&amp;quot;
import &amp;quot;net/http&amp;quot;

func HelloWorld(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte(&amp;quot;Hello World&amp;quot;))
}

func main() {
	http.HandleFunc(&amp;quot;/&amp;quot;, HelloWorld)
	http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}

是的，你没看错，就这简单几句代码，跑起来它就是一个Web服务。而且通过 go build 命令编译后，将生成的二进制直接丢到服务器上就能跑了，Linux，Mac OS，Windows三平台都能用，而且性能非常强！想想如果是Java或者Python, Go的程序已经丢到服务器在跑了，Java的进度条可能还处在配置Tomcat上，Python则还在配置gunico和supervisor...
最后
上面提到的三个特性，我觉得已经够向没了解过Go的用户介绍清楚他最与众不同的地方了。还有其他几个特性，比如非侵入性的接口，抛弃面相对象模型，多返回值，闭包等，这些相比其他语言我倒觉得不是其最大的亮点，留到以后有时间再介绍吧，这篇文章就已经写了我两个晚上了...

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/why-go-so-different">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/etcd-watch-source-code">
                        Etcd watch源码阅读
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-10</time>
                    
                        <a href="https://lwhile.github.io/tag/iYb5jfiTB" class="post-tag i-tag
                            i-tag-other_1">
            #etcd
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_2">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            公司的业务里面使用了Consul做服务发现, 发现其有一个watch机制.这个watch机制引起我的好奇, 因为刚好在看Etcd-raft的代码, Etcd也有类似的watch机制, 所以趁热打铁, 赶紧趁周末研究下etcd watch机制源码的实现.
在看源码之前, 我们通过一个简单的例子, 看看Etcd的watch是如何使用的.

先往Etcd写入一对KV


curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=&amp;quot;神蛋使者&amp;quot;


Watch这对KV


curl http://127.0.0.1:2379/v2/keys/name?wait=true

如果一切正常, 这时候请求会被阻塞住.

新开一个终端, 修改存进去的KV


curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=神蛋使者1号


阻塞的那个请求返回watch到的结果

{
  &amp;quot;action&amp;quot;:&amp;quot;set&amp;quot;,
  &amp;quot;node&amp;quot;:{ 
      &amp;quot;key&amp;quot;:&amp;quot;/name&amp;quot;,
      &amp;quot;value&amp;quot;:&amp;quot;神蛋使者1号&amp;quot;,
      &amp;quot;modifiedIndex&amp;quot;:25,
     &amp;quot;createdIndex&amp;quot;:25
  },
   &amp;quot;prevNode&amp;quot;: {
     &amp;quot;key&amp;quot;:&amp;quot;/name&amp;quot;,
     &amp;quot;value&amp;quot;:&amp;quot;神蛋使者&amp;quot;,
     &amp;quot;modifiedIndex&amp;quot;:24,
     &amp;quot;createdIndex&amp;quot;:24
   }
  }

体验流程大概就是这样, 下面正式看源码.
接口定义
type Watcher interface {
	// Watch watches on a key or prefix. The watched events will be returned
	// through the returned channel.
	// If the watch is slow or the required rev is compacted, the watch request
	// might be canceled from the server-side and the chan will be closed.
	// &#39;opts&#39; can be: &#39;WithRev&#39; and/or &#39;WithPrefix&#39;.
	Watch(ctx context.Context, key string, opts ...OpOption) WatchChan

	// Close closes the watcher and cancels all watch requests.
	Close() error
}

该接口定义了两个方法, Watch 和 Close
Watch 方法返回一个WatchChan 类似的变量, WatchChan是一个channel, 定义如下:
type WatchChan &amp;lt;-chan WatchResponse

该通道传递WatchResponse类型
type WatchResponse struct {
	Header pb.ResponseHeader
	Events []*Event

	// CompactRevision is the minimum revision the watcher may receive.
	CompactRevision int64

	// Canceled is used to indicate watch failure.
	// If the watch failed and the stream was about to close, before the channel is closed,
	// the channel sends a final response that has Canceled set to true with a non-nil Err().
	Canceled bool

	// Created is used to indicate the creation of the watcher.
	Created bool

	closeErr error
}

其中Event类型是一个gRPC生成的消息对象
type Event struct {
	// type is the kind of event. If type is a PUT, it indicates
	// new data has been stored to the key. If type is a DELETE,
	// it indicates the key was deleted.
	Type Event_EventType `protobuf:&amp;quot;varint,1,opt,name=type,proto3,enum=mvccpb.Event_EventType&amp;quot; json:&amp;quot;type,omitempty&amp;quot;`
	// kv holds the KeyValue for the event.
	// A PUT event contains current kv pair.
	// A PUT event with kv.Version=1 indicates the creation of a key.
	// A DELETE/EXPIRE event contains the deleted key with
	// its modification revision set to the revision of deletion.
	Kv *KeyValue `protobuf:&amp;quot;bytes,2,opt,name=kv&amp;quot; json:&amp;quot;kv,omitempty&amp;quot;`
	// prev_kv holds the key-value pair before the event happens.
	PrevKv *KeyValue `protobuf:&amp;quot;bytes,3,opt,name=prev_kv,json=prevKv&amp;quot; json:&amp;quot;prev_kv,omitempty&amp;quot;`
}

接下来看实现了Watcher接口的watcher类型
// watcher implements the Watcher interface
type watcher struct {
	remote pb.WatchClient

	// mu protects the grpc streams map
	mu sync.RWMutex

	// streams holds all the active grpc streams keyed by ctx value.
	streams map[string]*watchGrpcStream
}

watcher结构很简单, 只有3个字段. remote抽象了发起watch请求的客户端, streams是一个map, 这个map映射了交互的数据流.还有一个保护并发环境下数据流读写安全的读写锁.
streams所属的watchGrpcStream类型抽象了所有交互的数据, 它的结构定义如下:
type watchGrpcStream struct {
	owner  *watcher
	remote pb.WatchClient

	// ctx controls internal remote.Watch requests
	ctx context.Context
	// ctxKey is the key used when looking up this stream&#39;s context
	ctxKey string
	cancel context.CancelFunc

	// substreams holds all active watchers on this grpc stream
	substreams map[int64]*watcherStream
	// resuming holds all resuming watchers on this grpc stream
	resuming []*watcherStream

	// reqc sends a watch request from Watch() to the main goroutine
	reqc chan *watchRequest
	// respc receives data from the watch client
	respc chan *pb.WatchResponse
	// donec closes to broadcast shutdown
	donec chan struct{}
	// errc transmits errors from grpc Recv to the watch stream reconn logic
	errc chan error
	// closingc gets the watcherStream of closing watchers
	closingc chan *watcherStream
	// wg is Done when all substream goroutines have exited
	wg sync.WaitGroup

	// resumec closes to signal that all substreams should begin resuming
	resumec chan struct{}
	// closeErr is the error that closed the watch stream
	closeErr error
}

比较有意思的是, watchGrpcStream也包含了一个watcher类型的owner字段, watcher和watchGrpcStream可以互相引用到对方.同时又定义了watcher类型中已经定义过的remote,而且还不是指针类型, 这点不大明白作用是啥.
还有几个字段值得关注, 一个是substreams, 看下它的定义和注释:
// substreams holds all active watchers on this grpc stream
substreams map[int64]*watcherStream

再看看watcherStream类型的定义:
// watcherStream represents a registered watcher
type watcherStream struct {
	// initReq is the request that initiated this request
	initReq watchRequest

	// outc publishes watch responses to subscriber
	outc chan WatchResponse
	// recvc buffers watch responses before publishing
	recvc chan *WatchResponse
	// donec closes when the watcherStream goroutine stops.
	donec chan struct{}
	// closing is set to true when stream should be scheduled to shutdown.
	closing bool
	// id is the registered watch id on the grpc stream
	id int64

	// buf holds all events received from etcd but not yet consumed by the client
	buf []*WatchResponse
}

画个图整理下他们之间的关系:

接下来轮到watcher是如何watch方法的了:
// Watch posts a watch request to run() and waits for a new watcher channel
func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan {
	// 应用配置
	ow := opWatch(key, opts...)

	var filters []pb.WatchCreateRequest_FilterType
	if ow.filterPut {
		filters = append(filters, pb.WatchCreateRequest_NOPUT)
	}
	if ow.filterDelete {
		filters = append(filters, pb.WatchCreateRequest_NODELETE)
	}

	// 根据传入的参数构造watch请求
	wr := &amp;amp;watchRequest{
		ctx:            ctx,
		createdNotify:  ow.createdNotify,
		key:            string(ow.key),
		end:            string(ow.end),
		rev:            ow.rev,
		progressNotify: ow.progressNotify,
		filters:        filters,
		prevKV:         ow.prevKV,
		retc:           make(chan chan WatchResponse, 1),
	}

	ok := false
	// 将请求上下文格式化为字符串
	ctxKey := fmt.Sprintf(&amp;quot;%v&amp;quot;, ctx)

	// find or allocate appropriate grpc watch stream
	// 接下来配置对应的输出流, 注意得加锁
	w.mu.Lock()

	// 如果stream为空, 返回一个已经关闭的channel.
	// 这种情况应该是防止streams为空的情况
	if w.streams == nil {
		// closed
		w.mu.Unlock()
		ch := make(chan WatchResponse)
		close(ch)
		return ch
	}

	// 注意这里, 前面我们提到streams是一个map,该map的key是请求上下文
	// 如果该请求对应的流为空,则新建
	wgs := w.streams[ctxKey]
	if wgs == nil {
		wgs = w.newWatcherGrpcStream(ctx)
		w.streams[ctxKey] = wgs
	}
	donec := wgs.donec
	reqc := wgs.reqc
	w.mu.Unlock()

	// couldn&#39;t create channel; return closed channel
        // couldn&#39;t create channel; return closed channel
	// 这里要设置为缓冲的原因可能与下面的两个
	// closeCh &amp;lt;- WatchResponse{closeErr: wgs.closeErr}
	// 语句有关,这里不理解
	closeCh := make(chan WatchResponse, 1)

	// submit request
	select {
	// 发送上面构造好的watch请求给对应的流
	case reqc &amp;lt;- wr:
		ok = true
	// 请求断开(这里应该囊括了客户端请求断开的所有情况)
	case &amp;lt;-wr.ctx.Done():
	// watch完成
	// 这里应该是处理非正常完成的情况
	// 注意下面的重试逻辑
	case &amp;lt;-donec:
		if wgs.closeErr != nil {
			// 如果不是空上下文导致流被丢弃的情况
			// 则不应该重试
			closeCh &amp;lt;- WatchResponse{closeErr: wgs.closeErr}
			break
		}
		// retry; may have dropped stream from no ctxs
		return w.Watch(ctx, key, opts...)
	}

	// receive channel
	// 如果是初始请求顺利发送才会执行这里
	if ok {
		select {
		case ret := &amp;lt;-wr.retc:
			return ret
		case &amp;lt;-ctx.Done():
		case &amp;lt;-donec:
			if wgs.closeErr != nil {
				closeCh &amp;lt;- WatchResponse{closeErr: wgs.closeErr}
				break
			}
			// retry; may have dropped stream from no ctxs
			return w.Watch(ctx, key, opts...)
		}
	}

	close(closeCh)
	return closeCh
}

还有Watcher接口的另一个方法Close:
func (w *watcher) Close() (err error) {
	// 在锁内先将streams字段置为空
	// 在锁外再将一个个流都关闭
	// 这样做的意义在于不管哪个流关闭失败了
	// 都能先保证streams与这些流的关系被切断
	w.mu.Lock()
	streams := w.streams
	w.streams = nil
	w.mu.Unlock()
	for _, wgs := range streams {
		if werr := wgs.Close(); werr != nil {
			err = werr
		}
	}
	// etcd竟然也只是返回一个error
	// 虽然上面的for循环可能产生多个error
	return err
}

这样watcher就实现了Watcher接口.大致的实现思路本文就介绍到这里,剩下的代码也都是对其他相关数据结构的逻辑包装操作.
简单阅读Etcd的这一小部分源码下来, 我看到他们源码中的两个东西,算是Golang或者编程上面的一些最佳实践:


对包外只暴露一个公共接口, 包内的结构体实现该接口即可.就像本文中的Watcher接口和watcher结构体.这样有两个好处, 一个就是代码能够解耦,还有就是可以省去命名的苦恼(__)


另一个是注释的书写方式,我发现etcd源码里的注释很大一部分写在变量的定义上面,而且变量的定义名都很清晰.


抽象得体.这个其实不只是Etcd, 其他任何优秀的开源作品都把他们的代码抽象得很到位.突然想起我写的那些渣渣代码%&amp;gt;_&amp;lt;%


最后, 总结下etcd的watch机制.其实归根结底, 它的watch是通过gRPC的多路复用实现的,这是一个基于HTTP/2的特性.所以本文可能有些偏离了主题,探讨Etcd的watch机制, 其实应该研究HTTP/2才是.
算是给自己挖个坑.

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/etcd-watch-source-code">Read More ~</a>
                            </div>
                </div>
            </article>
            
                <!-- 翻页 -->
                
                </div>
                <!--  -->
                <div class="main-container-middle"></div>
                <!--  -->
                <div id="sidebar" class="main-container-right">

                    <!-- 个人信息 -->
                    
    <div class="id_card i-card">
        <div class="id_card-avatar" style="background-image: url(https://lwhile.github.io/images/avatar.png?v=1576378575617)">
        </div>
        <h1 class="id_card-title">
            神蛋杂谈
        </h1>
        <h2 class="id_card-description">
            
        </h2>
        <!--  -->
        <div class="id_card-sns">
            <!-- github -->
            
                    <!-- twitter -->
                    
                            <!-- weibo -->
                            
                                    <!-- facebook -->
                                    

        </div>
    </div>
    

                        <!-- 公告栏 -->
                        

                </div>
            </div>



            <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://lwhile.github.io/atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
    <script>
        $('#sidebar').stickySidebar({
            topSpacing: 80,
            // bottomSpacing: 60
        });
    </script>
</body>

</html>