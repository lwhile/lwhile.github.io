<html>

<head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>
    计算机 | 神蛋杂谈
</title>
<link rel="shortcut icon" href="https://lwhile.github.io/favicon.ico?v=1576341799975">
<!-- <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous"> -->
<link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://lwhile.github.io/styles/main.css">
<!-- js -->
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
<script src="https://lwhile.github.io/media/js/jquery.sticky-sidebar.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/moment.js/2.23.0/moment.min.js"></script>


</head>

<body>
    <div class="main">
        <div class="header">
    <div class="nav">
        <div class="logo">
            <a href="https://lwhile.github.io">
                <img class="avatar" src="https://lwhile.github.io/images/avatar.png?v=1576341799975" alt="">
            </a>
            <div class="site-title">
                <h1>
                    神蛋杂谈
                </h1>
            </div>
        </div>
        <span class="menu-btn fa fa-align-justify"></span>
        <div class="menu-container">
            <ul>
                
                    
                            <li>
                                <a href="/" class="menu">
                                    首页
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/archives" class="menu">
                                    归档
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/tags" class="menu">
                                    标签
                                </a>
                            </li>
                            
                                
                    
                            <li>
                                <a href="/post/about" class="menu">
                                    关于
                                </a>
                            </li>
                            
                                
            </ul>
        </div>
    </div>
</div>

<script>
    $(document).ready(function() {
        $(".menu-btn").click(function() {
            $(".menu-container").slideToggle();
        });
        $(window).resize(function() {

            if (window.matchMedia('(min-width: 960px)').matches) {
                $(".menu-container").css('display', 'block')
            } else {
                $(".menu-container").css('display', 'none')
            }

        });
    });
</script>

            <div id="main-content" class="post-container main-container">
                <div id="content" class="main-container-left">
                    
    <div class="i-card">
        <b>标签：#
        计算机</b>
    </div>
    
        
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/go-timezone">
                        Go 时区用法总结
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2019-02-19</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_3">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-banana">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            坊间有个说法，说中美两国的程序员除了薪资、加班强度外，还有个比较有趣的差异，就是在编码时对于时区的敏感程度也不同。
「大部分」美国程序员在时区问题上很有经验，因为美国在日常生活中会使用到 3 个时区。而中国「大部分」程序员在编码时基本不会考虑时区问题，因为大家都在东八区，没那个烦恼。
在过去一年我维护着一个公司内部的日历库，用于做交易时间的计算。在这个过程中被时区问题折磨得很难受，不过也积累一些 Go 处理时区上的经验。
时区配置
时区的配置一定要声明在配置文件中，比如下面这种 yaml 格式：

timezone: Asia/Hong_Kong

别想着用机器时区一劳永逸这个问题，因为机器的变数太多了。
载入时区
在 Go 中，时区被封装在 time.Location 中进行抽象，而载入时区有两种方法

通过 IANA 数据库

给 time.LoadLocation 传入一个符合 IANA 时区数据库的时区名字，比如：

Asia/Hong_Kong


通过偏移量

time.FixedZone(&amp;quot;UTC-8&amp;quot;, -8 **60 **60)
转换时区
比较简单的操作，直接调一个 time struct instance 的 In(*Location) 方法。
修改时区
注意修改时区和转换时区是两种不同的概念。
比如北京时间 20:00pm 转换时区，以美东时区为例，会是 7:00am (冬令时)
而修改时区是指把北京 20:00pm 修改为美东 20:00pm，可以这样操作：

// Pseudo code
var estTime time.Time
var cstTime time.Time
var estTimezone *time.Location
// 通过 time.Date 方法重新生成一个 time struct instance
estTime := time.Date(cstTime.Year(), cstTime.Month(),..., estTimezone)

Round to day
实在想不起一个名词可以形容 Round to day 的过程，我们用这个方法来做日内时间的比较。
比较典型的应用场景是判断是否到了开市/休市时间。由于在 Go 中 time 类型是包含有年、月信息的，所以要表达「每天早上 9:30」 这个时间不大好做。
既然不好做，那就把它们去掉。

// Pseudo code
func RoundToDay(t time.Time) time.Time {
// 注意在 Go 中 zero time 并不是 0 year 0 month 0 day，而是 1，所以用 0 的话会溢出喔
return time.Date(1,1,t.Day(), t.Hour() ...)
}

LMT
LMT 这个问题是在 Round to day 上面引发出来的，说起来也很有趣。LMT 本意是指 local mean time（地方平时，在指定的经度范围内使用一致时间的地方太阳时）。
如果你用试图在 Go 里面用香港时区构建一个 zero time 的话，就会发现最后返回出来的 time struct instance 的 location 信息是 LMT。这个问题是在一处单元测试中发现的，死活无法通过测试（嗯，单测真的很重要）。
不过这是为什么？
我相信知道这个知识点的人绝对少之又少，这是一个非常非常冷的知识（最后通过我那位知识储备量非常丰富的同事知道的这个）

香港時間的授時服務，是香港天文台從1883年成立至今的主要職責。早期香港天文台使用赤道儀及中星儀，透過觀測星象測量時間。當時香港時間是當地平均時間（LMT）UTC+7:36:42（準確值為UTC+7:36:41.8842）。香港天文台最早於1885年1月1日對公眾授時[1]，當年香港天文台於九龍尖沙咀警署設置桅杆，以升降時間球的方式對外發布時間。1885年1月1日，香港天文台於中午12時50分把時間球升到桅杆頂端，然後於下午1時正，首次把時間球降下，成為香港首次報時訊號，並作為香港時間的標準。用於報時的時間球訊號塔，後來於1933年因為電台報時的開展而拆除。1904年10月30日，香港時間正式確定為格林威治標準時間快8小時（GMT+8）[2]。2004年，天文台安裝了一套高準確度授時系統，利用全球定位系統共視方法，向國際度量衡局提供天文台的原子鐘時間數據，參與訂定協調世界時。天文台亦根據國際度量衡局提供的時間數據調校原子鐘，使其準確度保持在一百萬分之一秒以內。現時，香港天文台以銫原子鐘報時系統作為香港時間的標準，誤差僅為每日1微秒之內。香港天文台設有互聯網時間伺服器，為互聯網的用戶提供準確的時間校正服務[3]。

根据上面的记录来看几个例子吧，首先是 1904 年 10 月 30 日后香港使用 GMT+8，我们把时间定格在它前一秒：

location, _ := time.LoadLocation(&amp;quot;Asia/Hong_Kong&amp;quot;)

t := time.Date(1904,10,29,11,59,59,59,location)
fmt.Println(t) // 1904-10-29 11:59:59.000000059 +0736 LMT &amp;lt;- LMT 无疑
然后是 1904 年 10 月 30 日 零点：

location, _ := time.LoadLocation(&amp;quot;Asia/Hong_Kong&amp;quot;)

t := time.Date(1904,10,30,0,0,0,0,location)
fmt.Println(t) // 1904-10-30 00:23:18 +0800 HKT &amp;lt;- HKT 了。
厉害吧。。。还有更厉害的。。。

location, _ := time.LoadLocation(&amp;quot;Asia/Hong_Kong&amp;quot;)

t := time.Date(1942,10,30,0,0,0,0,location)
fmt.Println(t) // 1942-10-30 00:00:00 +0900 JST
用 1942 年 10 月 30 日构建一个香港时间，时区会是（JST）日本标准时间。Why?
原因是这个：

香港日佔時期，又稱為香港日治時期或香港淪陷時期，是指第二次世界大戰時大日本帝國軍事占領香港的時期：由1941年12月25日香港總督楊慕琦投降起，至1945年8月15日日本無條件投降為止；香港人俗稱這段時期為「三年零八個月」。

别以为 Go 的开发团队这么有心，实际上 Go 也是读操作系统的 zoneinfo 文件。只能说在某个层面上操作系统也记录着每个地区的历史。。。

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/go-timezone">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/exponential-backoff">
                        指数退避
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2018-04-02</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_4">
            #计算机
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            这是加入新公司后的第一篇技术博文，主角是一种叫做指数退避的算法。还是按照以往的惯例，这篇文章会从问题的发生至问题的解决记一个流水账。
问题的场景发生业务系统中的 RPC 调用中，需求是希望如果一个 RPC 调用失败了，能够进行重试。
作为一个菜鸟，我很快写入了第一个版本，用 Go 语言描述出来的逻辑如下：
func retry(maxRetry int, f func() error) {
    for i:=0;i&amp;lt;maxRetry;i++ {
        err := f()
        if err == nil {
          return 
        }
    }
}

很快这种代码在 review 的时候就被打下来了。理由是作为网络调用，这种重试机制是不合理的。试想以下如果是遇到机器重启或者网络抖动，对方的服务要在 1 分钟甚至更久之后才能恢复正常，那么将重试放在一个没有停歇的 for 循环里面，试错的机会很快就会被用完。
mentor 提示我可以试下指数退避算法（exponential backoff），网上看了一些算法的介绍，很快就写了第二个版本出来。
逻辑还是差不多，只不过多了一步休眠的操作：
func retry(maxRetry int, f func() error) {
    for i:=0;i&amp;lt;maxRetry;i++ {
        err := f()
        if err == nil {
          return 
        }
        // calculate the dynamic duration
        dur := call()

        time.Sleep(dur)
    }
}

算法的核心在于计算出重试的间隔，如果重试失败，那么就要相应得延长下次重试的时间。
重试间隔计算公式如下：
sleep = min(cap, base * 2 ** attempt)
随着重试次数 attempt 的增加，sleep 的数值会出现指数级的变化。
对于一些服务比如涉及到事务的数据库操作，如果有多个 client 同时使用上面这条公式的话，会出现请求挤压的情况导致系统的整体吞吐量下降，这时候我们可以引入一个随机因子避免该问题，经过修改的公式如下：
sleep = random_between(0, min(cap, base * 2 ** attempt))
如果你使用的是Go，推荐下这个库：https://github.com/cenkalti/backoff

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/exponential-backoff">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/raft-sharing">
                        公司 raft 算法分享会内容
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-12-13</time>
                    
                        <a href="https://lwhile.github.io/tag/fen-bu-shi-xi-tong" class="post-tag i-tag
                            i-tag-banana">
            #分布式系统
        </a>
                        
                        <a href="https://lwhile.github.io/tag/raft" class="post-tag i-tag
                            i-tag-other_2">
            #raft
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_3">
            #计算机
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            今天在团队内部做了一次分享，给组内的各位大神介绍了raft算法。老实说讲的内容中还是有些欠缺，主要是在内容深度上不够，只能按照论文上的内容讲，没有一些自己的提炼，再多多摸索吧。
这次会议我没有做PPT，只是打了一个文字稿，然后在白板上讲。下面贴的内容是按照文字稿复制过来的，当然和会议上实际讲的所有出入，就当做给没有听说过raft算法的朋友当做一个入门介绍吧。正儿八经学习还是得找其他资源，比如论文原文。
-----------------------分割线---------------------------
我会去了解raft是因为在来公司实习，尤其是接触到prometheus之后，就去网上搜了一下学习分布式系统要看哪些东西，然后就搜出了raft。老实说我最开始是一点都看不懂它在讲什么的，而且也看不到它的实际作用，感觉就是没必要整出这么一个东西来。我打印出来的论文也一直放在屋里吃灰。
变化是在体验到influxdb-relay和influxdb集群版的差别后，我开始渐渐意识到一致性算法对于分布式系统的重要性。relay它只是简单得将客户端的数据转发给多个influxdb实例，至于每个实例成不成功，它没法保证。后来我试用了InfluxDB带有集群功能的企业版，那个确实就是我们想要的功能，往任何一个节点写入任何一条数据，只要服务器告诉我写成功了，无论接下来这个集群发生了怎样极端的故障, 我还是基本能相信这个数据是没问题的.而Influxdb的集群版就正是使用了raft算法保证了这种一致性，并且他们用的还是和consul同样的库。这时候我再去学raft就和几个月前的感受不大一样了。
raft的论文是斯坦福一个计算机博士在13年发的,而另外一个一致性算法paxos是1990发的.对比下发现最近几年出来的项目很多都是用了raft而不是paxos,像etcd,consul,tidb这些都是用raft,而ceph用的是paxos,ceph查了下是10年开始的项目.所以还是能看出raft与同类比起来的一些优点的,应该确实是比较容易学.
然后接下来我就讲下raft它的算法过程,其实我也只是看了几遍论文,很多东西我的理解也不是很通透,这个要真正拿去实践下才能做得到.我就大概按照论文的内容讲,会省略掉一些内容.
我们先假设一个场景, 就拿consul来举例, 假设我们现在有一个3节点的consul集群,nodeA,nodeB,nodeC.这3个节点都是运行在consul的server模式下.我们看下raft会怎样处理这3个节点.
在raft里面, 有3种角色, 领导者, 候选者,跟随者.
领导者的负责管理集群,并处理来自用户的请求.
跟随者是领导者的一个副本, 它会在领导者出现故障的时候顶替它.
而从跟随者变成领导者,有一个中间状态,那就是候选者.
以我们上面的3个consul节点来做例子.我们先启动了3个consul,这时候他们都处于跟随者的状态,这也是raft算法中节点启动时的初始状态.跟随者启动之后,他们会等待一段时间,如果这个时间内没有领导者发心跳包给它,那么跟随者就会进入候选者的状态.进入选举阶段,这也是raft算法中3个子问题中的第一个.
如何选举的呢?每个候选者手中都有一张票, 这张牌可以投给集群任意一个节点.比如nodeA可以投给nodeB,nodeC,当然也可以投给自己.只要一个节点能够拿到大多数节点的选票,那么它就当选为leader.比如,nodeA拿到了nodeB,nodeC两张票,那么它就是leader
但是实际上raft的算法的选举没有这么简单,因为这种方法有一个问题,那就是选票可能会被瓜分掉.比如nodeA,nodeB,nodeC,他们分别把票都投给了自己,或者A投给B,B投给C,C投给A,这样谁也当不成leader.
raft是怎么解决这个问题的呢, 它引入两个概念,一个是任期,一个是选举超时时间.任期就是把时间轴切成一个个可以被编号的时间段.比如,我们上面3个节点启动的时候,都会进入第一个任期.而选举的超时时间是定义每个节点等待投票信息的最长时间的
还是拿我们上面的三个节点举例子.A,B,C启动,进入跟随者状态,接下来等不到领导者的心跳过来,都进入候选者状态,开始请求选举leader,这时候任期是1.raft会给每个节点在一个范围内随机设置一个超时时间,比如A是10ms,B是20ms,C是30ms.如果A在10ms内当不成leader, 那么它就给自己的任期号加1,变成任期2,重新开始投票.这时候B和C还是任期1.而且raft有个限制,就是会拒绝来自任期号低于自己的节点的选票,.比如这时候只能A投给B和C, B和C如果投给A那么会被A拒绝掉.并且节点在通讯的时候发现有比自己的任期号更高的,它会也马上提升自己的任期.用这种方法几轮下来,是一定可以选出一个leader的.
上面讲的这个选举问题,是raft三个子问题中的的第一个,领导选举.接下来我们看第二个问题,日志复制.
当leader被选举出来后,我们就假设A当上了leader.集群开始能够处理用户的请求.我们可以把用户的每一个请求都当成是一个指令,leader的一个重要任务,就是要将这些指令复制到集群的其他节点去.raft里面用日志这个词来描述这样一种指令,日志里面会记录一个任期号,一个日志的编号,还有执行的指令.Raft会维护日志,让他们能够满足下面的两个很重要的特性:
1. 如果两个日志条目拥有相同的日志编号和任期编号,那么两个日志就存储了相同的指令
2. 如果两个日志条目拥有相同的日志编号和任期编号,那么他们前面的日志所有日志都会相同
leader会决定什么时候可以应用日志中的指令,这种可以被应用的日志的状态在raft里面被描述为commited,所有commited的日志都会持久化存储.一个日志怎样才能算是commited呢.raft里面的定义就是,leader已经能确认这个日志已经被复制到大多数节点上面.
它的流程是这样的:比如我们有一个请求是要存一个kv到consul里面,请求到了leader nodeA这里,nodeA会将包含请求中的指令的日志发送到nodeB和nodeC上面.nodeA会等待他们返回的结果, 如果写入的节点能够超过半数,那么nodeA才会告诉客户端请求是成功的.
如果leader永远正常运行,那么理论上那些能够正常工作的副本的日志是会和leader保持一致的.但现实情况是leader总会有崩溃的那一天.比如NodeA在复制日志给B和C的过程中,B复制好了,但C还没复制好时nodeA就崩溃了,这种情况就会导致集群中的状态不一致,更加极端的是leader可能会在短时间内更换多次,这种情况下,集群内的日志会更加混乱.
(画图)
前面说过一个合格的一致性算法,只要leader告诉我们请求是ok的,那么不管发生什么极端的情况,任何时候任何节点的数据应该都是ok,但是像上面上面这种情况完全是有可能出现的,raft是如何保证数据的安全性呢?接下来就是raft的第三个子问题,安全性.
安全性一:
选举限制:
raft在选择leader的时候还有一个限制,就是这个候选人在选举时要联系集群中的大部分节点,验证这个候选者是不是拥有所有已经提交过的日志.如果没有,那么它就不可能当上leader.
raft的比较规则是这样的:
如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。
安全性二:
对提交旧日志进行限制,在提交当前term的日志之前,不准leader提交当前任期之前的任何日志.
-------------------------------------分割线--------------------------
准备的草稿只有这些了，昨晚准备到12点多，挡不住困意只能作罢，第二天再临场讲了一些内容。
留张照片做个纪念，以后不知道还会不会有。

https://mp.weixin.qq.com/s/4YXcMMvkk2EJGib_URGhLw

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/raft-sharing">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/how-to-design-monitor-platform">
                        如何设计监控平台的告警组件
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-20</time>
                    
                        <a href="https://lwhile.github.io/tag/jian-kong-xi-tong" class="post-tag i-tag
                            i-tag-banana">
            #监控系统
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-error">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-other_4">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            当业务发展到一定程度的时候，开发人员会开始考虑在系统中引入监控系统来对系统/业务进行监控。绝大多数监控系统都有两大核心功能，一个是工程师通过这个监控系统，能够对整个系统的运行情况一目了然，另外一个，就是当发生意外情况的时候，监控系统能将事件通知到人手上，毕竟人不可能24小时都在工作。这篇文章将要介绍的，就是第二个核心功能的承担着，告警组件。
项目背景
我们公司目前的监控系统采用的是TICK架构中的TI,即用Telegraf采集数据，Influxdb做存储。C我们用了更加流行的Grafana做了替换。至于我们为什么选择了Influxdb，可以参考这篇文章：InfluxDB与Prometheus用于监控系统上的对比
剩下K，即Kapacitor,我们最后抛弃了它，主要还是因为Kapacitor的太过于臃肿，上手和维护成本太高，很多功能我们都用不上，还不如自己开发一个。而Grafana的报警功能其实还可以，但是对于我们来说有个不大不小的缺陷，这里就不提了。于是自己心里先把实现思路过了一遍，觉得能Hold得住，就向领导请示想自己开发，接下来轮子就造起来了。
需求与设计原则
作为一个核心组件，我给自己先定了一个最基本的目标：稳定。功能多不多、炫不炫要让位给稳定性。
接下来开始思考告警组件的两个基本需求：通知，和异常事件的发现。
通知方面，邮件的方式是必不可少的。另外因为我们公司有自己的IM产品线，所以支持Webhook也是要留在考虑项里面。至于短信这些和客户的需求耦合度比较高，所以暂不考虑。
而异常事件的发现，我选择参考Kapacitor的方式，主动去DB做查询，拿到数据再做触发的判断。这种方式有个缺点，如果数据库挂了，那么数据的流入端就断了，这为系统的可用性增加了一个不确定性因素。但我们在Influxdb前面使用relay做了高可用网关，而在我们集群中relay也是高可用的，这可以抵消掉一些上面的不确定性因素。
但是反过来，如果参考Prometheus或者Open-Falcon的方式，在数据送入数据库之前，先经过告警组做判断，我们目前的监控系统就需要增加一个网关，或者将告警功能嵌入relay里面，这样一来相当于监控的数据流在进入DB前会经过两扇门，每一扇都会降低整个系统一定的吞吐量。还有一个点不得不考虑，对配置的每一次修改都需要重启程序，这势必会造成数据的丢失。为了解决这个问题，Prometheus和Open-Falcon都是将待进入DB的数据复制一份，导到告警组件这里来，而这又需要对采集组件的配合。所以基于上面基点的考虑，我就选择了主动去DB做查询的方式。
说完上面两个基本需求，还有一个定制化的需求也要考虑，我们希望能在我们的虚拟机管理平台上像主流的公有云厂商一样能够让用户配置告警的策略，所以这引入另外一个需求：对外暴露易操作的API，让前端妹子调用。
内部设计
明确了基本需求后，接下来开始内部的设计。为了理清思路，我写下了一份我（从使用者的角度）想要的配置文件（yaml格式）,来帮助我对事物进行抽象：
alert:
- name: 宿主机监控
type: influxdb
url: http://120.25.127.4:8086
db: telegraf
interval: 2s
query:
- name: cpu空闲值
sql: SELECT usage_idle FROM cpu WHERE time &amp;gt; now() - 1m
threshold: 100
op: &amp;quot;&amp;lt;=&amp;quot;
- name: 内存使用率
sql: SELECT used_percent FROM mem WHERE time &amp;gt; now() - 1m
op: &amp;quot;&amp;gt;=&amp;quot;
threshold: 50

notifier:
- name: 测试组
enable: true
type: mail 
host: smtp.163.com
port: 25
username: qq912293672@163.com
password: xxxxxx
from: qq912293672@163.com
to: [912293672@qq.com]

从配置文件上可以看到，我对告警组件抽象出了两个大的划分，一个是alert,抽象了数据的获取。一个是notifier,抽象了事件的通知。
在alert里面，我赋予了alert几个属性，其中type用来标识数据库的类型，因为我希望这个告警组件能支持多个存储后端。接下来是query，sql属性让用户自定义数据的查询方式，并且用threshold和op表示触发的阀值以及如何触发。总的来说，我采用了将多个查询实体组合成一个告警单位。这是经过思考的结果，目的是为了避免通知风暴：即很多机器很不幸都出异常的时，多个事件将聚合成一个告警，而不是发出多个邮件，而每个邮件的内容却很少。
而notifier的配置，我特意添加了type,也是为了支持多种通知方式，以及一个开关enable。
将notifier与alert分开，以及alert中包含query的设计，其实也是从Grafana和prometheus中学到的思路。在此感谢下今天的开源文化，让我等普通人有机会学到别人优秀的设计理念。
抽象得差不多后，可以考试编码了，按照分类，将代码主要分为3个模块，notify，alert,service。其中service对应我们上面的第三个需求，对外暴露API。
接口设计
开发语言上我使用的是Go，我们将会有多个query在执行，这刚好对上的Go的强项，并发。下面看下几个主要的接口：
// Executor :type Executor interface {
    Execute() ([]Result, error)
    Interval() time.Duration
    Config() Config
    Close() error
}

因为alert其实可以当做一个获取数据的执行单位，所以我在这里又抽象出了一个执行器Executor,接下来我们只需要让我们Alert实现该接口，就能被调用执行。
type Analyzer interface {
    Analyze(string, interface{}, QueryConfig) (Result, bool)
}

Analyzer接口实现对各个监控系统数据处理。
type Result interface {
    String() string
    QueryName() string}

Result接口抽象了监控系统的返回数据，屏蔽掉各个监控系统之间的数据差异。
type Notifier interface {
    Send(content string) error
    Name() string
    Type() string
    To() []string
    Enable() bool
    Config() *Config
}

通知接口
接下来我们需要实现一个调度器，实现了对上面Executor的调度和控制：
type Scheduler interface {
    Run()
    AddExecutor(executor.Executor)
    RemoveExecutor(name string)
    ExecutorExist(name string) bool
    Stop()
}

核心调度逻辑：
runFn := func(schItem *scheduleItem) {
        bf := bytes.NewBufferString(&amp;quot;&amp;quot;)
        ticker := time.NewTicker(schItem.executor.Interval())
        notiMap := make(map[string]int)
        mutex := &amp;amp;sync.RWMutex{}        for {            select {            // 定时器到期
            case &amp;lt;-ticker.C:
                results, err := schItem.executor.Execute()                if err != nil {
                    log.Error(err)                    continue
                }
                notifiers, err := adaper.ReadAllNotifier()                if err != nil {
                    log.Error(err)                    continue
                }                for _, result := range results {                    // 对通知数进行累加
                    mutex.Lock()
                    notiMap[result.QueryName()]++
                    mutex.Unlock()                    // 通知数已经超过了限制
                    log.Debugf(&amp;quot;notiMap:%+v&amp;quot;, notiMap)
                    result := result                    if notiMap[result.QueryName()] &amp;gt; notiSeqCount {                        if notiMap[result.QueryName()] == notiSeqCount+1 {                            go func(name string) {
                                time.Sleep(notiSleepDuration)
                                mutex.Lock()
                                notiMap[name] = 0
                                mutex.Unlock()
                            }(result.QueryName())
                        }                        continue
                    }                    if _, err := bf.WriteString(result.String()); err != nil {
                        log.Error(err)
                    }
                    bf.WriteString(&amp;quot;&amp;lt;br&amp;gt;&amp;quot;)
                }
                msgBody := bf.String()
                bf.Reset()                // 内容为空则跳过通知
                if msgBody == &amp;quot;&amp;quot; {                    continue
                }                // 遍历通知器将报警发送出去
                for _, notifier := range notifiers {
                    log.Debugf(&amp;quot;bool:%v&amp;quot;, notifier.Enable())                    if !notifier.Enable() {                        continue
                    }
                    notifier := notifier                    go func() {
                        err := notifier.Send(msgBody)                        if err != nil {
                            log.Errorf(&amp;quot;Send %s notify to %s fail:%s&amp;quot;, notifier.Type(), notifier.To(), err.Error())                            return
                        }
                        log.Infof(&amp;quot;Send %s notify to %s success&amp;quot;, notifier.Type(), notifier.To())
                    }()
                }            // 收到退出信号
            case &amp;lt;-schItem.closeCh:
                ticker.Stop()
                log.Infof(&amp;quot;Executor %s exit&amp;quot;, schItem.executor.Config().Name)                return
            }
        }
    }

上面的调度控制中，为了避免某个异常事件在短时间没有解决时，我实现了自己的一个控制逻辑：当同一个query的通知已经连续超过3次时，我会让它定制通知半小时。若半小时异常还继续，则再发三次通知给接受者，如此循环下去。
最后还有HTTP API的实现以及对数据的存储。这属于常规的开发逻辑，和我们这个告警组件的关系不是很大，就不一一介绍了。
总结
写这篇文章主要是总结下设计的思路，尤其是在几个核心问题上。在设计之初，除了告诉自己要保持住稳定性之外，还特别注意了如何对代码做到恰到好处的抽象，这也是最近半年看了那么多优秀开源项目的代码后的想法。老实说我这一次又对自己做得不满意，有机会我重构下整个组件。另外其实还有一个比较棘手的问题，就是如何让告警组件做到高可用（这无法通过简单部署多个服务就能实现，这样会导致通知事件的重复发送），这个问题最近正在解决，可以期待下一篇文章。
https://mp.weixin.qq.com/s/qr8WyroAWqx4D85J89RbvQ

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/how-to-design-monitor-platform">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/why-go-so-different">
                        Go语言究竟不一样在哪
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-13</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_3">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-primary">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                        <a href="https://lwhile.github.io/post/why-go-so-different" class="post-feature-image" style="background-image:url(https://lwhile.github.io/post-images/why-go-so-different.png) ">
                        </a>
                        
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            第一次听到Go语言是在什么时间已经想不起来了。
我是14年进入的大学，在15年面试校内一个技术组织时，被问了解哪些服务端语言。那时候我提到了Go。不过正式把它列入学习计划，还要等到16年的十月份。后来在17年三月份出来实习使用Go做开发，才有更多的机会将它投入实践中。
在Go语言之前，我学习了C，C++， Java，Python。Go语言是唯一一门我认为自己能算得上掌握的语言。
这门语言最与众不同的地方在于，将写并发型的代码变得异常容易。另一个能在简洁程度（指并发）上能和它一比的，大概只有Actor世界里的Erlang/Elixir了。
在继续介绍Go之前我觉得有必要提下它的老爹。不过我要说的不是谷歌(Go最开始是谷歌的内部项目)，而是计算机领域的那三尊神。
Ken Thompson
Rob Pike
Robert Griesemer
如果你知道C语言的作者叫做Dennis Ritchie，并且看过它的一张图片，就是这张：


你应该对这两位老头有印象，很多介绍C语言或者计算机导论的书都有这张照片。右边那位就是Dennis Ritchie，2011年他已经驾鹤西去，在他离世前的一个星期，地球上另一个传奇也走了，他叫做史蒂夫・乔布斯。那一年我刚考上高中，乔布斯走后凡客出了一件纪念衫和一本他的传记，我都买了，现在还留着。

左边那位，就是另外的一尊神，Ken Thompson。Unix知道吧？他最先设计和实现出来的（我说最初的几个Unix版本内核是汇编写的你信吗？）C语言的前身—— B语言，他弄的。Plan9，他参与了。正则表达式和UTF-8编码的设计，他也凑了一脚。在大牛这个称谓泛滥的今天，我只能用神这个字去形容我的这位偶像了。
UTF-8还有另外一个发明者，和Ken一样也是Unix的成员，他就是目前Go语言的实际控制者，Rob Pike。你不一定会写Go代码，但你一定对Go那个萌贱萌贱的Logo有印象，这个Logo就是由Rob Pike的老婆设计的。

最后一尊神，Robert Griesemer，手握几个个赫赫有名的项目，其中两个每天都会有一大波程序员它们打交道。这两个项目就是V8和JVM。Go 1.3之后改变了GC的工作机制，据说就是由他操刀。
三位大神介绍完了，最终我想说的是，Go多多少少会凝聚他们三位在编程方面的宝贵经验，当初也是因为这一点我才会坚定得学习它，虽然我知道可能毕业的时候找不到一份Go这样一门新兴语言的工作。
好了，题外话不继续了，接下来开始列举Go的一些特性。
1. goroutine
目前在并发编程领域，我们有3种最基本的模型选择。
第一种，即多进程/线程模型，这也是C/C++，Java最常用的并发模式。这种模型的好处是因为操作系统的原生支持，语言的实现者不用考虑其调度问题，交给操作系统就行了。但缺点就是会在并发数上升到一定程度后，系统需要将时间片更多花在进程/线程的上下文切换上。Linux默认的线程栈空间大小是1MB, 开一个1000个进程什么也不干，就需要接近1G的内存空间了。
第二种，是事件驱动机制，典型如Nginx和Node。我们都知道Nginx和Node扛并发的能力很强，但实际上占用的资源却极少，很大一部分将所有的事件通过一些特殊的数据结构（如红黑树）组织起来，等到事件发生的时候再放到一个单一的进程/线程去处理。这个模型只适合IO密集型的工作场景，因为事件驱动的本质只是充分利用起CPU的空闲时间。
第三种，就是Go使用的模型，协程模型，和Python，Lua，Erlang/Elixir里面的协程是同一个东西，只不过在Erlang/Elixir里面叫做process，Go里面叫做goroutine。我们知道线程比进程轻量，而协程则比线程更加轻量化。在Go里面，一个协程所占用的基本空间是2KB，只有线程的1/512。换句话说，起1000个空跑的goroutine，大概需要2MB的内存，而起1000个线程，则需要接近1GB。更低的内存占用，意味着可以开更多的并发单位，以及消耗更少的上下文切换时间。
可以看一个简单的例子：
func ShotOut() {
	// 休眠1秒钟
	time.Sleep(time.Second)

	// 向控制台抵茶
	fmt.Println(&amp;quot;给大佬抵茶&amp;quot;)
}

func main() {
	for i := 0; i &amp;lt; 5; i++ {
		// 起5个goroutine
		go ShotOut()
	}

	// 别让main退出，作用类似C里面的getchar()
	select {}
}

如果没有使用并发，那么执行这个程序需要5s，但实际执行时间会在1s左右，这证明我们调用ShotOut函数的时候确实是并发了。
如果 Go 只是像 Python 或者Lua那样简单得引入了协程，那么它绝对不可能有今天的地位。Go的开发团队最伟大的地方在于，他们赋予了Go的runtime调度goroutine的能力，就像Linux内核调度线程那样。正是这一点，才让我们编写并发代码变得更加简单。
2.channel
说完goroutine，还有一个与它配合使用特性叫channel，可以说这两个特性加起来就锻造成了Go的屠龙宝刀。
上面的例子中起了5个goroutine，这5个并发单位都比较简单，彼此之间不需要通信。如果需要呢？channel就是用于相互隔离间的并发单位进行通信的一个消息队列。
看一个简单的例子：
func main() {
	// 声明一个存放int类型的channel
	ch := make(chan int)

	go func() {
		// 休眠1秒钟
		time.Sleep(time.Second)

		// 向channel写入整数1
		ch &amp;lt;- 1
	}()

	go func() {
		// 等待从通道中取出内容
		res := &amp;lt;-ch
		fmt.Println(res) // 1s后输出1
	}()

	// 别让main退出，作用类似C里面的getchar()
	select {}
}

我们对最上面的例子改下需求，要求第一个goroutine执行完毕后，才能继续执行另外两个。第二波执行的这两个goroutine执行完毕后，才能执行最后剩下的2个gourotine。下面是Go的实现，大家可以想下如果用C++或者Java要如何写：
func ShotOut() {
	// 休眠1秒钟
	time.Sleep(time.Second)

	// 向控制台抵茶
	fmt.Println(&amp;quot;给大佬抵茶&amp;quot;)
}

func main() {
	// 声明两个存放int类型的channel
	ch1 := make(chan int)
	ch2 := make(chan int)

	go func() {
		// 休眠1秒钟
		time.Sleep(time.Second)

		// 第1次跑抵茶函数
		ShotOut()

		// 抵完第1杯茶，向channel写入整数1
		ch1 &amp;lt;- 1
	}()

	go func() {
		// 等待第1杯茶递完的信号
		&amp;lt;-ch1

		// 收到信号，开始抵第2和递3杯茶
		ShotOut()
		ShotOut()

		// 通知最后2个
		ch2 &amp;lt;- 1
	}()

	go func() {
		// 等待第2和第3杯茶递完
		&amp;lt;-ch2

		// 递最后两杯
		ShotOut()
		ShotOut()
	}()

	// 别让main退出，作用类似C里面的getchar()
	select {}
}


注意加了go关键字在面前的函数都会以并发的方式进行

3. 无依赖运行
如果说上面那两个特性要在并发环境下才能体现用处，那么Go可以编译成一个完成无依赖的二进制这一功能，是真的能大大提升你编程的意愿：因为你写的东西很容易传播给别人。
就拿能向浏览器输出Hello World的Web程序来举例子好了
pakcage main 

import &amp;quot;fmt&amp;quot;
import &amp;quot;net/http&amp;quot;

func HelloWorld(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte(&amp;quot;Hello World&amp;quot;))
}

func main() {
	http.HandleFunc(&amp;quot;/&amp;quot;, HelloWorld)
	http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil)
}

是的，你没看错，就这简单几句代码，跑起来它就是一个Web服务。而且通过 go build 命令编译后，将生成的二进制直接丢到服务器上就能跑了，Linux，Mac OS，Windows三平台都能用，而且性能非常强！想想如果是Java或者Python, Go的程序已经丢到服务器在跑了，Java的进度条可能还处在配置Tomcat上，Python则还在配置gunico和supervisor...
最后
上面提到的三个特性，我觉得已经够向没了解过Go的用户介绍清楚他最与众不同的地方了。还有其他几个特性，比如非侵入性的接口，抛弃面相对象模型，多返回值，闭包等，这些相比其他语言我倒觉得不是其最大的亮点，留到以后有时间再介绍吧，这篇文章就已经写了我两个晚上了...

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/why-go-so-different">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/etcd-watch-source-code">
                        Etcd watch源码阅读
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-10</time>
                    
                        <a href="https://lwhile.github.io/tag/iYb5jfiTB" class="post-tag i-tag
                            i-tag-warning">
            #etcd
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-error">
            #计算机
        </a>
                        
                        <a href="https://lwhile.github.io/tag/5UKnY2CH9d" class="post-tag i-tag
                            i-tag-error">
            #Go
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            公司的业务里面使用了Consul做服务发现, 发现其有一个watch机制.这个watch机制引起我的好奇, 因为刚好在看Etcd-raft的代码, Etcd也有类似的watch机制, 所以趁热打铁, 赶紧趁周末研究下etcd watch机制源码的实现.
在看源码之前, 我们通过一个简单的例子, 看看Etcd的watch是如何使用的.

先往Etcd写入一对KV


curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=&amp;quot;神蛋使者&amp;quot;


Watch这对KV


curl http://127.0.0.1:2379/v2/keys/name?wait=true

如果一切正常, 这时候请求会被阻塞住.

新开一个终端, 修改存进去的KV


curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=神蛋使者1号


阻塞的那个请求返回watch到的结果

{
  &amp;quot;action&amp;quot;:&amp;quot;set&amp;quot;,
  &amp;quot;node&amp;quot;:{ 
      &amp;quot;key&amp;quot;:&amp;quot;/name&amp;quot;,
      &amp;quot;value&amp;quot;:&amp;quot;神蛋使者1号&amp;quot;,
      &amp;quot;modifiedIndex&amp;quot;:25,
     &amp;quot;createdIndex&amp;quot;:25
  },
   &amp;quot;prevNode&amp;quot;: {
     &amp;quot;key&amp;quot;:&amp;quot;/name&amp;quot;,
     &amp;quot;value&amp;quot;:&amp;quot;神蛋使者&amp;quot;,
     &amp;quot;modifiedIndex&amp;quot;:24,
     &amp;quot;createdIndex&amp;quot;:24
   }
  }

体验流程大概就是这样, 下面正式看源码.
接口定义
type Watcher interface {
	// Watch watches on a key or prefix. The watched events will be returned
	// through the returned channel.
	// If the watch is slow or the required rev is compacted, the watch request
	// might be canceled from the server-side and the chan will be closed.
	// &#39;opts&#39; can be: &#39;WithRev&#39; and/or &#39;WithPrefix&#39;.
	Watch(ctx context.Context, key string, opts ...OpOption) WatchChan

	// Close closes the watcher and cancels all watch requests.
	Close() error
}

该接口定义了两个方法, Watch 和 Close
Watch 方法返回一个WatchChan 类似的变量, WatchChan是一个channel, 定义如下:
type WatchChan &amp;lt;-chan WatchResponse

该通道传递WatchResponse类型
type WatchResponse struct {
	Header pb.ResponseHeader
	Events []*Event

	// CompactRevision is the minimum revision the watcher may receive.
	CompactRevision int64

	// Canceled is used to indicate watch failure.
	// If the watch failed and the stream was about to close, before the channel is closed,
	// the channel sends a final response that has Canceled set to true with a non-nil Err().
	Canceled bool

	// Created is used to indicate the creation of the watcher.
	Created bool

	closeErr error
}

其中Event类型是一个gRPC生成的消息对象
type Event struct {
	// type is the kind of event. If type is a PUT, it indicates
	// new data has been stored to the key. If type is a DELETE,
	// it indicates the key was deleted.
	Type Event_EventType `protobuf:&amp;quot;varint,1,opt,name=type,proto3,enum=mvccpb.Event_EventType&amp;quot; json:&amp;quot;type,omitempty&amp;quot;`
	// kv holds the KeyValue for the event.
	// A PUT event contains current kv pair.
	// A PUT event with kv.Version=1 indicates the creation of a key.
	// A DELETE/EXPIRE event contains the deleted key with
	// its modification revision set to the revision of deletion.
	Kv *KeyValue `protobuf:&amp;quot;bytes,2,opt,name=kv&amp;quot; json:&amp;quot;kv,omitempty&amp;quot;`
	// prev_kv holds the key-value pair before the event happens.
	PrevKv *KeyValue `protobuf:&amp;quot;bytes,3,opt,name=prev_kv,json=prevKv&amp;quot; json:&amp;quot;prev_kv,omitempty&amp;quot;`
}

接下来看实现了Watcher接口的watcher类型
// watcher implements the Watcher interface
type watcher struct {
	remote pb.WatchClient

	// mu protects the grpc streams map
	mu sync.RWMutex

	// streams holds all the active grpc streams keyed by ctx value.
	streams map[string]*watchGrpcStream
}

watcher结构很简单, 只有3个字段. remote抽象了发起watch请求的客户端, streams是一个map, 这个map映射了交互的数据流.还有一个保护并发环境下数据流读写安全的读写锁.
streams所属的watchGrpcStream类型抽象了所有交互的数据, 它的结构定义如下:
type watchGrpcStream struct {
	owner  *watcher
	remote pb.WatchClient

	// ctx controls internal remote.Watch requests
	ctx context.Context
	// ctxKey is the key used when looking up this stream&#39;s context
	ctxKey string
	cancel context.CancelFunc

	// substreams holds all active watchers on this grpc stream
	substreams map[int64]*watcherStream
	// resuming holds all resuming watchers on this grpc stream
	resuming []*watcherStream

	// reqc sends a watch request from Watch() to the main goroutine
	reqc chan *watchRequest
	// respc receives data from the watch client
	respc chan *pb.WatchResponse
	// donec closes to broadcast shutdown
	donec chan struct{}
	// errc transmits errors from grpc Recv to the watch stream reconn logic
	errc chan error
	// closingc gets the watcherStream of closing watchers
	closingc chan *watcherStream
	// wg is Done when all substream goroutines have exited
	wg sync.WaitGroup

	// resumec closes to signal that all substreams should begin resuming
	resumec chan struct{}
	// closeErr is the error that closed the watch stream
	closeErr error
}

比较有意思的是, watchGrpcStream也包含了一个watcher类型的owner字段, watcher和watchGrpcStream可以互相引用到对方.同时又定义了watcher类型中已经定义过的remote,而且还不是指针类型, 这点不大明白作用是啥.
还有几个字段值得关注, 一个是substreams, 看下它的定义和注释:
// substreams holds all active watchers on this grpc stream
substreams map[int64]*watcherStream

再看看watcherStream类型的定义:
// watcherStream represents a registered watcher
type watcherStream struct {
	// initReq is the request that initiated this request
	initReq watchRequest

	// outc publishes watch responses to subscriber
	outc chan WatchResponse
	// recvc buffers watch responses before publishing
	recvc chan *WatchResponse
	// donec closes when the watcherStream goroutine stops.
	donec chan struct{}
	// closing is set to true when stream should be scheduled to shutdown.
	closing bool
	// id is the registered watch id on the grpc stream
	id int64

	// buf holds all events received from etcd but not yet consumed by the client
	buf []*WatchResponse
}

画个图整理下他们之间的关系:

接下来轮到watcher是如何watch方法的了:
// Watch posts a watch request to run() and waits for a new watcher channel
func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan {
	// 应用配置
	ow := opWatch(key, opts...)

	var filters []pb.WatchCreateRequest_FilterType
	if ow.filterPut {
		filters = append(filters, pb.WatchCreateRequest_NOPUT)
	}
	if ow.filterDelete {
		filters = append(filters, pb.WatchCreateRequest_NODELETE)
	}

	// 根据传入的参数构造watch请求
	wr := &amp;amp;watchRequest{
		ctx:            ctx,
		createdNotify:  ow.createdNotify,
		key:            string(ow.key),
		end:            string(ow.end),
		rev:            ow.rev,
		progressNotify: ow.progressNotify,
		filters:        filters,
		prevKV:         ow.prevKV,
		retc:           make(chan chan WatchResponse, 1),
	}

	ok := false
	// 将请求上下文格式化为字符串
	ctxKey := fmt.Sprintf(&amp;quot;%v&amp;quot;, ctx)

	// find or allocate appropriate grpc watch stream
	// 接下来配置对应的输出流, 注意得加锁
	w.mu.Lock()

	// 如果stream为空, 返回一个已经关闭的channel.
	// 这种情况应该是防止streams为空的情况
	if w.streams == nil {
		// closed
		w.mu.Unlock()
		ch := make(chan WatchResponse)
		close(ch)
		return ch
	}

	// 注意这里, 前面我们提到streams是一个map,该map的key是请求上下文
	// 如果该请求对应的流为空,则新建
	wgs := w.streams[ctxKey]
	if wgs == nil {
		wgs = w.newWatcherGrpcStream(ctx)
		w.streams[ctxKey] = wgs
	}
	donec := wgs.donec
	reqc := wgs.reqc
	w.mu.Unlock()

	// couldn&#39;t create channel; return closed channel
        // couldn&#39;t create channel; return closed channel
	// 这里要设置为缓冲的原因可能与下面的两个
	// closeCh &amp;lt;- WatchResponse{closeErr: wgs.closeErr}
	// 语句有关,这里不理解
	closeCh := make(chan WatchResponse, 1)

	// submit request
	select {
	// 发送上面构造好的watch请求给对应的流
	case reqc &amp;lt;- wr:
		ok = true
	// 请求断开(这里应该囊括了客户端请求断开的所有情况)
	case &amp;lt;-wr.ctx.Done():
	// watch完成
	// 这里应该是处理非正常完成的情况
	// 注意下面的重试逻辑
	case &amp;lt;-donec:
		if wgs.closeErr != nil {
			// 如果不是空上下文导致流被丢弃的情况
			// 则不应该重试
			closeCh &amp;lt;- WatchResponse{closeErr: wgs.closeErr}
			break
		}
		// retry; may have dropped stream from no ctxs
		return w.Watch(ctx, key, opts...)
	}

	// receive channel
	// 如果是初始请求顺利发送才会执行这里
	if ok {
		select {
		case ret := &amp;lt;-wr.retc:
			return ret
		case &amp;lt;-ctx.Done():
		case &amp;lt;-donec:
			if wgs.closeErr != nil {
				closeCh &amp;lt;- WatchResponse{closeErr: wgs.closeErr}
				break
			}
			// retry; may have dropped stream from no ctxs
			return w.Watch(ctx, key, opts...)
		}
	}

	close(closeCh)
	return closeCh
}

还有Watcher接口的另一个方法Close:
func (w *watcher) Close() (err error) {
	// 在锁内先将streams字段置为空
	// 在锁外再将一个个流都关闭
	// 这样做的意义在于不管哪个流关闭失败了
	// 都能先保证streams与这些流的关系被切断
	w.mu.Lock()
	streams := w.streams
	w.streams = nil
	w.mu.Unlock()
	for _, wgs := range streams {
		if werr := wgs.Close(); werr != nil {
			err = werr
		}
	}
	// etcd竟然也只是返回一个error
	// 虽然上面的for循环可能产生多个error
	return err
}

这样watcher就实现了Watcher接口.大致的实现思路本文就介绍到这里,剩下的代码也都是对其他相关数据结构的逻辑包装操作.
简单阅读Etcd的这一小部分源码下来, 我看到他们源码中的两个东西,算是Golang或者编程上面的一些最佳实践:


对包外只暴露一个公共接口, 包内的结构体实现该接口即可.就像本文中的Watcher接口和watcher结构体.这样有两个好处, 一个就是代码能够解耦,还有就是可以省去命名的苦恼(__)


另一个是注释的书写方式,我发现etcd源码里的注释很大一部分写在变量的定义上面,而且变量的定义名都很清晰.


抽象得体.这个其实不只是Etcd, 其他任何优秀的开源作品都把他们的代码抽象得很到位.突然想起我写的那些渣渣代码%&amp;gt;_&amp;lt;%


最后, 总结下etcd的watch机制.其实归根结底, 它的watch是通过gRPC的多路复用实现的,这是一个基于HTTP/2的特性.所以本文可能有些偏离了主题,探讨Etcd的watch机制, 其实应该研究HTTP/2才是.
算是给自己挖个坑.

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/etcd-watch-source-code">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/hi-openresty">
                        使用Openresty构建认证网关
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-07</time>
                    
                        <a href="https://lwhile.github.io/tag/s2UnbI257" class="post-tag i-tag
                            i-tag-">
            #openresty
        </a>
                        
                        <a href="https://lwhile.github.io/tag/94Hr92W69j" class="post-tag i-tag
                            i-tag-">
            #网络
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-warning">
            #计算机
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            在单体应用中, 我们可以通过 cookie + session, 或者 JSON web token, 将认证逻辑在单体应用中实现, 简单高效, 还特别省事.
然而这几年随着服务化潮流越来越火(我觉得这是必然趋势, 想想我们人类社会是如何运作的), 很多以前单体应用不存在的问题, 现在已成为对单体应用拆分过程中的第一个障碍, 比如系统的认证体系.
如果每个拆出来的服务都要做一次认证(就是程序员多写几份认证的代码啦), 对于有理想有追求的灵魂の码农来说, 是绝对无法接受的.你说认证代码copy就好了, 不用重新写.no no no, 这样搞出来的架构不仅看着别扭, 代码闻着就觉得臭, 而且迟早有一天会出问题.
解决单体应用拆分服务后的认证问题其实很常规, 回想下祖师爷们帮我们总结的一句话: &amp;quot;Any problem in computer science can be solved by another layer of indirection.&amp;quot; 我们可以在所有服务前面增加一层认证服务.

看到认证服务这一层用来作为用户请求的总入口, 有Nginx或者Apache使用经验的同学自然而然就想到它们. 如果能把认证模块的功能整合进Nginx或者Apache这些Web服务器, 那岂不是更完美 ?
而这篇文章的主角: Openresty,就可以帮助我们简单快速得完成这个想法.这是一个由春哥(Github)发起的项目.你可以将Openresty看做Nginx + 常用模块构成的软件包, 但是最重要的功能是我们可以使用Lua在Nginx实现Web框架才能实现的逻辑, 接下来文章将会开始介绍如何使用Openresty, 将上面提到的认证服务整合进Nginx里.
安装
Openresty有两种安装方式, 一种是使用源码编译安装.一种使用官方提供的预编译包:
具体可参考官网的安装文档
Hello world
如果你没修改过Openresty的安装位置, 默认会被安装在/use/local/openresty目录下.我们现在可以尝试写一个Hello world级别的demo.
为Openresty创建工作目录并创建配置文件:

mkdir ~/openresty_work
cd ~/openresty_work
touch nginx.conf

接下来在nginx.conf里面配置一个路由规则
worker_processes 1;
error_log logs/error.log;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8080;
		location /hello {
			default_type text/html;
			content_by_lua_block {
				ngx.say(&amp;quot;Hello Openresty.&amp;quot;)
			}			
		}
	}
}


跟普通的Nginx配置文件比起来, 上面的配置多了一个content_by_lua_block指令, 正是通过调用该指令, 访问该路由的时候,才会输出相应的内容.这个指令是由Openresty中的LuaNginxModule模块提供的功能, 请求进来的时候, Nginx会启动lua的虚拟机, 输出的内容则由lua提供.
我们可以使用content_by_lua_file指令替代content_by_lua_block, 将相关的lua代码写进文件里.
location /hello {
			content_by_lua_file lua/hello.lua;
		}


--- hello.lua
ngx.say(&amp;quot;Hello Openresty.&amp;quot;)

有了上面的铺垫,接下来可以开始构建我们的认证服务,认证的方式使用JWT
Openresty将一个请求的生命周期划分为4个阶段:

我们的认证服务将会挂载在第二阶段, 即 Rewrite/Access Phase 上.
接下来准备一个需要用到的库:
lua-resty-jwt
clone下来后放到hello.lua文件所在的文件夹,并将lua_package_path配置为:
lua_package_path &amp;quot;/root/openresty_work/lua/?.lua;/root/openresty_work/lua/lua-resty-jwt/lib/?.lua;;&amp;quot;;

构建的思路也很简单, 对用户提供一个登录请求, 验证身份后将jwt token分发给用户.用户接下来访问需要认证的接口, 则在header里面加入该token, 请求进入Openresty后由lua提取出token进行认证.
Nginx配置文件
server {
                listen 8080;
                location /hello {
                        content_by_lua_file lua/hello.lua;
                }
                location /login {
                        content_by_lua_file lua/sign.lua;
                }
                location /service1 {
                        access_by_lua_file lua/verify.lua;
                        # 需要反向代理在这配置
                }
                location /service {
                        access_by_lua_file lua/verify.lua;
                        # ...
                }
        }


下面是配置中相关的lua文件
sign.lua ↓:
local jwt = require &#39;resty.jwt&#39;

-- 只允许POST请求
if ngx.req.get_method() ~= &#39;POST&#39; then
    ngx.status = 405
    ngx.say(&amp;quot;Mehtod Not Allow&amp;quot;)
    return
end

-- 获取请求body
ngx.req.read_body()
local body_raw = ngx.req.get_body_data()
local body_json = cjson.decode(body_raw)
local username = body_json[&#39;username&#39;]
local password = body_json[&#39;password&#39;]

if not username or not password then
    ngx.log(ngx.ERR, username, password)
    ngx.status = 400
    ngx.say(&#39;无法获取账号或者密码&#39;)
    return
end

-- 验证账号和密码是否正确,如果验证失败则做如下处理
if not this_is_a_auth_method(username, password) then
    ngx.status = 401
    ngx.say(&#39;认证失败&#39;)
    return
end

verify.lua ↓:
local jwt = require &#39;resty.jwt&#39;

-- 从请求中提取header并从header从获取token字段
local headers = ngx.req.get_headers()
local token = headers[&#39;token&#39;]

-- 检查token是否存在
if not token then 
    ngx.status = 400
    ngx.say(&#39;无法获取token&#39;)
    return 
end 

-- 验证token
local jwt_obj = jwt:verify(vars.jwt_salt(), token)
if not jwt_obj[&#39;verified&#39;] then 
    ngx.status = 401
    ngx.say(&#39;无效的token&#39;)
    return 
end 

至此一个使用Openresty构建的认证网关的雏形已经出来了.需要说明的一句是, 上面的代码由于没有公司相关的运行环境,笔者没有经过测试和验证.所以只可阅读,不可复制后直接运行 😃
如果想把这套认证网关用在生产环境上, 还有很多东西需要考虑.比如跨域问题, 静态文件的代理问题等等.
个人接触Openresty的时间也不长, 文中难免会有地方写错了或者表达得很差, 欢迎发评论或者发邮件给我指正: lwhile521@gmail.com ,感谢.
对于Openresty, 个人认为要对它产生兴趣,关键在于认不认可让Nginx承担除了Web服务器之外更多的业务, 对于Openresty, 它能带来的好处有:


极致的性能.上文没有提到Openresty的性能, 其实Openresty的编程模型和NodeJS很像, 在Openresty的世界里面,所有东西都是非阻塞的,更难得可贵的是, 它不需要使用NodeJS中的回调函数, 代码写起来其实还是同步模型, 配合C语言编写的Nginx, 最快的脚本语言lua+luajit解释器,这套方案的性能无可挑剔了.


降低了Nginx模块的开发难度. Nginx + C/C++能做的, Openresty用lua都能做.开发效率高了, 性能还不怎么降, 何乐而不为呢?


参考资料


Openresty最佳实践
lua-resty-jwt



                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/hi-openresty">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/for-developer">
                        致开发人员
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-06</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-warning">
            #计算机
        </a>
                        
                </div>
                <div class="post-article">
                    
                        <a href="https://lwhile.github.io/post/for-developer" class="post-feature-image" style="background-image:url(https://lwhile.github.io/post-images/for-developer.jpg) ">
                        </a>
                        
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            「软技能:代码之外的生存指南」在开头部分有几句话,我看了之后觉得很棒,于是摘抄下来激励自己.

谨以本书献给所有自强不息,孜孜不倦地持续自我改进的开发人员,他们具备以下素质:
永远不会对&amp;quot;不错&amp;quot;感到心满意足
永远寻求每一个机会来扩展自己的视野, 探索未知事物
对知识的渴求永远不会熄灭
笃信软件开发并不仅仅意味着编写代码
知道失败不是结束,失败只是人生旅途上的小小一步
有过挣扎,有过失败,但仍然会爬起来继续战斗
拥有强烈意愿和决心,在人生的道路上不畏艰难
以及最重要的,愿意一路上帮助他人


                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/for-developer">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/ri-zhi-qie-fen-wen-ti-you-gan">
                        日志切分问题有感
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-11-05</time>
                    
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-info">
            #计算机
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            有过服务端开发经验的同学应该对日志这个东西不陌生, 把程序丢到服务器上跑, 日志就是我们了解运行情况,甚至解BUG的唯一入口了.
有些程序的日志量会增长得非常快, 比如Nginx, 当一个日志文件大到几百MB甚至上GB的时候, 要从这个文件找出我们要的信息就基本等于大海捞针了,所以这时候对日志进行管理就显得格外重要.
日志量大的平台可以上ELK,利用ES的搜索优势基本不担心日志数据量大的问题.但本文不打算涉及这方面的内容.接下来主要讲讲如何正确得对日志文件做切分.
Linxu上的日志切分有两种形式, 一种是使用Linux的logrotate工具, 另外一种是使用额外编写的脚本, 这种形式一般是和日志库配合使用.
因为业务的关系, 我们最开始抛弃了使用logrotate的方案, 因为我们觉得这会给实施人员增加系统的的维护负担(后来发现是我们对logrotate不够了解).于是我们使用第二种方案, 将日志的切分操作在我们的日志库里面实现, 我们封装了logrus和lfshook, 利用logrus的hook机制将切分的逻辑嵌入在日志库里面,代码调用的时候会自动触发切分操作.我们会这样做也是受到beego框架的影响, 它的日志库默认就带了切分功能.
一切运作得很顺利, 直到我们有一次在使用Openresty的时候, 发现Nginx的日志没有被切分.因为之前使用Nginx的时候,默认安装完毕后日志是会自动以天切分的, 于是我们开始找Nginx的配置项,看看是否漏掉了某些配置.但是不找没关系,了解后才发现Nginx是不提供日志切分功能的.What ? 那之前的切分功能是怎么来的?
接下来解决问题的过程中发现了在/etc/logrotate.d/下有nginx的配置, 同时还有Mysql和其他基础组件的,这时我们才想到有可能是RPM包(我们的系统是Centos)安装的时候自动生成了一个logrotate的配置文件,后来一查果然是(命令:rpm -qpl xxx.rpm).而我们的Openresty包没有生成这个配置文件,所以导致Nginx的日志文件没有被切分.
实际上很多软件都只会做日志的记录,不会帮忙做切分,这个确实是合理的.这让我们想起logrus为什么不提供日志切分的功能,而是得由第三方的库去完成.我们将日志切分的逻辑耦合进代码里面,现在回过头来看其实也不是很合理,正确的做法其实还是应该在打RPM包的时候, 生成一个logrotate的配置文件, 这样一来也不会增加实施人员的负担,而且也可以将切分功能统一到一个地方去做.

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/ri-zhi-qie-fen-wen-ti-you-gan">Read More ~</a>
                            </div>
                </div>
            </article>
            
            <article class="post i-card">
                <h2 class="post-title">
                    <a href="https://lwhile.github.io/post/influxdb-vs-prometheus">
                        InfluxDB与Prometheus用于监控系统上的对比
                    </a>
                </h2>
                <div class="post-info">
                    <time class="post-time">2017-07-20</time>
                    
                        <a href="https://lwhile.github.io/tag/influxdb" class="post-tag i-tag
                            i-tag-other_4">
            #influxdb
        </a>
                        
                        <a href="https://lwhile.github.io/tag/prometheus" class="post-tag i-tag
                            i-tag-other_4">
            #prometheus
        </a>
                        
                        <a href="https://lwhile.github.io/tag/jian-kong-xi-tong" class="post-tag i-tag
                            i-tag-other_1">
            #监控系统
        </a>
                        
                        <a href="https://lwhile.github.io/tag/GQVNWsaTp" class="post-tag i-tag
                            i-tag-other_1">
            #计算机
        </a>
                        
                </div>
                <div class="post-article">
                    
                            <div class="post-content">
                                
                                        <div class="post-content-content">
                                            总览
首先要明白, Prometheus 提供的是一整套监控体系, 包括数据的采集,数据存储,报警, 甚至是绘图(只不过很烂,官方也推荐使用 grafana).
而 InfluxDB 只是一个时序数据库, 使用它做监控系统的话, 还需要物色数据采集器,如 telegraf, collectd 等. 甚至连报警模块, 也需要使用同为 Influxdata 公司出的 Kapacitor.从这个角度来说, Prometheus 会有运维上的优势, 因为它用起来确实很省事.
数据的采集
Prometheus 和 InfluxDB 在数据的采集上两者就选择了不同的极端, 前者只能 pull, 后者只能 push, 关于 pull 和 push 的对比,这里暂不多述.
Prometheus 把 数据的采集器叫做 exporter, xxx-exporter 运行之后会在机器上占用一个端口, 等待 Prometheus server 拉取数据.
InfluxDB 的数据采集器我们使用了 telegraf, 官方宣传插件化驱动, 其实也就那么回事, 编译的时候把很多东西的采集功能包进去压在一个二进制里面, 再用配置文件控制插件是否开启. 功能其实是蛮强大了. 也是 Go 编写, 部署算不上困难, 但用起来总会觉得多了一点什么东西.
存在感.
没错, 多了一种存在感, 对于数据采集 agent 这种东西, 存在感越低越好.
Telegraf 的默认配置文件就多达2000多行, 里面包括 push 的目的地址, 各种插件的控制目等等.相比之下, Prometheus 的 exporter  不需要任何配置文件, 不需要任何依赖, 真正的开箱即用.
但 Telgraf 有一个很吸引人的功能, 就是它能够作为一个转发代理接受来自不同程序的消息
比如可以运行一段脚本, 将结果按照一定的格式输出给 telegraf 默认的8186端口, telegraf 再写进 InfluxDB, 这样就把一个特殊的第三方业务的数据采集起来了, 不需要重启 Telegraf, 也不需要重启 InfluxDB.
如果换用 Prometheus 要怎么做呢? 我们需要引入 prometheus 的 SDK 自己编写 exporter, 而且 prometheus 会有四种指标类型,编写完之后需要去 Prometheus server 重新配置要抓取的目标, 整个下来是比 Telegraf 那一套要麻烦的.
如果你的需求很特殊, 要监控的很多第三方特殊的指标, 而对于常见的资源如硬件,数据库等监控需求不大, 那么 Telegraf + InfluxDB 会是一个不错的组合.
数据的存储
单单比较数据存储的那一部分, 它们两者之间也有很多不同.
InfluxDB 的存储引擎是基于一种叫做TSM的自研引擎, Prometheus 则是柔和了 leveldb 与 自研的存储引擎.
总的趋势都是基于时序数据进行优化, 不仅要照顾读写性能, 还要照顾删除性能与稳定性.
不过不管怎样,性能与数据的压缩对于使用者来说都不是第一要考虑的因素, 不是不重要,而是因为他们两者都做得很棒, 对于一个监控系统来说.
在使用的灵活性方面, InfluxDB 是优于 Prometheus 的,这是由于他们的产品定位决定的: InfluxDB 是一个时序数据库, Prometheus 是一个附带数据库的监控系统.举个例子, InfluxDB 有类似 Mysql 中数据库, 表的概念, 而且可以针对每个数据库设置不同的存储策略, 不得不说这些功能对于一个专门存放数据的软件系统来说还是很有吸引力的.
数据的查询
在数据查询上面, InfluxDB 的查询语言 InfluxQL 与 SQL 类似, 但是不能像 SQL 那样做强大的表与表之间的操作.
Prometheus 的查询语言也很有特点, 看起来会像 JSON , 但是通过它也可以实现各种强大的查询操作.
下面分别是 InfluxDB 和 Prometheus 查询1分钟内 CPU 使用率的语句
SELECT 100 - usage_idel FROM &amp;quot;autogen&amp;quot;.&amp;quot;cpu&amp;quot; WHERE time &amp;gt; now() - 1m and &amp;quot;cpu&amp;quot;=&#39;cpu0&#39;

100 - (node_cpu{job=&amp;quot;node&amp;quot;,mode=&amp;quot;idle&amp;quot;}[1m]) 

如果硬要在查询语言上分个高低的话,我会选择 Prometheus, 原因很简单, 我觉得它更有友好与简单
高可用与集群功能
最后还要说一下集群和高可用性这块.很遗憾他们两个现在都做得不是很好,至少从免费的角度来说:)
InfluxDB 的集群功能是商业功能, 但是有一个高可用的套件叫做 Influxdb-relay, 这个一个跑在 InfluxDB 实例前面的一个转发代理, 数据经过它的时候会被分发到各个数据库实例上. 还凑合着能用吧,不过不支持 query 操作, 如果有需要的话可以参考这个fork https://github.com/shanexu/influxdb-relay
Prometheus 到目前为止还没有看到集群功能的消息, 高可用也是仅仅通过部署多个实例来实现, 这个方案是有局限性的, 就算不考虑资源的占用, 也会让系统的架构变得复杂.
笔者一直都在等待他们能一个高可用和集群部署的功能, 尤其是 Prometheus.因为毕竟总有公司需要数据有可靠性的保证的, 尤其是面向政企客户的公司.
有一段时间笔者学了 Raft 协议的后想要自己实现 InfluxDB 的集群功能, 但总是怕自己做不好, 花的时间打了水漂, 毕竟对于笔者这样的新手来选择把精力花在打实计算机基础以及扩充自己的知识面上面会更有性价比. 但是如果有哪位读者也有实现 InfluxDB 集群功能的想法, 请告诉我, 我很愿意一起协助...
报警
如果说在前面那两个方面 InfluxDB 和 Prometheus 还各有特点的话, 那么在报警这方面 InfluxDB 简直就是被 Prometheus 按在地上摩擦.
InfluxDB 官方出了一个叫做 Kapacitor 的软件, 官方说可以用它实现报警.
但是这货明明是拿来对 InfluxDB 做数据处理的, 用在监控系统的报警功能上面真的很差.
一方面是因为效率的原因, 它的工作原理是定时得去从 InfluxDB 取数据出来进行运算来检查是否触发报警条件, 而且万一数据库挂了的话岂不是报警也失效了 ? 一方面是它的 DSL 使用起来体验真心差, 谁用谁知道 !
总结
几个月体验下来, 笔者认为对于监控系统的选择来说, Prometheus 是不二之选,市场的反应也摆在我们面前了, 这就是趋势.
另外一方面, 如果你的业务不单单是监控系统, 还需要使用到一些时序数据库的特性用来存储其他数据, 那么也别纠结了, InfluxDB就是最适合的.

                                        </div>
                                        
                                            <a class="btn btn-text" href="https://lwhile.github.io/post/influxdb-vs-prometheus">Read More ~</a>
                            </div>
                </div>
            </article>
            
                <!-- 翻页 -->
                
    <div class="pagination-container">
        
                
                    <a href="https://lwhile.github.io/tag/GQVNWsaTp//page/2/" class="page-btn btn">下一页</a>
                    
    </div>
    
                </div>
                <!--  -->
                <div class="main-container-middle"></div>
                <!--  -->
                <div id="sidebar" class="main-container-right">

                    <!-- 个人信息 -->
                    
    <div class="id_card i-card">
        <div class="id_card-avatar" style="background-image: url(https://lwhile.github.io/images/avatar.png?v=1576341799975)">
        </div>
        <h1 class="id_card-title">
            神蛋杂谈
        </h1>
        <h2 class="id_card-description">
            
        </h2>
        <!--  -->
        <div class="id_card-sns">
            <!-- github -->
            
                    <!-- twitter -->
                    
                            <!-- weibo -->
                            
                                    <!-- facebook -->
                                    

        </div>
    </div>
    

                        <!-- 公告栏 -->
                        

                </div>
            </div>



            <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | 
  <a class="rss" href="https://lwhile.github.io/atom.xml" target="_blank">RSS</a>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
    <script>
        $('#sidebar').stickySidebar({
            topSpacing: 80,
            // bottomSpacing: 60
        });
    </script>
</body>

</html>