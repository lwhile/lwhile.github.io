<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://lwhile.github.io</id>
    <title>神蛋杂谈</title>
    <updated>2019-12-14T16:13:42.226Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://lwhile.github.io"/>
    <link rel="self" href="https://lwhile.github.io/atom.xml"/>
    <logo>https://lwhile.github.io/images/avatar.png</logo>
    <icon>https://lwhile.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, 神蛋杂谈</rights>
    <entry>
        <title type="html"><![CDATA[Go 时区用法总结]]></title>
        <id>https://lwhile.github.io/post/go-timezone</id>
        <link href="https://lwhile.github.io/post/go-timezone">
        </link>
        <updated>2019-02-19T15:31:05.000Z</updated>
        <content type="html"><![CDATA[<p>坊间有个说法，说中美两国的程序员除了薪资、加班强度外，还有个比较有趣的差异，就是在编码时对于时区的敏感程度也不同。</p>
<p>「大部分」美国程序员在时区问题上很有经验，因为美国在日常生活中会使用到 3 个时区。而中国「大部分」程序员在编码时基本不会考虑时区问题，因为大家都在东八区，没那个烦恼。</p>
<p>在过去一年我维护着一个公司内部的日历库，用于做交易时间的计算。在这个过程中被时区问题折磨得很难受，不过也积累一些 Go 处理时区上的经验。</p>
<h2 id="时区配置">时区配置</h2>
<p>时区的配置一定要声明在配置文件中，比如下面这种 yaml 格式：</p>
<blockquote>
<p>timezone: Asia/Hong_Kong</p>
</blockquote>
<p>别想着用机器时区一劳永逸这个问题，因为机器的变数太多了。</p>
<h2 id="载入时区">载入时区</h2>
<p>在 Go 中，时区被封装在 <code>time.Location</code> 中进行抽象，而载入时区有两种方法</p>
<ol>
<li>通过 IANA 数据库</li>
</ol>
<p>给 <code>time.LoadLocation</code> 传入一个符合 IANA 时区数据库的时区名字，比如：</p>
<blockquote>
<p>Asia/Hong_Kong</p>
</blockquote>
<ol start="2">
<li>通过偏移量</li>
</ol>
<p><code>time.FixedZone(&quot;UTC-8&quot;, -8 **60 **60)</code></p>
<h2 id="转换时区">转换时区</h2>
<p>比较简单的操作，直接调一个 time struct instance 的 <code>In(*Location)</code> 方法。</p>
<h2 id="修改时区">修改时区</h2>
<p>注意修改时区和转换时区是两种不同的概念。</p>
<p>比如北京时间 20:00pm 转换时区，以美东时区为例，会是 7:00am (冬令时)</p>
<p>而修改时区是指把北京 20:00pm 修改为美东 20:00pm，可以这样操作：</p>
<blockquote>
<p>// Pseudo code<br>
var estTime time.Time<br>
var cstTime time.Time<br>
var estTimezone *time.Location<br>
// 通过 time.Date 方法重新生成一个 time struct instance<br>
estTime := time.Date(cstTime.Year(), cstTime.Month(),..., estTimezone)</p>
</blockquote>
<h2 id="round-to-day">Round to day</h2>
<p>实在想不起一个名词可以形容 Round to day 的过程，我们用这个方法来做日内时间的比较。</p>
<p>比较典型的应用场景是判断是否到了开市/休市时间。由于在 Go 中 time 类型是包含有年、月信息的，所以要表达「每天早上 9:30」 这个时间不大好做。</p>
<p>既然不好做，那就把它们去掉。</p>
<blockquote>
<p>// Pseudo code<br>
func RoundToDay(t time.Time) time.Time {<br>
// 注意在 Go 中 zero time 并不是 0 year 0 month 0 day，而是 1，所以用 0 的话会溢出喔<br>
return time.Date(1,1,t.Day(), t.Hour() ...)<br>
}</p>
</blockquote>
<h2 id="lmt">LMT</h2>
<p>LMT 这个问题是在 Round to day 上面引发出来的，说起来也很有趣。LMT 本意是指 local mean time（地方平时，在指定的经度范围内使用一致时间的地方太阳时）。</p>
<p>如果你用试图在 Go 里面用香港时区构建一个 zero time 的话，就会发现最后返回出来的 time struct instance 的 location 信息是 LMT。这个问题是在一处单元测试中发现的，死活无法通过测试（嗯，单测真的很重要）。</p>
<p>不过这是为什么？</p>
<p>我相信知道这个知识点的人绝对少之又少，这是一个非常非常冷的知识（最后通过我那位知识储备量非常丰富的同事知道的这个）</p>
<blockquote>
<p>香港時間的授時服務，是香港天文台從1883年成立至今的主要職責。早期香港天文台使用赤道儀及中星儀，透過觀測星象測量時間。當時香港時間是當地平均時間（LMT）UTC+7:36:42（準確值為UTC+7:36:41.8842）。香港天文台最早於1885年1月1日對公眾授時[1]，當年香港天文台於九龍尖沙咀警署設置桅杆，以升降時間球的方式對外發布時間。1885年1月1日，香港天文台於中午12時50分把時間球升到桅杆頂端，然後於下午1時正，首次把時間球降下，成為香港首次報時訊號，並作為香港時間的標準。用於報時的時間球訊號塔，後來於1933年因為電台報時的開展而拆除。1904年10月30日，香港時間正式確定為格林威治標準時間快8小時（GMT+8）[2]。2004年，天文台安裝了一套高準確度授時系統，利用全球定位系統共視方法，向國際度量衡局提供天文台的原子鐘時間數據，參與訂定協調世界時。天文台亦根據國際度量衡局提供的時間數據調校原子鐘，使其準確度保持在一百萬分之一秒以內。現時，香港天文台以銫原子鐘報時系統作為香港時間的標準，誤差僅為每日1微秒之內。香港天文台設有互聯網時間伺服器，為互聯網的用戶提供準確的時間校正服務[3]。</p>
</blockquote>
<p>根据上面的记录来看几个例子吧，首先是 1904 年 10 月 30 日后香港使用 GMT+8，我们把时间定格在它前一秒：</p>
<blockquote>
<p>location, _ := time.LoadLocation(&quot;Asia/Hong_Kong&quot;)</p>
</blockquote>
<p>t := time.Date(1904,10,29,11,59,59,59,location)</p>
<p>fmt.Println(t) // 1904-10-29 11:59:59.000000059 +0736 LMT &lt;- LMT 无疑</p>
<p>然后是 1904 年 10 月 30 日 零点：</p>
<blockquote>
<p>location, _ := time.LoadLocation(&quot;Asia/Hong_Kong&quot;)</p>
</blockquote>
<p>t := time.Date(1904,10,30,0,0,0,0,location)</p>
<p>fmt.Println(t) // 1904-10-30 00:23:18 +0800 HKT &lt;- HKT 了。</p>
<p>厉害吧。。。还有更厉害的。。。</p>
<blockquote>
<p>location, _ := time.LoadLocation(&quot;Asia/Hong_Kong&quot;)</p>
</blockquote>
<p>t := time.Date(1942,10,30,0,0,0,0,location)</p>
<p>fmt.Println(t) // 1942-10-30 00:00:00 +0900 JST</p>
<p>用 1942 年 10 月 30 日构建一个香港时间，时区会是（JST）日本标准时间。Why?</p>
<p>原因是这个：</p>
<blockquote>
<p>香港日佔時期，又稱為香港日治時期或香港淪陷時期，是指第二次世界大戰時大日本帝國軍事占領香港的時期：由1941年12月25日香港總督楊慕琦投降起，至1945年8月15日日本無條件投降為止；香港人俗稱這段時期為「三年零八個月」。</p>
</blockquote>
<p>别以为 Go 的开发团队这么有心，实际上 Go 也是读操作系统的 zoneinfo 文件。只能说在某个层面上操作系统也记录着每个地区的历史。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[指数退避]]></title>
        <id>https://lwhile.github.io/post/exponential-backoff</id>
        <link href="https://lwhile.github.io/post/exponential-backoff">
        </link>
        <updated>2018-04-02T15:41:39.000Z</updated>
        <content type="html"><![CDATA[<p>这是加入新公司后的第一篇技术博文，主角是一种叫做指数退避的算法。还是按照以往的惯例，这篇文章会从问题的发生至问题的解决记一个流水账。</p>
<p>问题的场景发生业务系统中的 RPC 调用中，需求是希望如果一个 RPC 调用失败了，能够进行重试。</p>
<p>作为一个菜鸟，我很快写入了第一个版本，用 Go 语言描述出来的逻辑如下：</p>
<pre><code class="language-go">func retry(maxRetry int, f func() error) {
    for i:=0;i&lt;maxRetry;i++ {
        err := f()
        if err == nil {
          return 
        }
    }
}
</code></pre>
<p>很快这种代码在 review 的时候就被打下来了。理由是作为网络调用，这种重试机制是不合理的。试想以下如果是遇到机器重启或者网络抖动，对方的服务要在 1 分钟甚至更久之后才能恢复正常，那么将重试放在一个没有停歇的 for 循环里面，试错的机会很快就会被用完。</p>
<p>mentor 提示我可以试下指数退避算法（exponential backoff），网上看了一些算法的介绍，很快就写了第二个版本出来。</p>
<p>逻辑还是差不多，只不过多了一步休眠的操作：</p>
<pre><code class="language-go">func retry(maxRetry int, f func() error) {
    for i:=0;i&lt;maxRetry;i++ {
        err := f()
        if err == nil {
          return 
        }
        // calculate the dynamic duration
        dur := call()

        time.Sleep(dur)
    }
}
</code></pre>
<p>算法的核心在于计算出重试的间隔，如果重试失败，那么就要相应得延长下次重试的时间。</p>
<p>重试间隔计算公式如下：<br>
sleep = min(cap, base * 2 ** attempt)</p>
<p>随着重试次数 attempt 的增加，sleep 的数值会出现指数级的变化。</p>
<p>对于一些服务比如涉及到事务的数据库操作，如果有多个 client 同时使用上面这条公式的话，会出现请求挤压的情况导致系统的整体吞吐量下降，这时候我们可以引入一个随机因子避免该问题，经过修改的公式如下：</p>
<p>sleep = random_between(0, min(cap, base * 2 ** attempt))</p>
<p>如果你使用的是Go，推荐下这个库：https://github.com/cenkalti/backoff</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何设计监控平台的告警组件]]></title>
        <id>https://lwhile.github.io/post/how-to-design-monitor-platform</id>
        <link href="https://lwhile.github.io/post/how-to-design-monitor-platform">
        </link>
        <updated>2017-11-19T16:12:23.000Z</updated>
        <content type="html"><![CDATA[<p>当业务发展到一定程度的时候，开发人员会开始考虑在系统中引入监控系统来对系统/业务进行监控。绝大多数监控系统都有两大核心功能，一个是工程师通过这个监控系统，能够对整个系统的运行情况一目了然，另外一个，就是当发生意外情况的时候，监控系统能将事件通知到人手上，毕竟人不可能24小时都在工作。这篇文章将要介绍的，就是第二个核心功能的承担着，告警组件。</p>
<h2 id="项目背景">项目背景</h2>
<p>我们公司目前的监控系统采用的是TICK架构中的TI,即用Telegraf采集数据，Influxdb做存储。C我们用了更加流行的Grafana做了替换。至于我们为什么选择了Influxdb，可以参考这篇文章：InfluxDB与Prometheus用于监控系统上的对比</p>
<p>剩下K，即Kapacitor,我们最后抛弃了它，主要还是因为Kapacitor的太过于臃肿，上手和维护成本太高，很多功能我们都用不上，还不如自己开发一个。而Grafana的报警功能其实还可以，但是对于我们来说有个不大不小的缺陷，这里就不提了。于是自己心里先把实现思路过了一遍，觉得能Hold得住，就向领导请示想自己开发，接下来轮子就造起来了。</p>
<h2 id="需求与设计原则">需求与设计原则</h2>
<p>作为一个核心组件，我给自己先定了一个最基本的目标：稳定。功能多不多、炫不炫要让位给稳定性。</p>
<p>接下来开始思考告警组件的两个基本需求：通知，和异常事件的发现。</p>
<p>通知方面，邮件的方式是必不可少的。另外因为我们公司有自己的IM产品线，所以支持Webhook也是要留在考虑项里面。至于短信这些和客户的需求耦合度比较高，所以暂不考虑。</p>
<p>而异常事件的发现，我选择参考Kapacitor的方式，主动去DB做查询，拿到数据再做触发的判断。这种方式有个缺点，如果数据库挂了，那么数据的流入端就断了，这为系统的可用性增加了一个不确定性因素。但我们在Influxdb前面使用relay做了高可用网关，而在我们集群中relay也是高可用的，这可以抵消掉一些上面的不确定性因素。</p>
<p>但是反过来，如果参考Prometheus或者Open-Falcon的方式，在数据送入数据库之前，先经过告警组做判断，我们目前的监控系统就需要增加一个网关，或者将告警功能嵌入relay里面，这样一来相当于监控的数据流在进入DB前会经过两扇门，每一扇都会降低整个系统一定的吞吐量。还有一个点不得不考虑，对配置的每一次修改都需要重启程序，这势必会造成数据的丢失。为了解决这个问题，Prometheus和Open-Falcon都是将待进入DB的数据复制一份，导到告警组件这里来，而这又需要对采集组件的配合。所以基于上面基点的考虑，我就选择了主动去DB做查询的方式。</p>
<p>说完上面两个基本需求，还有一个定制化的需求也要考虑，我们希望能在我们的虚拟机管理平台上像主流的公有云厂商一样能够让用户配置告警的策略，所以这引入另外一个需求：对外暴露易操作的API，让前端妹子调用。</p>
<h2 id="内部设计">内部设计</h2>
<p>明确了基本需求后，接下来开始内部的设计。为了理清思路，我写下了一份我（从使用者的角度）想要的配置文件（yaml格式）,来帮助我对事物进行抽象：</p>
<pre><code class="language-go">alert:
- name: 宿主机监控
type: influxdb
url: http://120.25.127.4:8086
db: telegraf
interval: 2s
query:
- name: cpu空闲值
sql: SELECT usage_idle FROM cpu WHERE time &gt; now() - 1m
threshold: 100
op: &quot;&lt;=&quot;
- name: 内存使用率
sql: SELECT used_percent FROM mem WHERE time &gt; now() - 1m
op: &quot;&gt;=&quot;
threshold: 50

notifier:
- name: 测试组
enable: true
type: mail 
host: smtp.163.com
port: 25
username: qq912293672@163.com
password: xxxxxx
from: qq912293672@163.com
to: [912293672@qq.com]
</code></pre>
<p>从配置文件上可以看到，我对告警组件抽象出了两个大的划分，一个是alert,抽象了数据的获取。一个是notifier,抽象了事件的通知。</p>
<p>在alert里面，我赋予了alert几个属性，其中type用来标识数据库的类型，因为我希望这个告警组件能支持多个存储后端。接下来是query，sql属性让用户自定义数据的查询方式，并且用threshold和op表示触发的阀值以及如何触发。总的来说，我采用了将多个查询实体组合成一个告警单位。这是经过思考的结果，目的是为了避免通知风暴：即很多机器很不幸都出异常的时，多个事件将聚合成一个告警，而不是发出多个邮件，而每个邮件的内容却很少。</p>
<p>而notifier的配置，我特意添加了type,也是为了支持多种通知方式，以及一个开关enable。</p>
<p>将notifier与alert分开，以及alert中包含query的设计，其实也是从Grafana和prometheus中学到的思路。在此感谢下今天的开源文化，让我等普通人有机会学到别人优秀的设计理念。</p>
<p>抽象得差不多后，可以考试编码了，按照分类，将代码主要分为3个模块，notify，alert,service。其中service对应我们上面的第三个需求，对外暴露API。</p>
<h2 id="接口设计">接口设计</h2>
<p>开发语言上我使用的是Go，我们将会有多个query在执行，这刚好对上的Go的强项，并发。下面看下几个主要的接口：</p>
<pre><code class="language-go">// Executor :type Executor interface {
    Execute() ([]Result, error)
    Interval() time.Duration
    Config() Config
    Close() error
}
</code></pre>
<p>因为alert其实可以当做一个获取数据的执行单位，所以我在这里又抽象出了一个执行器Executor,接下来我们只需要让我们Alert实现该接口，就能被调用执行。</p>
<pre><code class="language-go">type Analyzer interface {
    Analyze(string, interface{}, QueryConfig) (Result, bool)
}
</code></pre>
<p>Analyzer接口实现对各个监控系统数据处理。</p>
<pre><code class="language-go">type Result interface {
    String() string
    QueryName() string}
</code></pre>
<p>Result接口抽象了监控系统的返回数据，屏蔽掉各个监控系统之间的数据差异。</p>
<pre><code class="language-go">type Notifier interface {
    Send(content string) error
    Name() string
    Type() string
    To() []string
    Enable() bool
    Config() *Config
}
</code></pre>
<p>通知接口</p>
<p>接下来我们需要实现一个调度器，实现了对上面Executor的调度和控制：</p>
<pre><code class="language-go">type Scheduler interface {
    Run()
    AddExecutor(executor.Executor)
    RemoveExecutor(name string)
    ExecutorExist(name string) bool
    Stop()
}
</code></pre>
<p>核心调度逻辑：</p>
<pre><code class="language-go">runFn := func(schItem *scheduleItem) {
        bf := bytes.NewBufferString(&quot;&quot;)
        ticker := time.NewTicker(schItem.executor.Interval())
        notiMap := make(map[string]int)
        mutex := &amp;sync.RWMutex{}        for {            select {            // 定时器到期
            case &lt;-ticker.C:
                results, err := schItem.executor.Execute()                if err != nil {
                    log.Error(err)                    continue
                }
                notifiers, err := adaper.ReadAllNotifier()                if err != nil {
                    log.Error(err)                    continue
                }                for _, result := range results {                    // 对通知数进行累加
                    mutex.Lock()
                    notiMap[result.QueryName()]++
                    mutex.Unlock()                    // 通知数已经超过了限制
                    log.Debugf(&quot;notiMap:%+v&quot;, notiMap)
                    result := result                    if notiMap[result.QueryName()] &gt; notiSeqCount {                        if notiMap[result.QueryName()] == notiSeqCount+1 {                            go func(name string) {
                                time.Sleep(notiSleepDuration)
                                mutex.Lock()
                                notiMap[name] = 0
                                mutex.Unlock()
                            }(result.QueryName())
                        }                        continue
                    }                    if _, err := bf.WriteString(result.String()); err != nil {
                        log.Error(err)
                    }
                    bf.WriteString(&quot;&lt;br&gt;&quot;)
                }
                msgBody := bf.String()
                bf.Reset()                // 内容为空则跳过通知
                if msgBody == &quot;&quot; {                    continue
                }                // 遍历通知器将报警发送出去
                for _, notifier := range notifiers {
                    log.Debugf(&quot;bool:%v&quot;, notifier.Enable())                    if !notifier.Enable() {                        continue
                    }
                    notifier := notifier                    go func() {
                        err := notifier.Send(msgBody)                        if err != nil {
                            log.Errorf(&quot;Send %s notify to %s fail:%s&quot;, notifier.Type(), notifier.To(), err.Error())                            return
                        }
                        log.Infof(&quot;Send %s notify to %s success&quot;, notifier.Type(), notifier.To())
                    }()
                }            // 收到退出信号
            case &lt;-schItem.closeCh:
                ticker.Stop()
                log.Infof(&quot;Executor %s exit&quot;, schItem.executor.Config().Name)                return
            }
        }
    }
</code></pre>
<p>上面的调度控制中，为了避免某个异常事件在短时间没有解决时，我实现了自己的一个控制逻辑：当同一个query的通知已经连续超过3次时，我会让它定制通知半小时。若半小时异常还继续，则再发三次通知给接受者，如此循环下去。</p>
<p>最后还有HTTP API的实现以及对数据的存储。这属于常规的开发逻辑，和我们这个告警组件的关系不是很大，就不一一介绍了。</p>
<h2 id="总结">总结</h2>
<p>写这篇文章主要是总结下设计的思路，尤其是在几个核心问题上。在设计之初，除了告诉自己要保持住稳定性之外，还特别注意了如何对代码做到恰到好处的抽象，这也是最近半年看了那么多优秀开源项目的代码后的想法。老实说我这一次又对自己做得不满意，有机会我重构下整个组件。另外其实还有一个比较棘手的问题，就是如何让告警组件做到高可用（这无法通过简单部署多个服务就能实现，这样会导致通知事件的重复发送），这个问题最近正在解决，可以期待下一篇文章。</p>
<p>https://mp.weixin.qq.com/s/qr8WyroAWqx4D85J89RbvQ</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go语言究竟不一样在哪]]></title>
        <id>https://lwhile.github.io/post/why-go-so-different</id>
        <link href="https://lwhile.github.io/post/why-go-so-different">
        </link>
        <updated>2017-11-12T16:10:19.000Z</updated>
        <content type="html"><![CDATA[<p>第一次听到Go语言是在什么时间已经想不起来了。</p>
<p>我是14年进入的大学，在15年面试校内一个技术组织时，被问了解哪些服务端语言。那时候我提到了Go。不过正式把它列入学习计划，还要等到16年的十月份。后来在17年三月份出来实习使用Go做开发，才有更多的机会将它投入实践中。</p>
<p>在Go语言之前，我学习了C，C++， Java，Python。Go语言是唯一一门我认为自己能算得上掌握的语言。</p>
<p>这门语言最与众不同的地方在于，将写并发型的代码变得异常容易。另一个能在简洁程度（指并发）上能和它一比的，大概只有Actor世界里的Erlang/Elixir了。</p>
<p>在继续介绍Go之前我觉得有必要提下它的老爹。不过我要说的不是谷歌(Go最开始是谷歌的内部项目)，而是计算机领域的那三尊神。</p>
<p>Ken Thompson</p>
<p>Rob Pike</p>
<p>Robert Griesemer</p>
<p>如果你知道C语言的作者叫做Dennis Ritchie，并且看过它的一张图片，就是这张：<br>
<img src="http://upload-images.jianshu.io/upload_images/1244770-b4139e59247921a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<blockquote>
<p>你应该对这两位老头有印象，很多介绍C语言或者计算机导论的书都有这张照片。右边那位就是Dennis Ritchie，2011年他已经驾鹤西去，在他离世前的一个星期，地球上另一个传奇也走了，他叫做史蒂夫・乔布斯。那一年我刚考上高中，乔布斯走后凡客出了一件纪念衫和一本他的传记，我都买了，现在还留着。</p>
</blockquote>
<p>左边那位，就是另外的一尊神，Ken Thompson。Unix知道吧？他最先设计和实现出来的（我说最初的几个Unix版本内核是汇编写的你信吗？）C语言的前身—— B语言，他弄的。Plan9，他参与了。正则表达式和UTF-8编码的设计，他也凑了一脚。在大牛这个称谓泛滥的今天，我只能用神这个字去形容我的这位偶像了。</p>
<p>UTF-8还有另外一个发明者，和Ken一样也是Unix的成员，他就是目前Go语言的实际控制者，Rob Pike。你不一定会写Go代码，但你一定对Go那个萌贱萌贱的Logo有印象，这个Logo就是由Rob Pike的老婆设计的。<br>
<img src="http://upload-images.jianshu.io/upload_images/1244770-24024501bee66802.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>最后一尊神，Robert Griesemer，手握几个个赫赫有名的项目，其中两个每天都会有一大波程序员它们打交道。这两个项目就是V8和JVM。Go 1.3之后改变了GC的工作机制，据说就是由他操刀。</p>
<p>三位大神介绍完了，最终我想说的是，Go多多少少会凝聚他们三位在编程方面的宝贵经验，当初也是因为这一点我才会坚定得学习它，虽然我知道可能毕业的时候找不到一份Go这样一门新兴语言的工作。</p>
<p>好了，题外话不继续了，接下来开始列举Go的一些特性。</p>
<h2 id="1-goroutine">1. goroutine</h2>
<p>目前在并发编程领域，我们有3种最基本的模型选择。</p>
<p>第一种，即多进程/线程模型，这也是C/C++，Java最常用的并发模式。这种模型的好处是因为操作系统的原生支持，语言的实现者不用考虑其调度问题，交给操作系统就行了。但缺点就是会在并发数上升到一定程度后，系统需要将时间片更多花在进程/线程的上下文切换上。Linux默认的线程栈空间大小是1MB, 开一个1000个进程什么也不干，就需要接近1G的内存空间了。</p>
<p>第二种，是事件驱动机制，典型如Nginx和Node。我们都知道Nginx和Node扛并发的能力很强，但实际上占用的资源却极少，很大一部分将所有的事件通过一些特殊的数据结构（如红黑树）组织起来，等到事件发生的时候再放到一个单一的进程/线程去处理。这个模型只适合IO密集型的工作场景，因为事件驱动的本质只是充分利用起CPU的空闲时间。</p>
<p>第三种，就是Go使用的模型，协程模型，和Python，Lua，Erlang/Elixir里面的协程是同一个东西，只不过在Erlang/Elixir里面叫做process，Go里面叫做goroutine。我们知道线程比进程轻量，而协程则比线程更加轻量化。在Go里面，一个协程所占用的基本空间是2KB，只有线程的1/512。换句话说，起1000个空跑的goroutine，大概需要2MB的内存，而起1000个线程，则需要接近1GB。更低的内存占用，意味着可以开更多的并发单位，以及消耗更少的上下文切换时间。</p>
<p>可以看一个简单的例子：</p>
<pre><code class="language-go">func ShotOut() {
	// 休眠1秒钟
	time.Sleep(time.Second)

	// 向控制台抵茶
	fmt.Println(&quot;给大佬抵茶&quot;)
}

func main() {
	for i := 0; i &lt; 5; i++ {
		// 起5个goroutine
		go ShotOut()
	}

	// 别让main退出，作用类似C里面的getchar()
	select {}
}
</code></pre>
<p>如果没有使用并发，那么执行这个程序需要5s，但实际执行时间会在1s左右，这证明我们调用ShotOut函数的时候确实是并发了。</p>
<p>如果 Go 只是像 Python 或者Lua那样简单得引入了协程，那么它绝对不可能有今天的地位。Go的开发团队最伟大的地方在于，他们赋予了Go的runtime调度goroutine的能力，就像Linux内核调度线程那样。正是这一点，才让我们编写并发代码变得更加简单。</p>
<h2 id="2channel">2.channel</h2>
<p>说完goroutine，还有一个与它配合使用特性叫channel，可以说这两个特性加起来就锻造成了Go的屠龙宝刀。</p>
<p>上面的例子中起了5个goroutine，这5个并发单位都比较简单，彼此之间不需要通信。如果需要呢？channel就是用于相互隔离间的并发单位进行通信的一个消息队列。</p>
<p>看一个简单的例子：</p>
<pre><code class="language-go">func main() {
	// 声明一个存放int类型的channel
	ch := make(chan int)

	go func() {
		// 休眠1秒钟
		time.Sleep(time.Second)

		// 向channel写入整数1
		ch &lt;- 1
	}()

	go func() {
		// 等待从通道中取出内容
		res := &lt;-ch
		fmt.Println(res) // 1s后输出1
	}()

	// 别让main退出，作用类似C里面的getchar()
	select {}
}
</code></pre>
<p>我们对最上面的例子改下需求，要求第一个goroutine执行完毕后，才能继续执行另外两个。第二波执行的这两个goroutine执行完毕后，才能执行最后剩下的2个gourotine。下面是Go的实现，大家可以想下如果用C++或者Java要如何写：</p>
<pre><code class="language-go">func ShotOut() {
	// 休眠1秒钟
	time.Sleep(time.Second)

	// 向控制台抵茶
	fmt.Println(&quot;给大佬抵茶&quot;)
}

func main() {
	// 声明两个存放int类型的channel
	ch1 := make(chan int)
	ch2 := make(chan int)

	go func() {
		// 休眠1秒钟
		time.Sleep(time.Second)

		// 第1次跑抵茶函数
		ShotOut()

		// 抵完第1杯茶，向channel写入整数1
		ch1 &lt;- 1
	}()

	go func() {
		// 等待第1杯茶递完的信号
		&lt;-ch1

		// 收到信号，开始抵第2和递3杯茶
		ShotOut()
		ShotOut()

		// 通知最后2个
		ch2 &lt;- 1
	}()

	go func() {
		// 等待第2和第3杯茶递完
		&lt;-ch2

		// 递最后两杯
		ShotOut()
		ShotOut()
	}()

	// 别让main退出，作用类似C里面的getchar()
	select {}
}
</code></pre>
<blockquote>
<p>注意加了go关键字在面前的函数都会以并发的方式进行</p>
</blockquote>
<h2 id="3-无依赖运行">3. 无依赖运行</h2>
<p>如果说上面那两个特性要在并发环境下才能体现用处，那么Go可以编译成一个完成无依赖的二进制这一功能，是真的能大大提升你编程的意愿：因为你写的东西很容易传播给别人。</p>
<p>就拿能向浏览器输出Hello World的Web程序来举例子好了</p>
<pre><code class="language-go">pakcage main 

import &quot;fmt&quot;
import &quot;net/http&quot;

func HelloWorld(w http.ResponseWriter, r *http.Request) {
	w.Write([]byte(&quot;Hello World&quot;))
}

func main() {
	http.HandleFunc(&quot;/&quot;, HelloWorld)
	http.ListenAndServe(&quot;:8080&quot;, nil)
}
</code></pre>
<p>是的，你没看错，就这简单几句代码，跑起来它就是一个Web服务。而且通过 go build 命令编译后，将生成的二进制直接丢到服务器上就能跑了，Linux，Mac OS，Windows三平台都能用，而且性能非常强！想想如果是Java或者Python, Go的程序已经丢到服务器在跑了，Java的进度条可能还处在配置Tomcat上，Python则还在配置gunico和supervisor...</p>
<h3 id="最后">最后</h3>
<p>上面提到的三个特性，我觉得已经够向没了解过Go的用户介绍清楚他最与众不同的地方了。还有其他几个特性，比如非侵入性的接口，抛弃面相对象模型，多返回值，闭包等，这些相比其他语言我倒觉得不是其最大的亮点，留到以后有时间再介绍吧，这篇文章就已经写了我两个晚上了...</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[打个小卡]]></title>
        <id>https://lwhile.github.io/post/litter-memory-2017</id>
        <link href="https://lwhile.github.io/post/litter-memory-2017">
        </link>
        <updated>2017-11-10T16:06:20.000Z</updated>
        <content type="html"><![CDATA[<p>今天领到我人生中的第八份工资，解决掉这个月信用卡的账单，第一次有了超过5位数的存款。</p>
<p>元旦那天，我在奇妙清单里写下了几个目标，其中最重要的一个是能够去陌生城市独立生存下来。三月份的时候，很幸运得到一个实习的机会。如我所愿，真的成了一名程序员。</p>
<figure data-type="image" tabindex="1"><img src="http://upload-images.jianshu.io/upload_images/1244770-9f92a14aa91c2280.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></figure>
<p>接下来的那半年，每周要来回一次广州和肇庆，不断得平衡学校与公司之间的关系。入职的第一个星期，领导问我之前坚持过最久的一件事是什么，我说减肥。</p>
<p>这八个月中间心态确实有崩过，但最终还是坚持了下来，到了现在。</p>
<p>如果要问最近这一年我得到哪些成长，抛开我专业上的那些东西，我觉得最重要的，是将社会的模样从校园里面的那个轮廓，一天天得将细节渲染出来。见得越多，想要的越多，再看看自己有什么，心里就越来越恐慌，一点都不敢停下自己的脚步。</p>
<p>这周突然开始更新公众号，看到两年前自己推的那些东西，自己也会笑出来。大学终究还是在离我们远去，想起学校里面的生活，不由觉得大学生活真的是人生中最幸福的时光，怕是以后都不会有了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Etcd watch源码阅读]]></title>
        <id>https://lwhile.github.io/post/etcd-watch-source-code</id>
        <link href="https://lwhile.github.io/post/etcd-watch-source-code">
        </link>
        <updated>2017-11-10T15:57:50.000Z</updated>
        <content type="html"><![CDATA[<p>公司的业务里面使用了Consul做服务发现, 发现其有一个watch机制.这个watch机制引起我的好奇, 因为刚好在看Etcd-raft的代码, Etcd也有类似的watch机制, 所以趁热打铁, 赶紧趁周末研究下etcd watch机制源码的实现.</p>
<p>在看源码之前, 我们通过一个简单的例子, 看看Etcd的watch是如何使用的.</p>
<ol>
<li>先往Etcd写入一对KV</li>
</ol>
<blockquote>
<p>curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=&quot;神蛋使者&quot;</p>
</blockquote>
<ol start="2">
<li>Watch这对KV</li>
</ol>
<blockquote>
<p>curl http://127.0.0.1:2379/v2/keys/name?wait=true</p>
</blockquote>
<p>如果一切正常, 这时候请求会被阻塞住.</p>
<ol start="3">
<li>新开一个终端, 修改存进去的KV</li>
</ol>
<blockquote>
<p>curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=神蛋使者1号</p>
</blockquote>
<ol start="4">
<li>阻塞的那个请求返回watch到的结果</li>
</ol>
<pre><code class="language-json">{
  &quot;action&quot;:&quot;set&quot;,
  &quot;node&quot;:{ 
      &quot;key&quot;:&quot;/name&quot;,
      &quot;value&quot;:&quot;神蛋使者1号&quot;,
      &quot;modifiedIndex&quot;:25,
     &quot;createdIndex&quot;:25
  },
   &quot;prevNode&quot;: {
     &quot;key&quot;:&quot;/name&quot;,
     &quot;value&quot;:&quot;神蛋使者&quot;,
     &quot;modifiedIndex&quot;:24,
     &quot;createdIndex&quot;:24
   }
  }
</code></pre>
<p>体验流程大概就是这样, 下面正式看源码.</p>
<h2 id="接口定义">接口定义</h2>
<pre><code class="language-go">type Watcher interface {
	// Watch watches on a key or prefix. The watched events will be returned
	// through the returned channel.
	// If the watch is slow or the required rev is compacted, the watch request
	// might be canceled from the server-side and the chan will be closed.
	// 'opts' can be: 'WithRev' and/or 'WithPrefix'.
	Watch(ctx context.Context, key string, opts ...OpOption) WatchChan

	// Close closes the watcher and cancels all watch requests.
	Close() error
}
</code></pre>
<p>该接口定义了两个方法, Watch 和 Close</p>
<p>Watch 方法返回一个WatchChan 类似的变量, WatchChan是一个channel, 定义如下:</p>
<pre><code class="language-go">type WatchChan &lt;-chan WatchResponse
</code></pre>
<p>该通道传递WatchResponse类型</p>
<pre><code class="language-go">type WatchResponse struct {
	Header pb.ResponseHeader
	Events []*Event

	// CompactRevision is the minimum revision the watcher may receive.
	CompactRevision int64

	// Canceled is used to indicate watch failure.
	// If the watch failed and the stream was about to close, before the channel is closed,
	// the channel sends a final response that has Canceled set to true with a non-nil Err().
	Canceled bool

	// Created is used to indicate the creation of the watcher.
	Created bool

	closeErr error
}
</code></pre>
<p>其中Event类型是一个gRPC生成的消息对象</p>
<pre><code class="language-go">type Event struct {
	// type is the kind of event. If type is a PUT, it indicates
	// new data has been stored to the key. If type is a DELETE,
	// it indicates the key was deleted.
	Type Event_EventType `protobuf:&quot;varint,1,opt,name=type,proto3,enum=mvccpb.Event_EventType&quot; json:&quot;type,omitempty&quot;`
	// kv holds the KeyValue for the event.
	// A PUT event contains current kv pair.
	// A PUT event with kv.Version=1 indicates the creation of a key.
	// A DELETE/EXPIRE event contains the deleted key with
	// its modification revision set to the revision of deletion.
	Kv *KeyValue `protobuf:&quot;bytes,2,opt,name=kv&quot; json:&quot;kv,omitempty&quot;`
	// prev_kv holds the key-value pair before the event happens.
	PrevKv *KeyValue `protobuf:&quot;bytes,3,opt,name=prev_kv,json=prevKv&quot; json:&quot;prev_kv,omitempty&quot;`
}
</code></pre>
<p>接下来看实现了Watcher接口的watcher类型</p>
<pre><code class="language-go">// watcher implements the Watcher interface
type watcher struct {
	remote pb.WatchClient

	// mu protects the grpc streams map
	mu sync.RWMutex

	// streams holds all the active grpc streams keyed by ctx value.
	streams map[string]*watchGrpcStream
}
</code></pre>
<p>watcher结构很简单, 只有3个字段. remote抽象了发起watch请求的客户端, streams是一个map, 这个map映射了交互的数据流.还有一个保护并发环境下数据流读写安全的读写锁.</p>
<p>streams所属的watchGrpcStream类型抽象了所有交互的数据, 它的结构定义如下:</p>
<pre><code class="language-go">type watchGrpcStream struct {
	owner  *watcher
	remote pb.WatchClient

	// ctx controls internal remote.Watch requests
	ctx context.Context
	// ctxKey is the key used when looking up this stream's context
	ctxKey string
	cancel context.CancelFunc

	// substreams holds all active watchers on this grpc stream
	substreams map[int64]*watcherStream
	// resuming holds all resuming watchers on this grpc stream
	resuming []*watcherStream

	// reqc sends a watch request from Watch() to the main goroutine
	reqc chan *watchRequest
	// respc receives data from the watch client
	respc chan *pb.WatchResponse
	// donec closes to broadcast shutdown
	donec chan struct{}
	// errc transmits errors from grpc Recv to the watch stream reconn logic
	errc chan error
	// closingc gets the watcherStream of closing watchers
	closingc chan *watcherStream
	// wg is Done when all substream goroutines have exited
	wg sync.WaitGroup

	// resumec closes to signal that all substreams should begin resuming
	resumec chan struct{}
	// closeErr is the error that closed the watch stream
	closeErr error
}
</code></pre>
<p>比较有意思的是, watchGrpcStream也包含了一个watcher类型的owner字段, watcher和watchGrpcStream可以互相引用到对方.同时又定义了watcher类型中已经定义过的remote,而且还不是指针类型, 这点不大明白作用是啥.</p>
<p>还有几个字段值得关注, 一个是substreams, 看下它的定义和注释:</p>
<pre><code class="language-go">// substreams holds all active watchers on this grpc stream
substreams map[int64]*watcherStream
</code></pre>
<p>再看看watcherStream类型的定义:</p>
<pre><code class="language-go">// watcherStream represents a registered watcher
type watcherStream struct {
	// initReq is the request that initiated this request
	initReq watchRequest

	// outc publishes watch responses to subscriber
	outc chan WatchResponse
	// recvc buffers watch responses before publishing
	recvc chan *WatchResponse
	// donec closes when the watcherStream goroutine stops.
	donec chan struct{}
	// closing is set to true when stream should be scheduled to shutdown.
	closing bool
	// id is the registered watch id on the grpc stream
	id int64

	// buf holds all events received from etcd but not yet consumed by the client
	buf []*WatchResponse
}
</code></pre>
<p>画个图整理下他们之间的关系:</p>
<figure data-type="image" tabindex="1"><img src="http://upload-images.jianshu.io/upload_images/1244770-8d56f4f0d90de613.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="下载.png"></figure>
<p>接下来轮到watcher是如何watch方法的了:</p>
<pre><code class="language-go">// Watch posts a watch request to run() and waits for a new watcher channel
func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan {
	// 应用配置
	ow := opWatch(key, opts...)

	var filters []pb.WatchCreateRequest_FilterType
	if ow.filterPut {
		filters = append(filters, pb.WatchCreateRequest_NOPUT)
	}
	if ow.filterDelete {
		filters = append(filters, pb.WatchCreateRequest_NODELETE)
	}

	// 根据传入的参数构造watch请求
	wr := &amp;watchRequest{
		ctx:            ctx,
		createdNotify:  ow.createdNotify,
		key:            string(ow.key),
		end:            string(ow.end),
		rev:            ow.rev,
		progressNotify: ow.progressNotify,
		filters:        filters,
		prevKV:         ow.prevKV,
		retc:           make(chan chan WatchResponse, 1),
	}

	ok := false
	// 将请求上下文格式化为字符串
	ctxKey := fmt.Sprintf(&quot;%v&quot;, ctx)

	// find or allocate appropriate grpc watch stream
	// 接下来配置对应的输出流, 注意得加锁
	w.mu.Lock()

	// 如果stream为空, 返回一个已经关闭的channel.
	// 这种情况应该是防止streams为空的情况
	if w.streams == nil {
		// closed
		w.mu.Unlock()
		ch := make(chan WatchResponse)
		close(ch)
		return ch
	}

	// 注意这里, 前面我们提到streams是一个map,该map的key是请求上下文
	// 如果该请求对应的流为空,则新建
	wgs := w.streams[ctxKey]
	if wgs == nil {
		wgs = w.newWatcherGrpcStream(ctx)
		w.streams[ctxKey] = wgs
	}
	donec := wgs.donec
	reqc := wgs.reqc
	w.mu.Unlock()

	// couldn't create channel; return closed channel
        // couldn't create channel; return closed channel
	// 这里要设置为缓冲的原因可能与下面的两个
	// closeCh &lt;- WatchResponse{closeErr: wgs.closeErr}
	// 语句有关,这里不理解
	closeCh := make(chan WatchResponse, 1)

	// submit request
	select {
	// 发送上面构造好的watch请求给对应的流
	case reqc &lt;- wr:
		ok = true
	// 请求断开(这里应该囊括了客户端请求断开的所有情况)
	case &lt;-wr.ctx.Done():
	// watch完成
	// 这里应该是处理非正常完成的情况
	// 注意下面的重试逻辑
	case &lt;-donec:
		if wgs.closeErr != nil {
			// 如果不是空上下文导致流被丢弃的情况
			// 则不应该重试
			closeCh &lt;- WatchResponse{closeErr: wgs.closeErr}
			break
		}
		// retry; may have dropped stream from no ctxs
		return w.Watch(ctx, key, opts...)
	}

	// receive channel
	// 如果是初始请求顺利发送才会执行这里
	if ok {
		select {
		case ret := &lt;-wr.retc:
			return ret
		case &lt;-ctx.Done():
		case &lt;-donec:
			if wgs.closeErr != nil {
				closeCh &lt;- WatchResponse{closeErr: wgs.closeErr}
				break
			}
			// retry; may have dropped stream from no ctxs
			return w.Watch(ctx, key, opts...)
		}
	}

	close(closeCh)
	return closeCh
}
</code></pre>
<p>还有Watcher接口的另一个方法Close:</p>
<pre><code class="language-go">func (w *watcher) Close() (err error) {
	// 在锁内先将streams字段置为空
	// 在锁外再将一个个流都关闭
	// 这样做的意义在于不管哪个流关闭失败了
	// 都能先保证streams与这些流的关系被切断
	w.mu.Lock()
	streams := w.streams
	w.streams = nil
	w.mu.Unlock()
	for _, wgs := range streams {
		if werr := wgs.Close(); werr != nil {
			err = werr
		}
	}
	// etcd竟然也只是返回一个error
	// 虽然上面的for循环可能产生多个error
	return err
}
</code></pre>
<p>这样watcher就实现了Watcher接口.大致的实现思路本文就介绍到这里,剩下的代码也都是对其他相关数据结构的逻辑包装操作.</p>
<p>简单阅读Etcd的这一小部分源码下来, 我看到他们源码中的两个东西,算是Golang或者编程上面的一些最佳实践:</p>
<ol>
<li>
<p>对包外只暴露一个公共接口, 包内的结构体实现该接口即可.就像本文中的Watcher接口和watcher结构体.这样有两个好处, 一个就是代码能够解耦,还有就是可以省去命名的苦恼(<em><sup>__</sup></em>)</p>
</li>
<li>
<p>另一个是注释的书写方式,我发现etcd源码里的注释很大一部分写在变量的定义上面,而且变量的定义名都很清晰.</p>
</li>
<li>
<p>抽象得体.这个其实不只是Etcd, 其他任何优秀的开源作品都把他们的代码抽象得很到位.突然想起我写的那些渣渣代码%&gt;_&lt;%</p>
</li>
</ol>
<p>最后, 总结下etcd的watch机制.其实归根结底, 它的watch是通过gRPC的多路复用实现的,这是一个基于HTTP/2的特性.所以本文可能有些偏离了主题,探讨Etcd的watch机制, 其实应该研究HTTP/2才是.</p>
<p>算是给自己挖个坑.</p>
<!--在看源码之前, 可以猜想下如果是自己实现的话, 会采用什么思路? 如果是我, 我会使用gRPC, 之前体验过gRPC使用的HTTP/2多路复用机制, 实现watch确实很搭. 如果不使用gRPC(HTTP/2)呢 ? 假设HTTP请求可以让我们无限阻塞,不会超时断开, 后端该如何考虑 ?-->]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用Openresty构建认证网关]]></title>
        <id>https://lwhile.github.io/post/hi-openresty</id>
        <link href="https://lwhile.github.io/post/hi-openresty">
        </link>
        <updated>2017-11-07T15:51:32.000Z</updated>
        <content type="html"><![CDATA[<p>在单体应用中, 我们可以通过 cookie + session, 或者 JSON web token, 将认证逻辑在单体应用中实现, 简单高效, 还特别省事.</p>
<p>然而这几年随着服务化潮流越来越火(我觉得这是必然趋势, 想想我们人类社会是如何运作的), 很多以前单体应用不存在的问题, 现在已成为对单体应用拆分过程中的第一个障碍, 比如系统的认证体系.</p>
<p>如果每个拆出来的服务都要做一次认证(就是程序员多写几份认证的代码啦), 对于有理想有追求的灵魂の码农来说, 是绝对无法接受的.你说认证代码copy就好了, 不用重新写.no no no, 这样搞出来的架构不仅看着别扭, 代码闻着就觉得臭, 而且迟早有一天会出问题.</p>
<p>解决单体应用拆分服务后的认证问题其实很常规, 回想下祖师爷们帮我们总结的一句话: &quot;Any problem in computer science can be solved by another layer of indirection.&quot; 我们可以在所有服务前面增加一层认证服务.</p>
<figure data-type="image" tabindex="1"><img src="http://upload-images.jianshu.io/upload_images/1244770-7c58571258714b45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></figure>
<p>看到认证服务这一层用来作为用户请求的总入口, 有Nginx或者Apache使用经验的同学自然而然就想到它们. 如果能把认证模块的功能整合进Nginx或者Apache这些Web服务器, 那岂不是更完美 ?</p>
<p>而这篇文章的主角: Openresty,就可以帮助我们简单快速得完成这个想法.这是一个由<a href="https://github.com/agentzh">春哥(Github)</a>发起的项目.你可以将Openresty看做Nginx + 常用模块构成的软件包, 但是最重要的功能是我们可以使用Lua在Nginx实现Web框架才能实现的逻辑, 接下来文章将会开始介绍如何使用Openresty, 将上面提到的认证服务整合进Nginx里.</p>
<h2 id="安装">安装</h2>
<p>Openresty有两种安装方式, 一种是使用源码编译安装.一种使用官方提供的预编译包:<br>
具体可参考官网的<a href="https://openresty.org/cn/linux-packages.html">安装文档</a></p>
<h2 id="hello-world">Hello world</h2>
<p>如果你没修改过Openresty的安装位置, 默认会被安装在/use/local/openresty目录下.我们现在可以尝试写一个Hello world级别的demo.</p>
<p>为Openresty创建工作目录并创建配置文件:</p>
<blockquote>
<p>mkdir ~/openresty_work<br>
cd ~/openresty_work<br>
touch nginx.conf</p>
</blockquote>
<p>接下来在nginx.conf里面配置一个路由规则</p>
<pre><code>worker_processes 1;
error_log logs/error.log;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8080;
		location /hello {
			default_type text/html;
			content_by_lua_block {
				ngx.say(&quot;Hello Openresty.&quot;)
			}			
		}
	}
}

</code></pre>
<p>跟普通的Nginx配置文件比起来, 上面的配置多了一个content_by_lua_block指令, 正是通过调用该指令, 访问该路由的时候,才会输出相应的内容.这个指令是由Openresty中的<a href="http://openresty.org/cn/lua-nginx-module.html">LuaNginxModule</a>模块提供的功能, 请求进来的时候, Nginx会启动lua的虚拟机, 输出的内容则由lua提供.</p>
<p>我们可以使用<code>content_by_lua_file</code>指令替代<code>content_by_lua_block</code>, 将相关的lua代码写进文件里.</p>
<pre><code>location /hello {
			content_by_lua_file lua/hello.lua;
		}

</code></pre>
<pre><code class="language-lua">--- hello.lua
ngx.say(&quot;Hello Openresty.&quot;)
</code></pre>
<p>有了上面的铺垫,接下来可以开始构建我们的认证服务,认证的方式使用<a href="https://jwt.io/">JWT</a></p>
<p>Openresty将一个请求的生命周期划分为4个阶段:</p>
<figure data-type="image" tabindex="2"><img src="http://upload-images.jianshu.io/upload_images/1244770-7854b722bc3cf62c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></figure>
<p>我们的认证服务将会挂载在第二阶段, 即 Rewrite/Access Phase 上.</p>
<p>接下来准备一个需要用到的库:<br>
<a href="https://github.com/SkyLothar/lua-resty-jwt">lua-resty-jwt</a><br>
clone下来后放到hello.lua文件所在的文件夹,并将lua_package_path配置为:</p>
<pre><code>lua_package_path &quot;/root/openresty_work/lua/?.lua;/root/openresty_work/lua/lua-resty-jwt/lib/?.lua;;&quot;;
</code></pre>
<p>构建的思路也很简单, 对用户提供一个登录请求, 验证身份后将jwt token分发给用户.用户接下来访问需要认证的接口, 则在header里面加入该token, 请求进入Openresty后由lua提取出token进行认证.</p>
<p>Nginx配置文件</p>
<pre><code>server {
                listen 8080;
                location /hello {
                        content_by_lua_file lua/hello.lua;
                }
                location /login {
                        content_by_lua_file lua/sign.lua;
                }
                location /service1 {
                        access_by_lua_file lua/verify.lua;
                        # 需要反向代理在这配置
                }
                location /service {
                        access_by_lua_file lua/verify.lua;
                        # ...
                }
        }

</code></pre>
<p>下面是配置中相关的lua文件<br>
sign.lua ↓:</p>
<pre><code class="language-lua">local jwt = require 'resty.jwt'

-- 只允许POST请求
if ngx.req.get_method() ~= 'POST' then
    ngx.status = 405
    ngx.say(&quot;Mehtod Not Allow&quot;)
    return
end

-- 获取请求body
ngx.req.read_body()
local body_raw = ngx.req.get_body_data()
local body_json = cjson.decode(body_raw)
local username = body_json['username']
local password = body_json['password']

if not username or not password then
    ngx.log(ngx.ERR, username, password)
    ngx.status = 400
    ngx.say('无法获取账号或者密码')
    return
end

-- 验证账号和密码是否正确,如果验证失败则做如下处理
if not this_is_a_auth_method(username, password) then
    ngx.status = 401
    ngx.say('认证失败')
    return
end
</code></pre>
<p>verify.lua ↓:</p>
<pre><code>local jwt = require 'resty.jwt'

-- 从请求中提取header并从header从获取token字段
local headers = ngx.req.get_headers()
local token = headers['token']

-- 检查token是否存在
if not token then 
    ngx.status = 400
    ngx.say('无法获取token')
    return 
end 

-- 验证token
local jwt_obj = jwt:verify(vars.jwt_salt(), token)
if not jwt_obj['verified'] then 
    ngx.status = 401
    ngx.say('无效的token')
    return 
end 
</code></pre>
<p>至此一个使用Openresty构建的认证网关的雏形已经出来了.需要说明的一句是, 上面的代码由于没有公司相关的运行环境,笔者没有经过测试和验证.所以只可阅读,不可复制后直接运行 😃</p>
<p>如果想把这套认证网关用在生产环境上, 还有很多东西需要考虑.比如跨域问题, 静态文件的代理问题等等.</p>
<p>个人接触Openresty的时间也不长, 文中难免会有地方写错了或者表达得很差, 欢迎发评论或者发邮件给我指正: lwhile521@gmail.com ,感谢.</p>
<p>对于Openresty, 个人认为要对它产生兴趣,关键在于认不认可让Nginx承担除了Web服务器之外更多的业务, 对于Openresty, 它能带来的好处有:</p>
<ol>
<li>
<p>极致的性能.上文没有提到Openresty的性能, 其实Openresty的编程模型和NodeJS很像, 在Openresty的世界里面,所有东西都是非阻塞的,更难得可贵的是, 它不需要使用NodeJS中的回调函数, 代码写起来其实还是同步模型, 配合C语言编写的Nginx, 最快的脚本语言lua+luajit解释器,这套方案的性能无可挑剔了.</p>
</li>
<li>
<p>降低了Nginx模块的开发难度. Nginx + C/C++能做的, Openresty用lua都能做.开发效率高了, 性能还不怎么降, 何乐而不为呢?</p>
</li>
</ol>
<h3 id="参考资料">参考资料</h3>
<blockquote>
<ol>
<li><a href="https://www.gitbook.com/book/moonbingbing/openresty-best-practices">Openresty最佳实践</a></li>
<li><a href="https://github.com/SkyLothar/lua-resty-jwt">lua-resty-jwt</a></li>
</ol>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[致开发人员]]></title>
        <id>https://lwhile.github.io/post/for-developer</id>
        <link href="https://lwhile.github.io/post/for-developer">
        </link>
        <updated>2017-11-06T15:47:49.000Z</updated>
        <content type="html"><![CDATA[<p>「软技能:代码之外的生存指南」在开头部分有几句话,我看了之后觉得很棒,于是摘抄下来激励自己.</p>
<blockquote>
<p>谨以本书献给所有自强不息,孜孜不倦地持续自我改进的开发人员,他们具备以下素质:<br>
永远不会对&quot;不错&quot;感到心满意足<br>
永远寻求每一个机会来扩展自己的视野, 探索未知事物<br>
对知识的渴求永远不会熄灭<br>
笃信软件开发并不仅仅意味着编写代码<br>
知道失败不是结束,失败只是人生旅途上的小小一步<br>
有过挣扎,有过失败,但仍然会爬起来继续战斗<br>
拥有强烈意愿和决心,在人生的道路上不畏艰难<br>
以及最重要的,愿意一路上帮助他人</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[日志切分问题有感]]></title>
        <id>https://lwhile.github.io/post/ri-zhi-qie-fen-wen-ti-you-gan</id>
        <link href="https://lwhile.github.io/post/ri-zhi-qie-fen-wen-ti-you-gan">
        </link>
        <updated>2017-11-05T15:46:56.000Z</updated>
        <content type="html"><![CDATA[<p>有过服务端开发经验的同学应该对日志这个东西不陌生, 把程序丢到服务器上跑, 日志就是我们了解运行情况,甚至解BUG的唯一入口了.</p>
<p>有些程序的日志量会增长得非常快, 比如Nginx, 当一个日志文件大到几百MB甚至上GB的时候, 要从这个文件找出我们要的信息就基本等于大海捞针了,所以这时候对日志进行管理就显得格外重要.</p>
<p>日志量大的平台可以上ELK,利用ES的搜索优势基本不担心日志数据量大的问题.但本文不打算涉及这方面的内容.接下来主要讲讲如何正确得对日志文件做切分.</p>
<p>Linxu上的日志切分有两种形式, 一种是使用Linux的logrotate工具, 另外一种是使用额外编写的脚本, 这种形式一般是和日志库配合使用.</p>
<p>因为业务的关系, 我们最开始抛弃了使用logrotate的方案, 因为我们觉得这会给实施人员增加系统的的维护负担(后来发现是我们对logrotate不够了解).于是我们使用第二种方案, 将日志的切分操作在我们的日志库里面实现, 我们封装了logrus和lfshook, 利用logrus的hook机制将切分的逻辑嵌入在日志库里面,代码调用的时候会自动触发切分操作.我们会这样做也是受到beego框架的影响, 它的日志库默认就带了切分功能.</p>
<p>一切运作得很顺利, 直到我们有一次在使用Openresty的时候, 发现Nginx的日志没有被切分.因为之前使用Nginx的时候,默认安装完毕后日志是会自动以天切分的, 于是我们开始找Nginx的配置项,看看是否漏掉了某些配置.但是不找没关系,了解后才发现Nginx是不提供日志切分功能的.What ? 那之前的切分功能是怎么来的?</p>
<p>接下来解决问题的过程中发现了在/etc/logrotate.d/下有nginx的配置, 同时还有Mysql和其他基础组件的,这时我们才想到有可能是RPM包(我们的系统是Centos)安装的时候自动生成了一个logrotate的配置文件,后来一查果然是(命令:rpm -qpl xxx.rpm).而我们的Openresty包没有生成这个配置文件,所以导致Nginx的日志文件没有被切分.</p>
<p>实际上很多软件都只会做日志的记录,不会帮忙做切分,这个确实是合理的.这让我们想起logrus为什么不提供日志切分的功能,而是得由第三方的库去完成.我们将日志切分的逻辑耦合进代码里面,现在回过头来看其实也不是很合理,正确的做法其实还是应该在打RPM包的时候, 生成一个logrotate的配置文件, 这样一来也不会增加实施人员的负担,而且也可以将切分功能统一到一个地方去做.</p>
]]></content>
    </entry>
</feed>