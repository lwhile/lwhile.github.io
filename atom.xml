<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>lwhile.github.io</id>
    <title>神蛋杂谈</title>
    <updated>2019-12-14T16:01:23.098Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="lwhile.github.io"/>
    <link rel="self" href="lwhile.github.io/atom.xml"/>
    <logo>lwhile.github.io/images/avatar.png</logo>
    <icon>lwhile.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, 神蛋杂谈</rights>
    <entry>
        <title type="html"><![CDATA[致开发人员]]></title>
        <id>lwhile.github.io/post/for-developer</id>
        <link href="lwhile.github.io/post/for-developer">
        </link>
        <updated>2019-11-06T15:47:49.000Z</updated>
        <content type="html"><![CDATA[<p>「软技能:代码之外的生存指南」在开头部分有几句话,我看了之后觉得很棒,于是摘抄下来激励自己.</p>
<blockquote>
<p>谨以本书献给所有自强不息,孜孜不倦地持续自我改进的开发人员,他们具备以下素质:<br>
永远不会对&quot;不错&quot;感到心满意足<br>
永远寻求每一个机会来扩展自己的视野, 探索未知事物<br>
对知识的渴求永远不会熄灭<br>
笃信软件开发并不仅仅意味着编写代码<br>
知道失败不是结束,失败只是人生旅途上的小小一步<br>
有过挣扎,有过失败,但仍然会爬起来继续战斗<br>
拥有强烈意愿和决心,在人生的道路上不畏艰难<br>
以及最重要的,愿意一路上帮助他人</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go 时区用法总结]]></title>
        <id>lwhile.github.io/post/go-timezone</id>
        <link href="lwhile.github.io/post/go-timezone">
        </link>
        <updated>2019-02-19T15:31:05.000Z</updated>
        <content type="html"><![CDATA[<p>坊间有个说法，说中美两国的程序员除了薪资、加班强度外，还有个比较有趣的差异，就是在编码时对于时区的敏感程度也不同。</p>
<p>「大部分」美国程序员在时区问题上很有经验，因为美国在日常生活中会使用到 3 个时区。而中国「大部分」程序员在编码时基本不会考虑时区问题，因为大家都在东八区，没那个烦恼。</p>
<p>在过去一年我维护着一个公司内部的日历库，用于做交易时间的计算。在这个过程中被时区问题折磨得很难受，不过也积累一些 Go 处理时区上的经验。</p>
<h2 id="时区配置">时区配置</h2>
<p>时区的配置一定要声明在配置文件中，比如下面这种 yaml 格式：</p>
<blockquote>
<p>timezone: Asia/Hong_Kong</p>
</blockquote>
<p>别想着用机器时区一劳永逸这个问题，因为机器的变数太多了。</p>
<h2 id="载入时区">载入时区</h2>
<p>在 Go 中，时区被封装在 <code>time.Location</code> 中进行抽象，而载入时区有两种方法</p>
<ol>
<li>通过 IANA 数据库</li>
</ol>
<p>给 <code>time.LoadLocation</code> 传入一个符合 IANA 时区数据库的时区名字，比如：</p>
<blockquote>
<p>Asia/Hong_Kong</p>
</blockquote>
<ol start="2">
<li>通过偏移量</li>
</ol>
<p><code>time.FixedZone(&quot;UTC-8&quot;, -8 **60 **60)</code></p>
<h2 id="转换时区">转换时区</h2>
<p>比较简单的操作，直接调一个 time struct instance 的 <code>In(*Location)</code> 方法。</p>
<h2 id="修改时区">修改时区</h2>
<p>注意修改时区和转换时区是两种不同的概念。</p>
<p>比如北京时间 20:00pm 转换时区，以美东时区为例，会是 7:00am (冬令时)</p>
<p>而修改时区是指把北京 20:00pm 修改为美东 20:00pm，可以这样操作：</p>
<blockquote>
<p>// Pseudo code<br>
var estTime time.Time<br>
var cstTime time.Time<br>
var estTimezone *time.Location<br>
// 通过 time.Date 方法重新生成一个 time struct instance<br>
estTime := time.Date(cstTime.Year(), cstTime.Month(),..., estTimezone)</p>
</blockquote>
<h2 id="round-to-day">Round to day</h2>
<p>实在想不起一个名词可以形容 Round to day 的过程，我们用这个方法来做日内时间的比较。</p>
<p>比较典型的应用场景是判断是否到了开市/休市时间。由于在 Go 中 time 类型是包含有年、月信息的，所以要表达「每天早上 9:30」 这个时间不大好做。</p>
<p>既然不好做，那就把它们去掉。</p>
<blockquote>
<p>// Pseudo code<br>
func RoundToDay(t time.Time) time.Time {<br>
// 注意在 Go 中 zero time 并不是 0 year 0 month 0 day，而是 1，所以用 0 的话会溢出喔<br>
return time.Date(1,1,t.Day(), t.Hour() ...)<br>
}</p>
</blockquote>
<h2 id="lmt">LMT</h2>
<p>LMT 这个问题是在 Round to day 上面引发出来的，说起来也很有趣。LMT 本意是指 local mean time（地方平时，在指定的经度范围内使用一致时间的地方太阳时）。</p>
<p>如果你用试图在 Go 里面用香港时区构建一个 zero time 的话，就会发现最后返回出来的 time struct instance 的 location 信息是 LMT。这个问题是在一处单元测试中发现的，死活无法通过测试（嗯，单测真的很重要）。</p>
<p>不过这是为什么？</p>
<p>我相信知道这个知识点的人绝对少之又少，这是一个非常非常冷的知识（最后通过我那位知识储备量非常丰富的同事知道的这个）</p>
<blockquote>
<p>香港時間的授時服務，是香港天文台從1883年成立至今的主要職責。早期香港天文台使用赤道儀及中星儀，透過觀測星象測量時間。當時香港時間是當地平均時間（LMT）UTC+7:36:42（準確值為UTC+7:36:41.8842）。香港天文台最早於1885年1月1日對公眾授時[1]，當年香港天文台於九龍尖沙咀警署設置桅杆，以升降時間球的方式對外發布時間。1885年1月1日，香港天文台於中午12時50分把時間球升到桅杆頂端，然後於下午1時正，首次把時間球降下，成為香港首次報時訊號，並作為香港時間的標準。用於報時的時間球訊號塔，後來於1933年因為電台報時的開展而拆除。1904年10月30日，香港時間正式確定為格林威治標準時間快8小時（GMT+8）[2]。2004年，天文台安裝了一套高準確度授時系統，利用全球定位系統共視方法，向國際度量衡局提供天文台的原子鐘時間數據，參與訂定協調世界時。天文台亦根據國際度量衡局提供的時間數據調校原子鐘，使其準確度保持在一百萬分之一秒以內。現時，香港天文台以銫原子鐘報時系統作為香港時間的標準，誤差僅為每日1微秒之內。香港天文台設有互聯網時間伺服器，為互聯網的用戶提供準確的時間校正服務[3]。</p>
</blockquote>
<p>根据上面的记录来看几个例子吧，首先是 1904 年 10 月 30 日后香港使用 GMT+8，我们把时间定格在它前一秒：</p>
<blockquote>
<p>location, _ := time.LoadLocation(&quot;Asia/Hong_Kong&quot;)</p>
</blockquote>
<p>t := time.Date(1904,10,29,11,59,59,59,location)</p>
<p>fmt.Println(t) // 1904-10-29 11:59:59.000000059 +0736 LMT &lt;- LMT 无疑</p>
<p>然后是 1904 年 10 月 30 日 零点：</p>
<blockquote>
<p>location, _ := time.LoadLocation(&quot;Asia/Hong_Kong&quot;)</p>
</blockquote>
<p>t := time.Date(1904,10,30,0,0,0,0,location)</p>
<p>fmt.Println(t) // 1904-10-30 00:23:18 +0800 HKT &lt;- HKT 了。</p>
<p>厉害吧。。。还有更厉害的。。。</p>
<blockquote>
<p>location, _ := time.LoadLocation(&quot;Asia/Hong_Kong&quot;)</p>
</blockquote>
<p>t := time.Date(1942,10,30,0,0,0,0,location)</p>
<p>fmt.Println(t) // 1942-10-30 00:00:00 +0900 JST</p>
<p>用 1942 年 10 月 30 日构建一个香港时间，时区会是（JST）日本标准时间。Why?</p>
<p>原因是这个：</p>
<blockquote>
<p>香港日佔時期，又稱為香港日治時期或香港淪陷時期，是指第二次世界大戰時大日本帝國軍事占領香港的時期：由1941年12月25日香港總督楊慕琦投降起，至1945年8月15日日本無條件投降為止；香港人俗稱這段時期為「三年零八個月」。</p>
</blockquote>
<p>别以为 Go 的开发团队这么有心，实际上 Go 也是读操作系统的 zoneinfo 文件。只能说在某个层面上操作系统也记录着每个地区的历史。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[指数退避]]></title>
        <id>lwhile.github.io/post/exponential-backoff</id>
        <link href="lwhile.github.io/post/exponential-backoff">
        </link>
        <updated>2018-04-02T15:41:39.000Z</updated>
        <content type="html"><![CDATA[<p>这是加入新公司后的第一篇技术博文，主角是一种叫做指数退避的算法。还是按照以往的惯例，这篇文章会从问题的发生至问题的解决记一个流水账。</p>
<p>问题的场景发生业务系统中的 RPC 调用中，需求是希望如果一个 RPC 调用失败了，能够进行重试。</p>
<p>作为一个菜鸟，我很快写入了第一个版本，用 Go 语言描述出来的逻辑如下：</p>
<pre><code class="language-go">func retry(maxRetry int, f func() error) {
    for i:=0;i&lt;maxRetry;i++ {
        err := f()
        if err == nil {
          return 
        }
    }
}
</code></pre>
<p>很快这种代码在 review 的时候就被打下来了。理由是作为网络调用，这种重试机制是不合理的。试想以下如果是遇到机器重启或者网络抖动，对方的服务要在 1 分钟甚至更久之后才能恢复正常，那么将重试放在一个没有停歇的 for 循环里面，试错的机会很快就会被用完。</p>
<p>mentor 提示我可以试下指数退避算法（exponential backoff），网上看了一些算法的介绍，很快就写了第二个版本出来。</p>
<p>逻辑还是差不多，只不过多了一步休眠的操作：</p>
<pre><code class="language-go">func retry(maxRetry int, f func() error) {
    for i:=0;i&lt;maxRetry;i++ {
        err := f()
        if err == nil {
          return 
        }
        // calculate the dynamic duration
        dur := call()

        time.Sleep(dur)
    }
}
</code></pre>
<p>算法的核心在于计算出重试的间隔，如果重试失败，那么就要相应得延长下次重试的时间。</p>
<p>重试间隔计算公式如下：<br>
sleep = min(cap, base * 2 ** attempt)</p>
<p>随着重试次数 attempt 的增加，sleep 的数值会出现指数级的变化。</p>
<p>对于一些服务比如涉及到事务的数据库操作，如果有多个 client 同时使用上面这条公式的话，会出现请求挤压的情况导致系统的整体吞吐量下降，这时候我们可以引入一个随机因子避免该问题，经过修改的公式如下：</p>
<p>sleep = random_between(0, min(cap, base * 2 ** attempt))</p>
<p>如果你使用的是Go，推荐下这个库：https://github.com/cenkalti/backoff</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Etcd watch源码阅读]]></title>
        <id>lwhile.github.io/post/etcd-watch-source-code</id>
        <link href="lwhile.github.io/post/etcd-watch-source-code">
        </link>
        <updated>2017-11-10T15:57:50.000Z</updated>
        <content type="html"><![CDATA[<p>公司的业务里面使用了Consul做服务发现, 发现其有一个watch机制.这个watch机制引起我的好奇, 因为刚好在看Etcd-raft的代码, Etcd也有类似的watch机制, 所以趁热打铁, 赶紧趁周末研究下etcd watch机制源码的实现.</p>
<p>在看源码之前, 我们通过一个简单的例子, 看看Etcd的watch是如何使用的.</p>
<ol>
<li>先往Etcd写入一对KV</li>
</ol>
<blockquote>
<p>curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=&quot;神蛋使者&quot;</p>
</blockquote>
<ol start="2">
<li>Watch这对KV</li>
</ol>
<blockquote>
<p>curl http://127.0.0.1:2379/v2/keys/name?wait=true</p>
</blockquote>
<p>如果一切正常, 这时候请求会被阻塞住.</p>
<ol start="3">
<li>新开一个终端, 修改存进去的KV</li>
</ol>
<blockquote>
<p>curl http://127.0.0.1:2379/v2/keys/name -XPUT -d value=神蛋使者1号</p>
</blockquote>
<ol start="4">
<li>阻塞的那个请求返回watch到的结果</li>
</ol>
<pre><code class="language-json">{
  &quot;action&quot;:&quot;set&quot;,
  &quot;node&quot;:{ 
      &quot;key&quot;:&quot;/name&quot;,
      &quot;value&quot;:&quot;神蛋使者1号&quot;,
      &quot;modifiedIndex&quot;:25,
     &quot;createdIndex&quot;:25
  },
   &quot;prevNode&quot;: {
     &quot;key&quot;:&quot;/name&quot;,
     &quot;value&quot;:&quot;神蛋使者&quot;,
     &quot;modifiedIndex&quot;:24,
     &quot;createdIndex&quot;:24
   }
  }
</code></pre>
<p>体验流程大概就是这样, 下面正式看源码.</p>
<h2 id="接口定义">接口定义</h2>
<pre><code class="language-go">type Watcher interface {
	// Watch watches on a key or prefix. The watched events will be returned
	// through the returned channel.
	// If the watch is slow or the required rev is compacted, the watch request
	// might be canceled from the server-side and the chan will be closed.
	// 'opts' can be: 'WithRev' and/or 'WithPrefix'.
	Watch(ctx context.Context, key string, opts ...OpOption) WatchChan

	// Close closes the watcher and cancels all watch requests.
	Close() error
}
</code></pre>
<p>该接口定义了两个方法, Watch 和 Close</p>
<p>Watch 方法返回一个WatchChan 类似的变量, WatchChan是一个channel, 定义如下:</p>
<pre><code class="language-go">type WatchChan &lt;-chan WatchResponse
</code></pre>
<p>该通道传递WatchResponse类型</p>
<pre><code class="language-go">type WatchResponse struct {
	Header pb.ResponseHeader
	Events []*Event

	// CompactRevision is the minimum revision the watcher may receive.
	CompactRevision int64

	// Canceled is used to indicate watch failure.
	// If the watch failed and the stream was about to close, before the channel is closed,
	// the channel sends a final response that has Canceled set to true with a non-nil Err().
	Canceled bool

	// Created is used to indicate the creation of the watcher.
	Created bool

	closeErr error
}
</code></pre>
<p>其中Event类型是一个gRPC生成的消息对象</p>
<pre><code class="language-go">type Event struct {
	// type is the kind of event. If type is a PUT, it indicates
	// new data has been stored to the key. If type is a DELETE,
	// it indicates the key was deleted.
	Type Event_EventType `protobuf:&quot;varint,1,opt,name=type,proto3,enum=mvccpb.Event_EventType&quot; json:&quot;type,omitempty&quot;`
	// kv holds the KeyValue for the event.
	// A PUT event contains current kv pair.
	// A PUT event with kv.Version=1 indicates the creation of a key.
	// A DELETE/EXPIRE event contains the deleted key with
	// its modification revision set to the revision of deletion.
	Kv *KeyValue `protobuf:&quot;bytes,2,opt,name=kv&quot; json:&quot;kv,omitempty&quot;`
	// prev_kv holds the key-value pair before the event happens.
	PrevKv *KeyValue `protobuf:&quot;bytes,3,opt,name=prev_kv,json=prevKv&quot; json:&quot;prev_kv,omitempty&quot;`
}
</code></pre>
<p>接下来看实现了Watcher接口的watcher类型</p>
<pre><code class="language-go">// watcher implements the Watcher interface
type watcher struct {
	remote pb.WatchClient

	// mu protects the grpc streams map
	mu sync.RWMutex

	// streams holds all the active grpc streams keyed by ctx value.
	streams map[string]*watchGrpcStream
}
</code></pre>
<p>watcher结构很简单, 只有3个字段. remote抽象了发起watch请求的客户端, streams是一个map, 这个map映射了交互的数据流.还有一个保护并发环境下数据流读写安全的读写锁.</p>
<p>streams所属的watchGrpcStream类型抽象了所有交互的数据, 它的结构定义如下:</p>
<pre><code class="language-go">type watchGrpcStream struct {
	owner  *watcher
	remote pb.WatchClient

	// ctx controls internal remote.Watch requests
	ctx context.Context
	// ctxKey is the key used when looking up this stream's context
	ctxKey string
	cancel context.CancelFunc

	// substreams holds all active watchers on this grpc stream
	substreams map[int64]*watcherStream
	// resuming holds all resuming watchers on this grpc stream
	resuming []*watcherStream

	// reqc sends a watch request from Watch() to the main goroutine
	reqc chan *watchRequest
	// respc receives data from the watch client
	respc chan *pb.WatchResponse
	// donec closes to broadcast shutdown
	donec chan struct{}
	// errc transmits errors from grpc Recv to the watch stream reconn logic
	errc chan error
	// closingc gets the watcherStream of closing watchers
	closingc chan *watcherStream
	// wg is Done when all substream goroutines have exited
	wg sync.WaitGroup

	// resumec closes to signal that all substreams should begin resuming
	resumec chan struct{}
	// closeErr is the error that closed the watch stream
	closeErr error
}
</code></pre>
<p>比较有意思的是, watchGrpcStream也包含了一个watcher类型的owner字段, watcher和watchGrpcStream可以互相引用到对方.同时又定义了watcher类型中已经定义过的remote,而且还不是指针类型, 这点不大明白作用是啥.</p>
<p>还有几个字段值得关注, 一个是substreams, 看下它的定义和注释:</p>
<pre><code class="language-go">// substreams holds all active watchers on this grpc stream
substreams map[int64]*watcherStream
</code></pre>
<p>再看看watcherStream类型的定义:</p>
<pre><code class="language-go">// watcherStream represents a registered watcher
type watcherStream struct {
	// initReq is the request that initiated this request
	initReq watchRequest

	// outc publishes watch responses to subscriber
	outc chan WatchResponse
	// recvc buffers watch responses before publishing
	recvc chan *WatchResponse
	// donec closes when the watcherStream goroutine stops.
	donec chan struct{}
	// closing is set to true when stream should be scheduled to shutdown.
	closing bool
	// id is the registered watch id on the grpc stream
	id int64

	// buf holds all events received from etcd but not yet consumed by the client
	buf []*WatchResponse
}
</code></pre>
<p>画个图整理下他们之间的关系:</p>
<figure data-type="image" tabindex="1"><img src="http://upload-images.jianshu.io/upload_images/1244770-8d56f4f0d90de613.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="下载.png"></figure>
<p>接下来轮到watcher是如何watch方法的了:</p>
<pre><code class="language-go">// Watch posts a watch request to run() and waits for a new watcher channel
func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan {
	// 应用配置
	ow := opWatch(key, opts...)

	var filters []pb.WatchCreateRequest_FilterType
	if ow.filterPut {
		filters = append(filters, pb.WatchCreateRequest_NOPUT)
	}
	if ow.filterDelete {
		filters = append(filters, pb.WatchCreateRequest_NODELETE)
	}

	// 根据传入的参数构造watch请求
	wr := &amp;watchRequest{
		ctx:            ctx,
		createdNotify:  ow.createdNotify,
		key:            string(ow.key),
		end:            string(ow.end),
		rev:            ow.rev,
		progressNotify: ow.progressNotify,
		filters:        filters,
		prevKV:         ow.prevKV,
		retc:           make(chan chan WatchResponse, 1),
	}

	ok := false
	// 将请求上下文格式化为字符串
	ctxKey := fmt.Sprintf(&quot;%v&quot;, ctx)

	// find or allocate appropriate grpc watch stream
	// 接下来配置对应的输出流, 注意得加锁
	w.mu.Lock()

	// 如果stream为空, 返回一个已经关闭的channel.
	// 这种情况应该是防止streams为空的情况
	if w.streams == nil {
		// closed
		w.mu.Unlock()
		ch := make(chan WatchResponse)
		close(ch)
		return ch
	}

	// 注意这里, 前面我们提到streams是一个map,该map的key是请求上下文
	// 如果该请求对应的流为空,则新建
	wgs := w.streams[ctxKey]
	if wgs == nil {
		wgs = w.newWatcherGrpcStream(ctx)
		w.streams[ctxKey] = wgs
	}
	donec := wgs.donec
	reqc := wgs.reqc
	w.mu.Unlock()

	// couldn't create channel; return closed channel
        // couldn't create channel; return closed channel
	// 这里要设置为缓冲的原因可能与下面的两个
	// closeCh &lt;- WatchResponse{closeErr: wgs.closeErr}
	// 语句有关,这里不理解
	closeCh := make(chan WatchResponse, 1)

	// submit request
	select {
	// 发送上面构造好的watch请求给对应的流
	case reqc &lt;- wr:
		ok = true
	// 请求断开(这里应该囊括了客户端请求断开的所有情况)
	case &lt;-wr.ctx.Done():
	// watch完成
	// 这里应该是处理非正常完成的情况
	// 注意下面的重试逻辑
	case &lt;-donec:
		if wgs.closeErr != nil {
			// 如果不是空上下文导致流被丢弃的情况
			// 则不应该重试
			closeCh &lt;- WatchResponse{closeErr: wgs.closeErr}
			break
		}
		// retry; may have dropped stream from no ctxs
		return w.Watch(ctx, key, opts...)
	}

	// receive channel
	// 如果是初始请求顺利发送才会执行这里
	if ok {
		select {
		case ret := &lt;-wr.retc:
			return ret
		case &lt;-ctx.Done():
		case &lt;-donec:
			if wgs.closeErr != nil {
				closeCh &lt;- WatchResponse{closeErr: wgs.closeErr}
				break
			}
			// retry; may have dropped stream from no ctxs
			return w.Watch(ctx, key, opts...)
		}
	}

	close(closeCh)
	return closeCh
}
</code></pre>
<p>还有Watcher接口的另一个方法Close:</p>
<pre><code class="language-go">func (w *watcher) Close() (err error) {
	// 在锁内先将streams字段置为空
	// 在锁外再将一个个流都关闭
	// 这样做的意义在于不管哪个流关闭失败了
	// 都能先保证streams与这些流的关系被切断
	w.mu.Lock()
	streams := w.streams
	w.streams = nil
	w.mu.Unlock()
	for _, wgs := range streams {
		if werr := wgs.Close(); werr != nil {
			err = werr
		}
	}
	// etcd竟然也只是返回一个error
	// 虽然上面的for循环可能产生多个error
	return err
}
</code></pre>
<p>这样watcher就实现了Watcher接口.大致的实现思路本文就介绍到这里,剩下的代码也都是对其他相关数据结构的逻辑包装操作.</p>
<p>简单阅读Etcd的这一小部分源码下来, 我看到他们源码中的两个东西,算是Golang或者编程上面的一些最佳实践:</p>
<ol>
<li>
<p>对包外只暴露一个公共接口, 包内的结构体实现该接口即可.就像本文中的Watcher接口和watcher结构体.这样有两个好处, 一个就是代码能够解耦,还有就是可以省去命名的苦恼(<em><sup>__</sup></em>)</p>
</li>
<li>
<p>另一个是注释的书写方式,我发现etcd源码里的注释很大一部分写在变量的定义上面,而且变量的定义名都很清晰.</p>
</li>
<li>
<p>抽象得体.这个其实不只是Etcd, 其他任何优秀的开源作品都把他们的代码抽象得很到位.突然想起我写的那些渣渣代码%&gt;_&lt;%</p>
</li>
</ol>
<p>最后, 总结下etcd的watch机制.其实归根结底, 它的watch是通过gRPC的多路复用实现的,这是一个基于HTTP/2的特性.所以本文可能有些偏离了主题,探讨Etcd的watch机制, 其实应该研究HTTP/2才是.</p>
<p>算是给自己挖个坑.</p>
<!--在看源码之前, 可以猜想下如果是自己实现的话, 会采用什么思路? 如果是我, 我会使用gRPC, 之前体验过gRPC使用的HTTP/2多路复用机制, 实现watch确实很搭. 如果不使用gRPC(HTTP/2)呢 ? 假设HTTP请求可以让我们无限阻塞,不会超时断开, 后端该如何考虑 ?-->]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用Openresty构建认证网关]]></title>
        <id>lwhile.github.io/post/hi-openresty</id>
        <link href="lwhile.github.io/post/hi-openresty">
        </link>
        <updated>2017-11-07T15:51:32.000Z</updated>
        <content type="html"><![CDATA[<p>在单体应用中, 我们可以通过 cookie + session, 或者 JSON web token, 将认证逻辑在单体应用中实现, 简单高效, 还特别省事.</p>
<p>然而这几年随着服务化潮流越来越火(我觉得这是必然趋势, 想想我们人类社会是如何运作的), 很多以前单体应用不存在的问题, 现在已成为对单体应用拆分过程中的第一个障碍, 比如系统的认证体系.</p>
<p>如果每个拆出来的服务都要做一次认证(就是程序员多写几份认证的代码啦), 对于有理想有追求的灵魂の码农来说, 是绝对无法接受的.你说认证代码copy就好了, 不用重新写.no no no, 这样搞出来的架构不仅看着别扭, 代码闻着就觉得臭, 而且迟早有一天会出问题.</p>
<p>解决单体应用拆分服务后的认证问题其实很常规, 回想下祖师爷们帮我们总结的一句话: &quot;Any problem in computer science can be solved by another layer of indirection.&quot; 我们可以在所有服务前面增加一层认证服务.</p>
<figure data-type="image" tabindex="1"><img src="http://upload-images.jianshu.io/upload_images/1244770-7c58571258714b45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></figure>
<p>看到认证服务这一层用来作为用户请求的总入口, 有Nginx或者Apache使用经验的同学自然而然就想到它们. 如果能把认证模块的功能整合进Nginx或者Apache这些Web服务器, 那岂不是更完美 ?</p>
<p>而这篇文章的主角: Openresty,就可以帮助我们简单快速得完成这个想法.这是一个由<a href="https://github.com/agentzh">春哥(Github)</a>发起的项目.你可以将Openresty看做Nginx + 常用模块构成的软件包, 但是最重要的功能是我们可以使用Lua在Nginx实现Web框架才能实现的逻辑, 接下来文章将会开始介绍如何使用Openresty, 将上面提到的认证服务整合进Nginx里.</p>
<h2 id="安装">安装</h2>
<p>Openresty有两种安装方式, 一种是使用源码编译安装.一种使用官方提供的预编译包:<br>
具体可参考官网的<a href="https://openresty.org/cn/linux-packages.html">安装文档</a></p>
<h2 id="hello-world">Hello world</h2>
<p>如果你没修改过Openresty的安装位置, 默认会被安装在/use/local/openresty目录下.我们现在可以尝试写一个Hello world级别的demo.</p>
<p>为Openresty创建工作目录并创建配置文件:</p>
<blockquote>
<p>mkdir ~/openresty_work<br>
cd ~/openresty_work<br>
touch nginx.conf</p>
</blockquote>
<p>接下来在nginx.conf里面配置一个路由规则</p>
<pre><code>worker_processes 1;
error_log logs/error.log;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8080;
		location /hello {
			default_type text/html;
			content_by_lua_block {
				ngx.say(&quot;Hello Openresty.&quot;)
			}			
		}
	}
}

</code></pre>
<p>跟普通的Nginx配置文件比起来, 上面的配置多了一个content_by_lua_block指令, 正是通过调用该指令, 访问该路由的时候,才会输出相应的内容.这个指令是由Openresty中的<a href="http://openresty.org/cn/lua-nginx-module.html">LuaNginxModule</a>模块提供的功能, 请求进来的时候, Nginx会启动lua的虚拟机, 输出的内容则由lua提供.</p>
<p>我们可以使用<code>content_by_lua_file</code>指令替代<code>content_by_lua_block</code>, 将相关的lua代码写进文件里.</p>
<pre><code>location /hello {
			content_by_lua_file lua/hello.lua;
		}

</code></pre>
<pre><code class="language-lua">--- hello.lua
ngx.say(&quot;Hello Openresty.&quot;)
</code></pre>
<p>有了上面的铺垫,接下来可以开始构建我们的认证服务,认证的方式使用<a href="https://jwt.io/">JWT</a></p>
<p>Openresty将一个请求的生命周期划分为4个阶段:</p>
<figure data-type="image" tabindex="2"><img src="http://upload-images.jianshu.io/upload_images/1244770-7854b722bc3cf62c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></figure>
<p>我们的认证服务将会挂载在第二阶段, 即 Rewrite/Access Phase 上.</p>
<p>接下来准备一个需要用到的库:<br>
<a href="https://github.com/SkyLothar/lua-resty-jwt">lua-resty-jwt</a><br>
clone下来后放到hello.lua文件所在的文件夹,并将lua_package_path配置为:</p>
<pre><code>lua_package_path &quot;/root/openresty_work/lua/?.lua;/root/openresty_work/lua/lua-resty-jwt/lib/?.lua;;&quot;;
</code></pre>
<p>构建的思路也很简单, 对用户提供一个登录请求, 验证身份后将jwt token分发给用户.用户接下来访问需要认证的接口, 则在header里面加入该token, 请求进入Openresty后由lua提取出token进行认证.</p>
<p>Nginx配置文件</p>
<pre><code>server {
                listen 8080;
                location /hello {
                        content_by_lua_file lua/hello.lua;
                }
                location /login {
                        content_by_lua_file lua/sign.lua;
                }
                location /service1 {
                        access_by_lua_file lua/verify.lua;
                        # 需要反向代理在这配置
                }
                location /service {
                        access_by_lua_file lua/verify.lua;
                        # ...
                }
        }

</code></pre>
<p>下面是配置中相关的lua文件<br>
sign.lua ↓:</p>
<pre><code class="language-lua">local jwt = require 'resty.jwt'

-- 只允许POST请求
if ngx.req.get_method() ~= 'POST' then
    ngx.status = 405
    ngx.say(&quot;Mehtod Not Allow&quot;)
    return
end

-- 获取请求body
ngx.req.read_body()
local body_raw = ngx.req.get_body_data()
local body_json = cjson.decode(body_raw)
local username = body_json['username']
local password = body_json['password']

if not username or not password then
    ngx.log(ngx.ERR, username, password)
    ngx.status = 400
    ngx.say('无法获取账号或者密码')
    return
end

-- 验证账号和密码是否正确,如果验证失败则做如下处理
if not this_is_a_auth_method(username, password) then
    ngx.status = 401
    ngx.say('认证失败')
    return
end
</code></pre>
<p>verify.lua ↓:</p>
<pre><code>local jwt = require 'resty.jwt'

-- 从请求中提取header并从header从获取token字段
local headers = ngx.req.get_headers()
local token = headers['token']

-- 检查token是否存在
if not token then 
    ngx.status = 400
    ngx.say('无法获取token')
    return 
end 

-- 验证token
local jwt_obj = jwt:verify(vars.jwt_salt(), token)
if not jwt_obj['verified'] then 
    ngx.status = 401
    ngx.say('无效的token')
    return 
end 
</code></pre>
<p>至此一个使用Openresty构建的认证网关的雏形已经出来了.需要说明的一句是, 上面的代码由于没有公司相关的运行环境,笔者没有经过测试和验证.所以只可阅读,不可复制后直接运行 😃</p>
<p>如果想把这套认证网关用在生产环境上, 还有很多东西需要考虑.比如跨域问题, 静态文件的代理问题等等.</p>
<p>个人接触Openresty的时间也不长, 文中难免会有地方写错了或者表达得很差, 欢迎发评论或者发邮件给我指正: lwhile521@gmail.com ,感谢.</p>
<p>对于Openresty, 个人认为要对它产生兴趣,关键在于认不认可让Nginx承担除了Web服务器之外更多的业务, 对于Openresty, 它能带来的好处有:</p>
<ol>
<li>
<p>极致的性能.上文没有提到Openresty的性能, 其实Openresty的编程模型和NodeJS很像, 在Openresty的世界里面,所有东西都是非阻塞的,更难得可贵的是, 它不需要使用NodeJS中的回调函数, 代码写起来其实还是同步模型, 配合C语言编写的Nginx, 最快的脚本语言lua+luajit解释器,这套方案的性能无可挑剔了.</p>
</li>
<li>
<p>降低了Nginx模块的开发难度. Nginx + C/C++能做的, Openresty用lua都能做.开发效率高了, 性能还不怎么降, 何乐而不为呢?</p>
</li>
</ol>
<h3 id="参考资料">参考资料</h3>
<blockquote>
<ol>
<li><a href="https://www.gitbook.com/book/moonbingbing/openresty-best-practices">Openresty最佳实践</a></li>
<li><a href="https://github.com/SkyLothar/lua-resty-jwt">lua-resty-jwt</a></li>
</ol>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[日志切分问题有感]]></title>
        <id>lwhile.github.io/post/ri-zhi-qie-fen-wen-ti-you-gan</id>
        <link href="lwhile.github.io/post/ri-zhi-qie-fen-wen-ti-you-gan">
        </link>
        <updated>2017-11-05T15:46:56.000Z</updated>
        <content type="html"><![CDATA[<p>有过服务端开发经验的同学应该对日志这个东西不陌生, 把程序丢到服务器上跑, 日志就是我们了解运行情况,甚至解BUG的唯一入口了.</p>
<p>有些程序的日志量会增长得非常快, 比如Nginx, 当一个日志文件大到几百MB甚至上GB的时候, 要从这个文件找出我们要的信息就基本等于大海捞针了,所以这时候对日志进行管理就显得格外重要.</p>
<p>日志量大的平台可以上ELK,利用ES的搜索优势基本不担心日志数据量大的问题.但本文不打算涉及这方面的内容.接下来主要讲讲如何正确得对日志文件做切分.</p>
<p>Linxu上的日志切分有两种形式, 一种是使用Linux的logrotate工具, 另外一种是使用额外编写的脚本, 这种形式一般是和日志库配合使用.</p>
<p>因为业务的关系, 我们最开始抛弃了使用logrotate的方案, 因为我们觉得这会给实施人员增加系统的的维护负担(后来发现是我们对logrotate不够了解).于是我们使用第二种方案, 将日志的切分操作在我们的日志库里面实现, 我们封装了logrus和lfshook, 利用logrus的hook机制将切分的逻辑嵌入在日志库里面,代码调用的时候会自动触发切分操作.我们会这样做也是受到beego框架的影响, 它的日志库默认就带了切分功能.</p>
<p>一切运作得很顺利, 直到我们有一次在使用Openresty的时候, 发现Nginx的日志没有被切分.因为之前使用Nginx的时候,默认安装完毕后日志是会自动以天切分的, 于是我们开始找Nginx的配置项,看看是否漏掉了某些配置.但是不找没关系,了解后才发现Nginx是不提供日志切分功能的.What ? 那之前的切分功能是怎么来的?</p>
<p>接下来解决问题的过程中发现了在/etc/logrotate.d/下有nginx的配置, 同时还有Mysql和其他基础组件的,这时我们才想到有可能是RPM包(我们的系统是Centos)安装的时候自动生成了一个logrotate的配置文件,后来一查果然是(命令:rpm -qpl xxx.rpm).而我们的Openresty包没有生成这个配置文件,所以导致Nginx的日志文件没有被切分.</p>
<p>实际上很多软件都只会做日志的记录,不会帮忙做切分,这个确实是合理的.这让我们想起logrus为什么不提供日志切分的功能,而是得由第三方的库去完成.我们将日志切分的逻辑耦合进代码里面,现在回过头来看其实也不是很合理,正确的做法其实还是应该在打RPM包的时候, 生成一个logrotate的配置文件, 这样一来也不会增加实施人员的负担,而且也可以将切分功能统一到一个地方去做.</p>
]]></content>
    </entry>
</feed>